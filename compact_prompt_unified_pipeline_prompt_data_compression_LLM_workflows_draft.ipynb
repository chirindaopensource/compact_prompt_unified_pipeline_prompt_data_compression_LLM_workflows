{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1mhizWJEaa4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# CompactPrompt: A Unified Pipeline for Prompt and Data Compression in LLM Workflows\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2510.18043v1-b31b1b.svg)](https://arxiv.org/abs/2510.18043v1)\n",
        "[![Journal](https://img.shields.io/badge/Journal-ACM%20ICAIF%202025-003366)](https://arxiv.org/abs/2510.18043v1)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Computer%20Science%20%7C%20AI%20for%20Finance-00529B)](https://github.com/chirindaopensource/compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-TAT--QA-lightgrey)](https://github.com/NExTplusplus/TAT-QA)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-FinQA-lightgrey)](https://github.com/czyssrs/FinQA)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Wikipedia%20Dump-lightgrey)](https://dumps.wikimedia.org/)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-ShareGPT-lightgrey)](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-arXiv-lightgrey)](https://arxiv.org/)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Hard%20Prompt%20Pruning%20%7C%20N--gram%20Abbreviation%20%7C%20Quantization-orange)](https://github.com/chirindaopensource/compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Cost--Performance%20Trade--offs%20%7C%20Semantic%20Fidelity-red)](https://github.com/chirindaopensource/compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![Spacy](https://img.shields.io/badge/spaCy-%2309A3D5.svg?style=flat&logo=spaCy&logoColor=white)](https://spacy.io/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-gray?logo=yaml&logoColor=white)](https://pyyaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"CompactPrompt: A Unified Pipeline for Prompt and Data Compression in LLM Workflows\"** by:\n",
        "\n",
        "*   Joong Ho Choi\n",
        "*   Jiayang Zhao\n",
        "*   Jeel Shah\n",
        "*   Ritvika Sonawane\n",
        "*   Vedant Singh\n",
        "*   Avani Appalla\n",
        "*   Will Flanagan\n",
        "*   Filipe Condessa\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and offline corpus statistics generation to the core hard prompt pruning, n-gram abbreviation, numeric quantization, and comprehensive semantic fidelity evaluation.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_compactprompt_pipeline`](#key-callable-run_compactprompt_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Choi et al. (2025). The core of this repository is the iPython Notebook `compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed to be a generalizable toolkit for optimizing Large Language Model (LLM) inference costs and latency in enterprise environments.\n",
        "\n",
        "The paper addresses the challenge of processing long, data-rich contexts—combining free-form instructions, large documents, and numeric tables—under strict cost and context-window constraints. This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Construct a large offline corpus to compute static self-information scores for tokens.\n",
        "-   Implement **Hard Prompt Compression** by pruning low-information phrases identified via a hybrid static/dynamic scoring mechanism.\n",
        "-   Apply **Textual N-gram Abbreviation** to losslessly compress repetitive patterns in attached documents.\n",
        "-   Execute **Numerical Quantization** (Uniform and K-Means) to reduce the token footprint of tabular data while bounding approximation error.\n",
        "-   Select representative few-shot exemplars via embedding-based clustering to maximize prompt utility.\n",
        "-   Evaluate semantic fidelity using both embedding cosine similarity and a simulated human evaluation protocol.\n",
        "-   Automatically generate performance metrics and cost-savings analysis.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in information theory, natural language processing, and data compression principles.\n",
        "\n",
        "**1. Hybrid Self-Information Scoring:**\n",
        "The core of the prompt pruning strategy relies on identifying the information content of each token.\n",
        "-   **Static Self-Information ($I_{\\text{stat}}$):** Derived from a large offline corpus (Wikipedia, ShareGPT, arXiv), capturing global token rarity: $I_{\\text{stat}}(t) = -\\log_2 p(t)$.\n",
        "-   **Dynamic Self-Information ($s_{\\text{dyn}}$):** Derived from a scorer LLM's conditional probability, capturing context-specific surprise: $s_{\\text{dyn}}(t \\mid c) = -\\log_2 P_{\\text{model}}(t \\mid c)$.\n",
        "-   **Fusion Rule:** A combined score $C(t)$ prioritizes dynamic information when the two metrics diverge significantly ($\\Delta > 0.1$), ensuring context-critical tokens are preserved.\n",
        "\n",
        "**2. Dependency-Driven Pruning:**\n",
        "To maintain grammatical coherence, tokens are grouped into syntactic phrases (NP, VP, PP) using dependency parsing. Pruning decisions are made at the phrase level based on aggregated importance scores.\n",
        "\n",
        "**3. Data Compression:**\n",
        "-   **N-gram Abbreviation:** Frequent multi-token patterns in documents are replaced with short, unique placeholders, leveraging the heavy-tailed distribution of language in specialized domains.\n",
        "-   **Quantization:** Floating-point numbers in tables are mapped to integer codes ($q_i$) or cluster centroids, reducing token count while maintaining numerical relationships within a bounded error $\\varepsilon_{\\max}$.\n",
        "\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 35 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks the schema, content integrity, and structural consistency of TAT-QA and Fin-QA datasets.\n",
        "-   **Advanced NLP Processing:** Integrates `spaCy` for dependency parsing and `tiktoken`/`transformers` for precise token alignment.\n",
        "-   **Robust LLM Integration:** A unified interface for interacting with OpenAI, Anthropic, and Together AI models, supporting both generation and log-probability extraction.\n",
        "-   **Comprehensive Evaluation:** Includes automated semantic similarity checks using `all-mpnet-base-v2` and a randomized, bias-mitigated protocol for LLM-based human proxy evaluation.\n",
        "-   **Reproducible Artifacts:** Generates structured logs, compressed datasets, and detailed metric reports for every run.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Preprocessing (Tasks 1-6):** Ingests raw data, validates schemas, cleanses malformed entries, and normalizes numeric columns.\n",
        "2.  **Corpus Statistics (Tasks 7-10):** Builds an offline corpus and computes static self-information scores for the vocabulary.\n",
        "3.  **Prompt Engineering (Tasks 11-13):** Serializes tables to Markdown and constructs prompt templates with few-shot exemplars.\n",
        "4.  **Dynamic Scoring (Tasks 14-18):** Configures LLM resources, retrieves log-probabilities, and computes combined importance scores.\n",
        "5.  **Compression Engines (Tasks 19-27):** Executes phrase-level pruning, n-gram abbreviation, and numeric quantization.\n",
        "6.  **Exemplar Selection (Tasks 28-30):** Embeds candidate examples and selects representative prototypes via K-Means clustering and silhouette optimization.\n",
        "7.  **Evaluation (Tasks 31-35):** Computes embedding similarities and conducts a rigorous human-proxy evaluation to assess semantic fidelity.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 35 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_compactprompt_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_compactprompt_pipeline`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between all 35 sub-tasks.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `pyyaml`, `scipy`, `spacy`, `tiktoken`, `transformers`, `sentence-transformers`, `scikit-learn`.\n",
        "-   LLM Provider SDKs: `openai`, `anthropic`, `together`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows.git\n",
        "    cd compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy pyyaml scipy spacy tiktoken transformers sentence-transformers scikit-learn openai anthropic together\n",
        "    ```\n",
        "\n",
        "4.  **Download spaCy model:**\n",
        "    ```sh\n",
        "    python -m spacy download en_core_web_sm\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two primary DataFrames (`tatqa_raw_df` and `finqa_raw_df`) containing QA examples. Each row must adhere to the following schema:\n",
        "1.  **`example_id`**: Unique string identifier.\n",
        "2.  **`split`**: Dataset partition (\"train\", \"dev\", \"test\").\n",
        "3.  **`question_text`**: The natural language question.\n",
        "4.  **`tables`**: List of table dictionaries (with `headers` and `rows`).\n",
        "5.  **`passages`**: List of passage dictionaries (with `text`).\n",
        "6.  **`answer_type`**, **`answer_value`**, **`answer_unit`**: Ground truth answer fields.\n",
        "\n",
        "Additionally, an `offline_corpus.jsonl` file is required for static statistics, containing documents with `doc_id`, `source`, and `text`.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `run_compactprompt_pipeline` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        study_config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Load raw datasets (Example using synthetic generator provided in the notebook)\n",
        "    # In production, load from JSON/Parquet: pd.read_json(...)\n",
        "    tatqa_raw_df = ...\n",
        "    finqa_raw_df = ...\n",
        "    \n",
        "    # 3. Execute the entire replication study.\n",
        "    output, log = run_compactprompt_pipeline(\n",
        "        tatqa_raw_df=tatqa_raw_df,\n",
        "        finqa_raw_df=finqa_raw_df,\n",
        "        study_config=study_config,\n",
        "        condition=\"compressed_plus_data\",\n",
        "        target_llm=\"gpt-4o\",\n",
        "        scorer_llm=\"gpt-4.1-mini\"\n",
        "    )\n",
        "    \n",
        "    # 4. Access results\n",
        "    print(f\"Mean Compression Ratio: {output.metrics['mean_compression_ratio']:.2f}x\")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns an `OrchestratorOutput` object containing all analytical artifacts:\n",
        "-   **`tatqa_processed_df` / `finqa_processed_df`**: DataFrames with compressed text and quantized tables.\n",
        "-   **`pruned_prompts`**: Dictionary mapping example IDs to their hard-pruned prompt text.\n",
        "-   **`abbreviation_dict`**: The reversible n-gram dictionary.\n",
        "-   **`quantization_results`**: Metadata and codes for all quantized columns.\n",
        "-   **`representative_exemplars`**: IDs of selected few-shot prototypes.\n",
        "-   **`similarity_results`**: Semantic fidelity statistics (cosine similarity).\n",
        "-   **`human_evaluation`**: Results from the LLM-proxy annotation protocol.\n",
        "-   **`metrics`**: Aggregate performance indicators (compression ratio, accuracy).\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "compact_prompt_unified_pipeline/\n",
        "│\n",
        "├── compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows_draft.ipynb  # Main implementation notebook\n",
        "├── config.yaml                                                                        # Master configuration file\n",
        "├── requirements.txt                                                                   # Python package dependencies\n",
        "│\n",
        "├── compact_prompt_outputs/                                                            # Output directory (generated)\n",
        "│   ├── corpus_stats/\n",
        "│   ├── embeddings/\n",
        "│   ├── human_eval_results/\n",
        "│   └── processed_data/\n",
        "│\n",
        "├── LICENSE                                                                            # MIT Project License File\n",
        "└── README.md                                                                          # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as:\n",
        "-   **Compression Budgets:** `prompt_token_budget`.\n",
        "-   **N-gram Settings:** `ngram_length`, `top_n_T`.\n",
        "-   **Quantization:** `bit_width_b`, `num_clusters_k`.\n",
        "-   **Scoring Thresholds:** `delta_relative_difference_threshold`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Adaptive Compression:** Dynamically adjusting the token budget based on query complexity.\n",
        "-   **Privacy-Aware Pruning:** Integrating PII detection to prioritize removing sensitive entities.\n",
        "-   **Multimodal Support:** Extending the pipeline to compress image or chart data attachments.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{choi2025compactprompt,\n",
        "  title={CompactPrompt: A Unified Pipeline for Prompt and Data Compression in LLM Workflows},\n",
        "  author={Choi, Joong Ho and Zhao, Jiayang and Shah, Jeel and Sonawane, Ritvika and Singh, Vedant and Appalla, Avani and Flanagan, Will and Condessa, Filipe},\n",
        "  journal={arXiv preprint arXiv:2510.18043v1},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). CompactPrompt: An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Joong Ho Choi et al.** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, spaCy, and Hugging Face**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `compact_prompt_unified_pipeline_prompt_data_compression_LLM_workflows_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "wM_dLuukxZqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflow*\"\n",
        "\n",
        "Authors: Joong Ho Choi, Jiayang Zhao, Jeel Shah, Ritvika Sonawane, Vedant Singh, Avani Appalla, Will Flanagan, Filipe Condessa\n",
        "\n",
        "E-Journal Submission Date: 20 October 2025\n",
        "\n",
        "Conference Affiliation: Workshop on LLMs and Generative AI for Finance at ACM ICAIF 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2510.18043v1\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Large Language Models (LLMs) deliver powerful reasoning and generation capabilities but incur substantial run-time costs when operating in agentic workflows that chain together lengthy prompts and process rich data streams. We introduce CompactPrompt, an end-to-end pipeline that merges hard prompt compression with lightweight file-level data compression. CompactPrompt first prunes low-information tokens from prompts using self-information scoring and dependency-based phrase grouping. In parallel, it applies n-gram abbreviation to recurrent textual patterns in attached documents and uniform quantization to numerical columns, yielding compact yet semantically faithful representations. Integrated into standard LLM agents, CompactPrompt reduces total token usage and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA, while preserving output quality (Results in less than 5% accuracy drop for Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time compression decisions and quantify cost-performance trade-offs, laying the groundwork for leaner generative AI pipelines."
      ],
      "metadata": {
        "id": "6cjHZzuIEepQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Architectural Overview\n",
        "CompactPrompt is an end-to-end, training-free pipeline designed to compress prompt contexts (instructions + data attachments) for LLM inference. It operates on three distinct vectors simultaneously:\n",
        "1.  **Hard Prompt Pruning:** Removal of low-information tokens from natural language instructions.\n",
        "2.  **Textual N-gram Abbreviation:** Reversible dictionary encoding for repetitive patterns in attached documents.\n",
        "3.  **Numerical Quantization:** Bit-width reduction for floating-point data in structured tables.\n",
        "\n",
        "The pipeline is implemented as a pre-processing layer before the LLM API call, aiming to reduce token usage by up to 60% while maintaining or improving reasoning accuracy on financial QA tasks (TAT-QA, FinQA).\n",
        "\n",
        "### Hard Prompt Compression (Token Pruning)\n",
        "This module identifies and removes tokens that contribute minimal information to the semantic understanding of the prompt. It utilizes a hybrid scoring mechanism combining static and dynamic metrics.\n",
        "\n",
        "**A. Static Self-Information ($I_{\\text{stat}}$)**\n",
        "Calculated offline using a large corpus (Wikipedia, ShareGPT, arXiv). For a token $t$, the unigram probability $p(t)$ is:\n",
        "$$p(t) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}\\{w_i = t\\}$$\n",
        "The static self-information is:\n",
        "$$I_{\\text{stat}}(t) = -\\log_2 p(t)$$\n",
        "\n",
        "**B. Dynamic Self-Information ($I_{\\text{dyn}}$)**\n",
        "Calculated at runtime using a lightweight scorer model. Given a token $t$ and its preceding context $c$:\n",
        "$$I_{\\text{dyn}}(t \\mid c) = -\\log_2 P_{\\text{model}}(t \\mid c)$$\n",
        "\n",
        "**C. Hybrid Scoring Strategy**\n",
        "The system fuses these scores based on their relative divergence ($\\Delta$). If the scores are similar (within 10%), the average is used; otherwise, the dynamic score (context-aware) takes precedence.\n",
        "$$\\Delta = \\frac{|s_{\\text{dyn}} - s_{\\text{stat}}|}{s_{\\text{stat}}}$$\n",
        "$$C(s_{\\text{stat}}, s_{\\text{dyn}}) = \\begin{cases} \\frac{s_{\\text{stat}} + s_{\\text{dyn}}}{2}, & \\Delta \\leq 0.1 \\\\ s_{\\text{dyn}}, & \\Delta > 0.1 \\end{cases}$$\n",
        "Tokens falling below a specific information threshold are pruned, often using dependency parsing to ensure phrase-level coherence rather than isolated token removal.\n",
        "\n",
        "### Textual N-gram Abbreviation\n",
        "This module targets repetitive multi-word expressions in attached documents (e.g., \"interest expense\", \"per share\").\n",
        "\n",
        "**A. Extraction & Frequency Analysis**\n",
        "*   Extract all n-grams of length $n$ (typically $n \\in [2, 5]$).\n",
        "*   Compute frequency distributions across the document corpus.\n",
        "*   Select the Top-$K$ most frequent n-grams (typically $K \\in [100, 150]$).\n",
        "\n",
        "**B. Dictionary Construction & Replacement**\n",
        "*   Map each Top-$K$ n-gram to a unique, short placeholder token (e.g., \"ABC1\").\n",
        "*   Store the mapping $\\{ \\text{placeholder} \\leftrightarrow \\text{original n-gram} \\}$ in a metadata table.\n",
        "*   **Reversibility:** The compression is lossless regarding the specific n-grams targeted; the LLM sees the placeholder, and the mapping can be injected or used for reconstruction.\n",
        "\n",
        "**C. Optimal Configuration**\n",
        "Empirical results indicate that **Bi-grams ($n=2$)** with a **Top-3 ($K=3$)** frequency threshold yield the optimal trade-off between token reduction and semantic disruption ($\\Delta \\text{Accuracy} \\approx +5.0\\%$).\n",
        "\n",
        "### Numerical Quantization\n",
        "This module compresses numerical columns in structured data attachments (e.g., CSVs, financial tables) by reducing floating-point precision.\n",
        "\n",
        "**A. Uniform Integer Quantization**\n",
        "Given a column vector $\\mathbf{x}$, compute $\\min_x$ and $\\max_x$. Select a bit-width $b$ (levels $L=2^b$).\n",
        "Encode value $x_i$ to integer $q_i$:\n",
        "$$q_i = \\text{round}\\left( \\frac{x_i - \\min_x}{\\max_x - \\min_x} (L - 1) \\right)$$\n",
        "Reconstruct $\\hat{x}_i$ with maximum absolute error $\\varepsilon_{\\max}$:\n",
        "$$\\hat{x}_i = \\min_x + \\frac{q_i}{L-1}(\\max_x - \\min_x)$$\n",
        "$$\\varepsilon_{\\max} = \\frac{\\max_x - \\min_x}{L-1}$$\n",
        "\n",
        "**B. K-Means Quantization**\n",
        "Alternatively, apply k-means clustering to column values to find $k$ centroids $(\\mu_1, \\dots, \\mu_k)$. Map each $x_i$ to the index of the nearest centroid.\n",
        "\n",
        "### Representative Exemplar Selection (Few-Shot)\n",
        "To maximize the utility of few-shot examples within the compressed context window:\n",
        "1.  **Embedding:** Encode candidate examples using `all-mpnet-base-v2`.\n",
        "2.  **Clustering:** Perform k-means clustering ($k \\in [5, 50]$).\n",
        "3.  **Optimization:** Select optimal cluster count $k^*$ via Silhouette Score maximization.\n",
        "4.  **Selection:** Choose the example closest to the centroid of each cluster as a \"prototype.\"\n",
        "\n",
        "### Evaluation Metrics\n",
        "*   **Compression Ratio:** $\\frac{\\text{Original Token Count}}{\\text{Compressed Token Count}}$.\n",
        "*   **Semantic Fidelity:** Cosine similarity between embeddings of original and compressed prompts ($\\text{cosine}(E_{\\text{orig}}, E_{\\text{comp}})$). High fidelity is observed at similarity $\\geq 0.92$.\n",
        "*   **Downstream Accuracy:** Exact Match (EM) or F1 score on QA tasks. Results show up to **+6% accuracy** on TAT-QA and **+10%** on FinQA using Claude-3.5-Sonnet, despite >50% token reduction."
      ],
      "metadata": {
        "id": "nYKNC_g8KW8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "aNxrbptdK2C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  CompactPrompt: A Unified Pipeline for Prompt and Data Compression in LLM Workflows\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"CompactPrompt: A Unified Pipeline for Prompt\n",
        "#  and Data Compression in LLM Workflows\" by Choi et al. (2025). It delivers a\n",
        "#  computationally tractable system for optimizing large language model inference\n",
        "#  in enterprise environments by compressing long, data-rich contexts—combining\n",
        "#  free-form instructions, large documents, and numeric tables—under strict cost,\n",
        "#  latency, and context-window constraints, while preserving or improving\n",
        "#  downstream task accuracy.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Hard Prompt Pruning via hybrid static/dynamic self-information scoring\n",
        "#  • Dependency-driven phrase grouping for syntactically coherent pruning\n",
        "#  • Reversible textual n-gram abbreviation for high-frequency pattern compression\n",
        "#  • Uniform and K-Means numerical quantization for tabular data reduction\n",
        "#  • Clustering-based representative exemplar selection for few-shot prompting\n",
        "#  • Semantic fidelity evaluation using embedding similarity and human proxies\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Modular pipeline architecture with clear separation of concerns\n",
        "#  • Robust integration with multiple LLM providers (OpenAI, Anthropic, Together AI)\n",
        "#  • Efficient offline corpus statistics computation and tokenization alignment\n",
        "#  • Comprehensive evaluation harness for TAT-QA and Fin-QA benchmarks\n",
        "#  • Structured logging and artifact management for full reproducibility\n",
        "#  • Type-safe interfaces and rigorous input validation throughout\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Choi, J. H., Zhao, J., Shah, J., Sonawane, R., Singh, V., Appalla, A.,\n",
        "#  Flanagan, W., & Condessa, F. (2025). CompactPrompt: A Unified Pipeline for\n",
        "#  Prompt and Data Compression in LLM Workflows. arXiv preprint arXiv:2510.18043v1.\n",
        "#  https://arxiv.org/abs/2510.18043v1\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "# ==============================================================================#\n",
        "# COMPACTPROMPT PIPELINE IMPORTS\n",
        "# ==============================================================================#\n",
        "\n",
        "# 1. Future Compatibility\n",
        "from __future__ import annotations\n",
        "\n",
        "# 2. Standard Library Imports\n",
        "import abc\n",
        "import collections\n",
        "import dataclasses\n",
        "import datetime\n",
        "import enum\n",
        "import gzip\n",
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum, auto\n",
        "from typing import (\n",
        "    Any,\n",
        "    Counter as CounterType,\n",
        "    Dict,\n",
        "    Generator,\n",
        "    List,\n",
        "    Optional,\n",
        "    Set,\n",
        "    Tuple,\n",
        "    Union,\n",
        ")\n",
        "\n",
        "# 3. Third-Party Data Science & Math Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 4. Third-Party NLP & Machine Learning Libraries\n",
        "import spacy\n",
        "import tiktoken\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min, silhouette_score\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 5. Third-Party LLM Provider Libraries\n",
        "# Note: These libraries must be installed in the environment.\n",
        "# We import them within try-except blocks in specific callables for robustness,\n",
        "# but they are required for the full pipeline execution.\n",
        "try:\n",
        "    from anthropic import Anthropic\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    from together import Together\n",
        "except ImportError:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "ClL1Q2eOKzG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "fgqmgMDbK91A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "## **Discussion of Input-Process-Output flow of *CompactPrompt* Research Framework Callables**\n",
        "\n",
        "### **1. `validate_tatqa_dataset`**\n",
        "*   **Inputs:** The raw `pandas.DataFrame` containing the TAT-QA dataset.\n",
        "*   **Processes:** Orchestrates a sequence of validation sub-routines: schema verification (column presence), identifying field integrity checks (uniqueness of IDs), and complex structure validation (nested tables and passages).\n",
        "*   **Outputs:** A `ValidationReport` object detailing the validity status and specific error logs.\n",
        "*   **Data Transformation:** The input data is not mutated; it is inspected. The transformation is from **raw data** to **metadata regarding its quality**.\n",
        "*   **Research Context:** Implements the data preparation phase necessary for **Section 5.1 (Task Setup)**. It ensures the dataset conforms to the \"hybrid of Tabular and Textual Content\" structure described in the TAT-QA citation [29], ensuring that the \"structured tabular data\" and \"unstructured narrative passages\" are correctly formatted for downstream compression.\n",
        "\n",
        "### **2. `validate_finqa_dataset`**\n",
        "*   **Inputs:** The raw `pandas.DataFrame` containing the Fin-QA dataset.\n",
        "*   **Processes:** Executes validation logic specific to financial QA data: checking for numeric answer types, valid table headers, and consistent split identifiers.\n",
        "*   **Outputs:** A `ValidationReport` object.\n",
        "*   **Data Transformation:** Read-only inspection transforming **raw rows** into a **validity verdict**.\n",
        "*   **Research Context:** Implements data preparation for **Section 5.1 (Task Setup)** regarding Fin-QA [3]. It verifies the integrity of the \"financial reports\" data structure required for the \"numerical reasoning\" tasks evaluated in the study.\n",
        "\n",
        "### **3. `validate_and_update_study_config`**\n",
        "*   **Inputs:** A raw Python dictionary representing the study configuration (containing placeholders).\n",
        "*   **Processes:** Recursively inventories placeholders, resolves them with scientifically justifiable defaults (e.g., setting $\\Delta$ thresholds), and validates LLM API configurations.\n",
        "*   **Outputs:** A fully resolved, executable configuration dictionary.\n",
        "*   **Data Transformation:** Transforms a **template configuration** into a **runtime-ready system state**.\n",
        "*   **Research Context:** Establishes the hyperparameters defined in **Section 3 (Design)**. Specifically, it sets the threshold $\\Delta = 0.1$ used in **Equation (3)** and the n-gram parameters ($n, K$) used in **Section 3.2**.\n",
        "\n",
        "### **4. `cleanse_tatqa_dataset`**\n",
        "*   **Inputs:** The raw TAT-QA DataFrame.\n",
        "*   **Processes:** Filters rows based on critical missing fields, repairs malformed table structures (truncating/padding rows), and removes empty passages.\n",
        "*   **Outputs:** A cleansed `pandas.DataFrame` and a `CleansingLog`.\n",
        "*   **Data Transformation:** Transforms **noisy, potentially malformed data** into **structurally consistent data** suitable for tokenization.\n",
        "*   **Research Context:** Pre-processing required to ensure the \"long-form contexts\" described in **Section 5.1** are valid before being fed into the compression pipeline.\n",
        "\n",
        "### **5. `cleanse_finqa_dataset`**\n",
        "*   **Inputs:** The raw Fin-QA DataFrame.\n",
        "*   **Processes:** Similar to TAT-QA cleansing but enforces stricter requirements for table existence, as Fin-QA relies heavily on tabular data. Flags invalid numeric answers.\n",
        "*   **Outputs:** A cleansed `pandas.DataFrame` and a `FinQACleansingLog`.\n",
        "*   **Data Transformation:** Transforms **raw financial data** into **clean inputs** for the quantization module.\n",
        "*   **Research Context:** Ensures the integrity of the \"structured numerical data\" mentioned in **Section 1**, which is a prerequisite for the **Numerical Quantization (Section 3.3)** experiments.\n",
        "\n",
        "### **6. `normalize_data_task`**\n",
        "*   **Inputs:** Cleansed TAT-QA and Fin-QA DataFrames.\n",
        "*   **Processes:** Identifies numeric columns using a 90% parseability threshold and parses string answers into floating-point numbers using regex-based normalization.\n",
        "*   **Outputs:** DataFrames enriched with numeric metadata and parsed answers.\n",
        "*   **Data Transformation:** Transforms **string representations** of numbers (e.g., \"1.5M\") into **floating-point values** (e.g., 1,500,000.0).\n",
        "*   **Research Context:** This is the identification step for **Section 3.3 (Numerical Quantization)**. It determines which columns $x$ are eligible for the quantization transform $q_i = \\text{round}(\\dots)$.\n",
        "\n",
        "### **7. `build_offline_corpus`**\n",
        "*   **Inputs:** Configuration paths for Wikipedia, ShareGPT, and arXiv dumps.\n",
        "*   **Processes:** Ingests documents from diverse sources, normalizes text (stripping LaTeX/HTML), and consolidates them.\n",
        "*   **Outputs:** A single JSONL corpus file.\n",
        "*   **Data Transformation:** Transforms **heterogeneous source files** into a **unified text stream**.\n",
        "*   **Research Context:** Implements the corpus construction described in **Section 3.1.1 (Static Self-Information)**: \"We begin by constructing a large offline corpus... consisting of Wikipedia, ShareGPT conversations, and arXiv articles.\"\n",
        "\n",
        "### **8. `compute_corpus_statistics`**\n",
        "*   **Inputs:** The offline JSONL corpus.\n",
        "*   **Processes:** Streams the corpus, tokenizes text using a specific tokenizer (e.g., `cl100k_base`), and aggregates token counts.\n",
        "*   **Outputs:** A `Counter` of token IDs and the total token count $N$.\n",
        "*   **Data Transformation:** Transforms **text documents** into **frequency distributions**.\n",
        "*   **Research Context:** Calculates the denominator $N$ and the raw counts required for **Equation (1)** in **Section 3.1.1**: $f(t) = \\frac{1}{N} \\sum \\mathbb{1}\\{w_i = t\\}$.\n",
        "\n",
        "### **9. `compute_token_probabilities`**\n",
        "*   **Inputs:** Token counts and total count $N$.\n",
        "*   **Processes:** Divides counts by $N$ to obtain empirical probabilities.\n",
        "*   **Outputs:** A dictionary mapping token IDs to probabilities $p(t)$.\n",
        "*   **Data Transformation:** Transforms **raw counts** into **probability space**.\n",
        "*   **Research Context:** Implements the calculation of $p(t)$ derived from **Equation (1)** in **Section 3.1.1**, which is the basis for static self-information.\n",
        "\n",
        "### **10. `compute_static_self_information`**\n",
        "*   **Inputs:** Token probabilities $p(t)$.\n",
        "*   **Processes:** Computes the negative binary logarithm of probabilities.\n",
        "*   **Outputs:** A dictionary mapping token IDs to static self-information scores $I_{stat}$.\n",
        "*   **Data Transformation:** Transforms **probabilities** into **information content (bits)**.\n",
        "*   **Research Context:** Directly implements the static self-information formula from **Section 3.1.1**:\n",
        "    $$I_{\\text{stat}}(t) = -\\log_2 p(t)$$\n",
        "\n",
        "### **11. `serialize_tables_task`**\n",
        "*   **Inputs:** DataFrames with structured table dictionaries.\n",
        "*   **Processes:** Converts table objects into Markdown-formatted strings, handling headers and row alignment.\n",
        "*   **Outputs:** DataFrames with a new `serialized_tables` column.\n",
        "*   **Data Transformation:** Transforms **structured objects** into **linear text** suitable for LLM context windows.\n",
        "*   **Research Context:** Prepares the \"structured tabular data\" for the prompt, a necessary step before the \"Hard Prompt Pruning\" described in **Section 3.1** can operate on the table tokens.\n",
        "\n",
        "### **12. `construct_prompt`**\n",
        "*   **Inputs:** Question, serialized tables, passages, and optional exemplars.\n",
        "*   **Processes:** Injects these components into a dataset-specific template.\n",
        "*   **Outputs:** A single prompt string.\n",
        "*   **Data Transformation:** Aggregates **disparate context elements** into a **single inference payload**.\n",
        "*   **Research Context:** Constructs the input $P$ that is subject to compression. This represents the \"verbose or unstructured inputs\" mentioned in **Section 1** that CompactPrompt aims to optimize.\n",
        "\n",
        "### **13. `format_exemplars`**\n",
        "*   **Inputs:** A list of exemplar dictionaries and the target example ID.\n",
        "*   **Processes:** Formats exemplars into text blocks and filters out the target ID to prevent data leakage.\n",
        "*   **Outputs:** A formatted string of few-shot examples.\n",
        "*   **Data Transformation:** Transforms **retrieved examples** into **prompt context**.\n",
        "*   **Research Context:** Implements the few-shot prompting strategy discussed in **Section 3.4**, ensuring that the \"representative examples\" are correctly integrated into the prompt structure.\n",
        "\n",
        "### **14. `configure_llm_resources`**\n",
        "*   **Inputs:** The study configuration dictionary.\n",
        "*   **Processes:** Instantiates concrete `LLMInterface` objects (e.g., `GPT4oInterface`, `LlamaInterface`) based on model names.\n",
        "*   **Outputs:** A registry of initialized LLM interfaces.\n",
        "*   **Data Transformation:** Transforms **configuration strings** into **active API clients**.\n",
        "*   **Research Context:** Sets up the \"pretrained LLM or a lightweight scoring agent\" mentioned in **Section 3.1.2** for dynamic scoring, and the target models for **Section 5** evaluation.\n",
        "\n",
        "### **15. `prepare_dynamic_scoring_inputs`**\n",
        "*   **Inputs:** DataFrames and the scorer LLM interface.\n",
        "*   **Processes:** Serializes examples to prompts, tokenizes them using the scorer's tokenizer, and computes character offsets.\n",
        "*   **Outputs:** A dictionary of tokenized prompts and offsets.\n",
        "*   **Data Transformation:** Transforms **semantic examples** into **token sequences** ready for scoring.\n",
        "*   **Research Context:** Prepares the input $t$ and context $c$ required to query the model for $P_{\\text{model}}(t \\mid c)$ as described in **Section 3.1.2**.\n",
        "\n",
        "### **16. `get_prompt_logprobs_task`**\n",
        "*   **Inputs:** Tokenized prompts and the scorer interface.\n",
        "*   **Processes:** Queries the LLM API (using `echo=True` or iterative scoring) to retrieve log-probabilities.\n",
        "*   **Outputs:** A dictionary mapping example IDs to lists of log-probabilities.\n",
        "*   **Data Transformation:** Transforms **token sequences** into **raw model likelihoods**.\n",
        "*   **Research Context:** Retrieves the conditional probabilities $P_{\\text{model}}(t \\mid c)$ required for **Section 3.1.2 (Dynamic Self-Information)**.\n",
        "\n",
        "### **17. `compute_dynamic_scores_task`**\n",
        "*   **Inputs:** Raw log-probabilities (natural log).\n",
        "*   **Processes:** Converts natural logs to bits (base 2) and validates values.\n",
        "*   **Outputs:** A dictionary of dynamic self-information scores $s_{dyn}$.\n",
        "*   **Data Transformation:** Transforms **model log-probs** into **information scores**.\n",
        "*   **Research Context:** Implements the dynamic self-information formula implied in **Section 3.1.2**:\n",
        "    $$s_{\\text{dyn}}(t \\mid c) = -\\log_2 P_{\\text{model}}(t \\mid c)$$\n",
        "\n",
        "### **18. `compute_combined_scores_task`**\n",
        "*   **Inputs:** Static scores $s_{stat}$ and dynamic scores $s_{dyn}$.\n",
        "*   **Processes:** Computes the relative difference $\\Delta$ and applies the fusion rule.\n",
        "*   **Outputs:** A dictionary of combined importance scores $C(t)$.\n",
        "*   **Data Transformation:** Fuses **global** and **local** information measures into a **single utility metric**.\n",
        "*   **Research Context:** Implements **Equations (2) and (3)** from **Section 3.1.3**:\n",
        "    $$\\Delta = \\frac{|s_{\\text{dyn}} - s_{\\text{stat}}|}{s_{\\text{stat}}}$$\n",
        "    $$C(s_{\\text{stat}}, s_{\\text{dyn}}) = \\begin{cases} \\frac{s_{\\text{stat}} + s_{\\text{dyn}}}{2}, & \\Delta \\leq 0.1 \\\\ s_{\\text{dyn}}, & \\Delta > 0.1 \\end{cases}$$\n",
        "\n",
        "### **19. `group_tokens_into_phrases_task`**\n",
        "*   **Inputs:** Prompt text and token offsets.\n",
        "*   **Processes:** Runs a dependency parser to identify NP, VP, and PP spans, then maps tokens to these phrases.\n",
        "*   **Outputs:** A mapping of example IDs to lists of phrases (token indices).\n",
        "*   **Data Transformation:** Transforms **flat token sequences** into **hierarchical syntactic units**.\n",
        "*   **Research Context:** Implements the \"dependency-based phrase grouping\" described in **Section 3.1.3**, ensuring that pruning operates on grammatical units rather than individual tokens.\n",
        "\n",
        "### **20. `compute_phrase_scores_task`**\n",
        "*   **Inputs:** Combined token scores $C(t)$ and phrase groupings.\n",
        "*   **Processes:** Aggregates token scores within each phrase (e.g., via mean).\n",
        "*   **Outputs:** Phrase-level importance scores $C(\\phi)$.\n",
        "*   **Data Transformation:** Aggregates **atomic scores** into **structural scores**.\n",
        "*   **Research Context:** The final step of **Section 3.1.3**, assigning a utility value to each syntactic unit to enable \"Phrase-Level Pruning\".\n",
        "\n",
        "### **21. `prune_prompt_task`**\n",
        "*   **Inputs:** Phrase scores, phrase definitions, and a token budget.\n",
        "*   **Processes:** Sorts phrases by importance and greedily selects them until the budget is met, then reconstructs the text.\n",
        "*   **Outputs:** Compressed prompt text and compression metrics.\n",
        "*   **Data Transformation:** Transforms a **verbose prompt** into a **compressed prompt** $P'$.\n",
        "*   **Research Context:** Implements the core **Hard Prompt Pruning** logic described in **Section 3.1**, realizing the goal of reducing token usage while preserving high-information content.\n",
        "\n",
        "### **22. `extract_top_ngrams_task`**\n",
        "*   **Inputs:** Passage texts from the datasets.\n",
        "*   **Processes:** Tokenizes passages, counts n-grams using a sliding window, and selects the top $K$.\n",
        "*   **Outputs:** A list of the top-K n-grams and their counts.\n",
        "*   **Data Transformation:** Transforms **corpus text** into **frequency statistics**.\n",
        "*   **Research Context:** Implements **Section 3.2.1 (Extraction and Frequency Analysis)**, identifying the \"top (K) most frequent patterns\" for abbreviation.\n",
        "\n",
        "### **23. `construct_abbreviation_dict_task`**\n",
        "*   **Inputs:** Top-K n-grams.\n",
        "*   **Processes:** Assigns unique placeholders to n-grams and builds bidirectional maps.\n",
        "*   **Outputs:** An abbreviation dictionary $\\mathcal{D}_{abbr}$.\n",
        "*   **Data Transformation:** Transforms **frequent patterns** into a **substitution logic**.\n",
        "*   **Research Context:** Implements **Section 3.2.2 (Dictionary Construction)**, creating the mapping required for \"lossless round-trip reconstruction.\"\n",
        "\n",
        "### **24. `apply_abbreviation_task`**\n",
        "*   **Inputs:** Passage texts and the abbreviation dictionary.\n",
        "*   **Processes:** Replaces active n-grams (Top-T) with placeholders in the text.\n",
        "*   **Outputs:** DataFrames with abbreviated passages.\n",
        "*   **Data Transformation:** Transforms **redundant text** into **compressed text**.\n",
        "*   **Research Context:** Implements **Section 3.2.3 (Contextual Replacement)**, applying the \"user-configurable n-gram abbreviation pipeline\" to reduce the size of attached documents.\n",
        "\n",
        "### **25. `extract_numeric_values_task`**\n",
        "*   **Inputs:** DataFrames and numeric column metadata.\n",
        "*   **Processes:** Extracts and parses floating-point values from identified numeric columns.\n",
        "*   **Outputs:** A dictionary of numeric arrays $x$ for each column.\n",
        "*   **Data Transformation:** Transforms **tabular cells** into **numerical vectors**.\n",
        "*   **Research Context:** Prepares the input data for **Section 3.3 (Numerical Quantization)**, isolating the floating-point columns that require compression.\n",
        "\n",
        "### **26. `apply_uniform_quantization_task`**\n",
        "*   **Inputs:** Numeric arrays and bit-width $b$.\n",
        "*   **Processes:** Computes min/max ranges and maps values to integer codes.\n",
        "*   **Outputs:** Quantized codes $q_i$ and reconstruction metadata.\n",
        "*   **Data Transformation:** Transforms **floats** into **integers**.\n",
        "*   **Research Context:** Implements **Section 3.3.1 (Uniform Integer Quantization)**, specifically **Equation (4)**:\n",
        "    $$q_i = \\text{round}\\left( \\frac{x_i - \\min_x}{\\max_x - \\min_x} (L - 1) \\right)$$\n",
        "\n",
        "### **27. `apply_kmeans_quantization_task`**\n",
        "*   **Inputs:** Numeric arrays and cluster count $k$.\n",
        "*   **Processes:** Fits K-Means models and maps values to cluster centroids.\n",
        "*   **Outputs:** Cluster indices (codes) and centroids.\n",
        "*   **Data Transformation:** Transforms **floats** into **cluster indices**.\n",
        "*   **Research Context:** Implements **Section 3.3.2 (K-Means-Based Quantization)**, mapping each $x_i$ to the \"nearest centroid index.\"\n",
        "\n",
        "### **28. `embed_examples_task`**\n",
        "*   **Inputs:** DataFrames and the embedding model.\n",
        "*   **Processes:** Constructs text representations of examples and encodes them into vectors.\n",
        "*   **Outputs:** An embedding matrix $Z$.\n",
        "*   **Data Transformation:** Transforms **textual examples** into **vector space**.\n",
        "*   **Research Context:** Implements **Section 3.4.1 (Embedding and Normalization)**, using `all-mpnet-base-v2` to embed data for clustering.\n",
        "\n",
        "### **29. `select_optimal_k_task`**\n",
        "*   **Inputs:** Embedding matrix $Z$.\n",
        "*   **Processes:** Sweeps $k \\in [5, 50]$, computes silhouette scores, and selects the optimal $k^*$.\n",
        "*   **Outputs:** The optimal cluster count $k^*$.\n",
        "*   **Data Transformation:** Transforms **geometric structure** into a **hyperparameter decision**.\n",
        "*   **Research Context:** Implements **Section 3.4.2 (Clustering with Silhouette Optimization)** to \"identify the optimal cluster count ($k^*$).\"\n",
        "\n",
        "### **30. `select_representative_exemplars_task`**\n",
        "*   **Inputs:** Embeddings $Z$ and optimal $k^*$.\n",
        "*   **Processes:** Clusters data and selects the sample closest to each centroid.\n",
        "*   **Outputs:** A list of representative example IDs.\n",
        "*   **Data Transformation:** Transforms **clusters** into **prototypical instances**.\n",
        "*   **Research Context:** Implements **Section 3.4.3 (Representative Points Selection)**, selecting \"prototypes\" to serve as few-shot examples.\n",
        "\n",
        "### **31. `compute_similarity_embeddings_task`**\n",
        "*   **Inputs:** Original and compressed prompt pairs.\n",
        "*   **Processes:** Embeds both versions using `all-mpnet-base-v2`.\n",
        "*   **Outputs:** Embedding matrices $U$ (original) and $V$ (compressed).\n",
        "*   **Data Transformation:** Transforms **prompt pairs** into **comparable vectors**.\n",
        "*   **Research Context:** Prepares the data for **Section 3.5.1 (Full-Dimensional Cosine Similarity)** evaluation.\n",
        "\n",
        "### **32. `evaluate_semantic_similarity_task`**\n",
        "*   **Inputs:** Embedding matrices $U$ and $V$.\n",
        "*   **Processes:** Computes cosine similarity and aggregates statistics (mean, 5th percentile).\n",
        "*   **Outputs:** Similarity statistics and flagged outliers.\n",
        "*   **Data Transformation:** Transforms **vector relationships** into **fidelity metrics**.\n",
        "*   **Research Context:** Implements the metric calculation in **Section 3.5.1**, reporting \"mean and 5th percentile scores to ensure worst-case fidelity.\"\n",
        "\n",
        "### **33. `run_llm_calibration_task`**\n",
        "*   **Inputs:** Calibration pairs and LLM agents.\n",
        "*   **Processes:** Runs the calibration protocol, collecting scores and computing Cohen's Kappa.\n",
        "*   **Outputs:** Calibration agreement metrics.\n",
        "*   **Data Transformation:** Transforms **agent outputs** into **reliability metrics**.\n",
        "*   **Research Context:** Implements the **Human Validation** setup described in **Section 3.5.2**, ensuring evaluators (proxies) are aligned before the main study.\n",
        "\n",
        "### **34. `run_main_evaluation_task`**\n",
        "*   **Inputs:** Evaluation pairs and calibrated agents.\n",
        "*   **Processes:** Collects semantic equivalence ratings (1-5) for 90 pairs using a randomized protocol.\n",
        "*   **Outputs:** Raw rating data.\n",
        "*   **Data Transformation:** Transforms **comparative judgments** into **structured rating data**.\n",
        "*   **Research Context:** Executes the main **Human Validation** study from **Section 3.5.2**, rating \"90 original & compressed prompt pairs.\"\n",
        "\n",
        "### **35. `analyze_human_vs_embedding_task`**\n",
        "*   **Inputs:** Human ratings and embedding similarities.\n",
        "*   **Processes:** Aggregates ratings and compares them against cosine similarity thresholds.\n",
        "*   **Outputs:** A report on mismatches and correlation.\n",
        "*   **Data Transformation:** Transforms **multi-modal evaluation data** into **conclusions**.\n",
        "*   **Research Context:** Implements the analysis in **Section 3.5.2**, verifying the correlation between human scores and cosine similarities and identifying \"rare nuance shifts missed by embedding alone.\"\n",
        "\n",
        "### **36. `run_compactprompt_pipeline`**\n",
        "*   **Inputs:** Raw DataFrames, configuration, and experimental condition flags.\n",
        "*   **Processes:** Orchestrates the entire 12-phase pipeline, invoking all 35 sub-tasks in the correct dependency order, managing state, logging, and error handling.\n",
        "*   **Outputs:** An `OrchestratorOutput` object containing all artifacts (compressed data, metrics, logs).\n",
        "*   **Data Transformation:** Transforms **raw datasets and configuration** into **final experimental results**.\n",
        "*   **Research Context:** This is the **Unified Pipeline** itself, realizing the end-to-end system described in **Section 1 (Introduction)** and **Section 4 (CompactPrompt Tool)**. It binds the disparate compression techniques into a coherent workflow that \"merges hard prompt compression with lightweight file-level data compression.\"\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Usage Example**\n",
        "\n",
        "Below is a Python script which illustrates how to use the pipeline orchestrator callable:\n",
        "\n",
        "```python\n",
        "# ==============================================================================\n",
        "# COMPACTPROMPT PIPELINE USAGE EXAMPLE USING MOCK DATA\n",
        "# ==============================================================================\n",
        "# This script demonstrates the end-to-end execution of the CompactPrompt pipeline\n",
        "# using synthetically generated mock data and a configuration loaded from a YAML file.\n",
        "# It serves as a high-fidelity, implementation-grade reference for deploying\n",
        "# the compression and selection layer in enterprise LLM workflows.\n",
        "# ==============================================================================\n",
        "# Note: It is assumed that you have pre-imported all the requisite Python modules and Callables\n",
        "\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure logging for the example execution\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"CompactPromptExample\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# STEP 1: SYNTHETIC DATA GENERATION\n",
        "# ------------------------------------------------------------------------------\n",
        "# We generate professional-grade synthetic DataFrames for TAT-QA and Fin-QA\n",
        "# that strictly adhere to the schema requirements defined in the task list.\n",
        "# These DataFrames simulate the raw input data ingested by the pipeline.\n",
        "\n",
        "logger.info(\"Generating synthetic TAT-QA and Fin-QA datasets...\")\n",
        "\n",
        "# 1.1 Synthetic TAT-QA DataFrame\n",
        "tatqa_data = [\n",
        "    {\n",
        "        \"example_id\": \"tatqa_syn_001\",\n",
        "        \"split\": \"dev\",\n",
        "        \"question_text\": \"What was the total revenue for Alpha Corp in 2023?\",\n",
        "        \"tables\": [\n",
        "            {\n",
        "                \"table_id\": \"tatqa_syn_001_t1\",\n",
        "                \"caption\": \"Consolidated Statements of Operations\",\n",
        "                \"headers\": [\"Year\", \"Revenue (USD millions)\", \"Net Income (USD millions)\"],\n",
        "                \"rows\": [\n",
        "                    [\"2021\", \"1500\", \"200\"],\n",
        "                    [\"2022\", \"1650\", \"220\"],\n",
        "                    [\"2023\", \"1800\", \"250\"]\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"passages\": [\n",
        "            {\n",
        "                \"passage_id\": \"tatqa_syn_001_p1\",\n",
        "                \"text\": (\n",
        "                    \"Alpha Corp reported strong financial performance in 2023, driven by \"\n",
        "                    \"robust demand in its cloud services segment. Total revenue reached \"\n",
        "                    \"record highs, reflecting a 9% year-over-year increase.\"\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        \"answer_type\": \"span_number\",\n",
        "        \"answer_value\": \"1800\",\n",
        "        \"answer_unit\": \"USD millions\"\n",
        "    },\n",
        "    {\n",
        "        \"example_id\": \"tatqa_syn_002\",\n",
        "        \"split\": \"dev\",\n",
        "        \"question_text\": \"By how much did net income increase from 2021 to 2022?\",\n",
        "        \"tables\": [\n",
        "            {\n",
        "                \"table_id\": \"tatqa_syn_002_t1\",\n",
        "                \"caption\": \"Financial Highlights\",\n",
        "                \"headers\": [\"Metric\", \"2021\", \"2022\"],\n",
        "                \"rows\": [\n",
        "                    [\"Revenue\", \"1500\", \"1650\"],\n",
        "                    [\"Net Income\", \"200\", \"220\"]\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"passages\": [\n",
        "            {\n",
        "                \"passage_id\": \"tatqa_syn_002_p1\",\n",
        "                \"text\": \"Net income growth was supported by operational efficiency improvements.\"\n",
        "            }\n",
        "        ],\n",
        "        \"answer_type\": \"arithmetic\",\n",
        "        \"answer_value\": \"20\",\n",
        "        \"answer_unit\": \"USD millions\"\n",
        "    }\n",
        "]\n",
        "tatqa_raw_df = pd.DataFrame(tatqa_data)\n",
        "\n",
        "# 1.2 Synthetic Fin-QA DataFrame\n",
        "finqa_data = [\n",
        "    {\n",
        "        \"example_id\": \"finqa_syn_001\",\n",
        "        \"split\": \"dev\",\n",
        "        \"question_text\": \"What was the percentage growth in EPS from 2022 to 2023?\",\n",
        "        \"tables\": [\n",
        "            {\n",
        "                \"table_id\": \"finqa_syn_001_t1\",\n",
        "                \"caption\": \"Earnings Per Share Data\",\n",
        "                \"headers\": [\"Year\", \"Basic EPS\", \"Diluted EPS\"],\n",
        "                \"rows\": [\n",
        "                    [\"2022\", \"2.50\", \"2.48\"],\n",
        "                    [\"2023\", \"2.75\", \"2.72\"]\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"passages\": [\n",
        "            {\n",
        "                \"passage_id\": \"finqa_syn_001_p1\",\n",
        "                \"text\": (\n",
        "                    \"The company's earnings per share (EPS) benefited from share repurchases \"\n",
        "                    \"executed throughout the fiscal year 2023.\"\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        \"answer_type\": \"number\",\n",
        "        \"answer_value\": \"10\",\n",
        "        \"answer_unit\": \"percent\"\n",
        "    },\n",
        "    {\n",
        "        \"example_id\": \"finqa_syn_002\",\n",
        "        \"split\": \"dev\",\n",
        "        \"question_text\": \"Calculate the operating margin for 2023.\",\n",
        "        \"tables\": [\n",
        "            {\n",
        "                \"table_id\": \"finqa_syn_002_t1\",\n",
        "                \"caption\": \"Income Statement\",\n",
        "                \"headers\": [\"Item\", \"2023 Amount (USD millions)\"],\n",
        "                \"rows\": [\n",
        "                    [\"Revenue\", \"5000\"],\n",
        "                    [\"Operating Income\", \"1250\"]\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"passages\": [],\n",
        "        \"answer_type\": \"number\",\n",
        "        \"answer_value\": \"25\",\n",
        "        \"answer_unit\": \"percent\"\n",
        "    }\n",
        "]\n",
        "finqa_raw_df = pd.DataFrame(finqa_data)\n",
        "\n",
        "logger.info(f\"Created TAT-QA DataFrame with {len(tatqa_raw_df)} rows.\")\n",
        "logger.info(f\"Created Fin-QA DataFrame with {len(finqa_raw_df)} rows.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# STEP 2: CONFIGURATION LOADING\n",
        "# ------------------------------------------------------------------------------\n",
        "# We load the study configuration from the 'config.yaml' file. This file contains\n",
        "# all hyperparameters for compression, model selection, and evaluation.\n",
        "# Ensure 'config.yaml' exists in your working directory\n",
        "\n",
        "config_path = \"config.yaml\"\n",
        "\n",
        "with open(config_path, \"r\") as f:\n",
        "    study_config = yaml.safe_load(f)\n",
        "\n",
        "logger.info(\"Configuration loaded successfully.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# STEP 3: OFFLINE CORPUS PREPARATION (MOCK SETUP)\n",
        "# ------------------------------------------------------------------------------\n",
        "# The pipeline requires an offline corpus for static self-information.\n",
        "# In a real deployment, this would be a large JSONL file containing Wikipedia,\n",
        "# ShareGPT, and arXiv dumps.\n",
        "#\n",
        "# Format of \"offline_corpus.jsonl\":\n",
        "# Each line is a JSON object with keys:\n",
        "#   - \"doc_id\": Unique string ID (e.g., \"wiki_123\")\n",
        "#   - \"source\": Source identifier (\"wikipedia\", \"sharegpt\", \"arxiv\")\n",
        "#   - \"title\": Document title (optional)\n",
        "#   - \"text\": The raw text content of the document.\n",
        "#\n",
        "# For this example, we will create a dummy corpus file if it doesn't exist\n",
        "# to allow the pipeline to run.\n",
        "\n",
        "corpus_dir = \"compactprompt_outputs\"\n",
        "if not os.path.exists(corpus_dir):\n",
        "    os.makedirs(corpus_dir)\n",
        "\n",
        "corpus_path = os.path.join(corpus_dir, \"offline_corpus.jsonl\")\n",
        "if not os.path.exists(corpus_path):\n",
        "    logger.info(\"Creating dummy offline corpus for demonstration...\")\n",
        "    dummy_corpus = [\n",
        "        {\"doc_id\": \"wiki_1\", \"source\": \"wikipedia\", \"title\": \"Finance\", \"text\": \"Finance is the study of money and currency.\"},\n",
        "        {\"doc_id\": \"wiki_2\", \"source\": \"wikipedia\", \"title\": \"Revenue\", \"text\": \"Revenue is the income that a business has from its normal business activities.\"},\n",
        "        {\"doc_id\": \"sharegpt_1\", \"source\": \"sharegpt\", \"title\": \"Chat 1\", \"text\": \"User: What is EPS? Assistant: Earnings Per Share.\"},\n",
        "        {\"doc_id\": \"arxiv_1\", \"source\": \"arxiv\", \"title\": \"Transformers\", \"text\": \"Attention is all you need.\"}\n",
        "    ]\n",
        "    with open(corpus_path, \"w\") as f:\n",
        "        for doc in dummy_corpus:\n",
        "            f.write(json.dumps(doc) + \"\\n\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# STEP 4: PIPELINE EXECUTION\n",
        "# ------------------------------------------------------------------------------\n",
        "# We now invoke the orchestrator function.\n",
        "#\n",
        "# CRITICAL NOTE ON MODEL NAMES:\n",
        "# The user MUST verify that the model names specified in `target_llm` and `scorer_llm`\n",
        "# match the exact identifiers required by their API providers (OpenAI, Anthropic, Together).\n",
        "# The config file provides defaults, but API updates may change these strings.\n",
        "# Ensure your environment variables (OPENAI_API_KEY, etc.) are set before running.\n",
        "\n",
        "logger.info(\"Initializing CompactPrompt Pipeline...\")\n",
        "\n",
        "try:\n",
        "    # We use the 'compressed_plus_data' condition to demonstrate the full suite\n",
        "    # of compression techniques (hard pruning + n-gram abbreviation + quantization).\n",
        "    # We skip human evaluation here for automated execution, but in a real study,\n",
        "    # set skip_human_eval=False to run the LLM-proxy annotation protocol.\n",
        "    \n",
        "    orchestrator_output, orchestrator_log = run_compactprompt_pipeline(\n",
        "        tatqa_raw_df=tatqa_raw_df,\n",
        "        finqa_raw_df=finqa_raw_df,\n",
        "        study_config=study_config,\n",
        "        condition=\"compressed_plus_data\",\n",
        "        target_llm=\"gpt-4o\",          # Ensure this matches your API access\n",
        "        scorer_llm=\"gpt-4.1-mini\",    # Efficient scorer\n",
        "        ngram_params={\"top_n_T\": 3, \"ngram_size_G\": 2}, # Best config from paper\n",
        "        quantization_params={\"bit_width\": 8},           # Uniform 8-bit quantization\n",
        "        output_dir=\"compactprompt_outputs\",\n",
        "        skip_corpus_build=False,      # Build stats from our dummy corpus\n",
        "        skip_human_eval=True,         # Skip human eval for this demo run\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # STEP 5: RESULTS INSPECTION\n",
        "    # --------------------------------------------------------------------------\n",
        "    logger.info(\"Pipeline execution successful.\")\n",
        "    \n",
        "    # Accessing Metrics\n",
        "    metrics = orchestrator_output.metrics\n",
        "    logger.info(\"=== Pipeline Metrics ===\")\n",
        "    logger.info(f\"Mean Compression Ratio: {metrics['mean_compression_ratio']:.2f}x\")\n",
        "    logger.info(f\"Mean Cosine Similarity: {metrics['mean_cosine_similarity']:.4f}\")\n",
        "    logger.info(f\"5th Percentile Similarity: {metrics['percentile_5_similarity']:.4f}\")\n",
        "    \n",
        "    # Accessing Compressed Data\n",
        "    # Example: Inspecting the first compressed prompt\n",
        "    first_id = list(orchestrator_output.pruned_prompts.keys())[0]\n",
        "    compressed_data = orchestrator_output.pruned_prompts[first_id]\n",
        "    \n",
        "    logger.info(f\"=== Example Compression ({first_id}) ===\")\n",
        "    logger.info(f\"Original Tokens: {compressed_data['original_tokens']}\")\n",
        "    logger.info(f\"Compressed Tokens: {compressed_data['compressed_tokens']}\")\n",
        "    logger.info(f\"Ratio: {compressed_data['compression_ratio']:.2f}x\")\n",
        "    # logger.info(f\"Compressed Text Snippet: {compressed_data['compressed_text'][:200]}...\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Pipeline execution failed: {e}\")\n",
        "    # In a real scenario, inspect orchestrator_log for detailed error trace\n",
        "    raise\n",
        "```"
      ],
      "metadata": {
        "id": "5NtP9LSKLBDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 – Validate `tatqa_raw_df` Schema and Content Quality\n",
        "\n",
        "@dataclass\n",
        "class ValidationError:\n",
        "    \"\"\"\n",
        "    Dataclass to represent a single validation error found in the dataset.\n",
        "\n",
        "    Attributes:\n",
        "        row_index (int): The index of the row in the DataFrame where the error occurred.\n",
        "        example_id (Optional[str]): The example_id of the row, if available.\n",
        "        column (str): The name of the column where the error was found.\n",
        "        error_type (str): A categorical description of the error (e.g., 'missing_column', 'dtype_mismatch').\n",
        "        message (str): A detailed description of the error.\n",
        "    \"\"\"\n",
        "    row_index: int\n",
        "    column: str\n",
        "    error_type: str\n",
        "    message: str\n",
        "    example_id: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class ValidationReport:\n",
        "    \"\"\"\n",
        "    Dataclass to aggregate validation results.\n",
        "\n",
        "    Attributes:\n",
        "        is_valid (bool): True if no critical errors were found, False otherwise.\n",
        "        errors (List[ValidationError]): A list of all validation errors encountered.\n",
        "        missing_columns (List[str]): List of required columns missing from the DataFrame.\n",
        "        extra_columns (List[str]): List of unexpected columns present in the DataFrame.\n",
        "    \"\"\"\n",
        "    is_valid: bool = True\n",
        "    errors: List[ValidationError] = field(default_factory=list)\n",
        "    missing_columns: List[str] = field(default_factory=list)\n",
        "    extra_columns: List[str] = field(default_factory=list)\n",
        "\n",
        "    def add_error(self, error: ValidationError):\n",
        "        \"\"\"Adds an error to the report and marks the dataset as invalid.\"\"\"\n",
        "        self.errors.append(error)\n",
        "        self.is_valid = False\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate tatqa_raw_df Schema and Content Quality\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Validate column presence and types\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_tatqa_columns(df: pd.DataFrame) -> ValidationReport:\n",
        "    \"\"\"\n",
        "    Validates that the TAT-QA DataFrame contains exactly the required columns and that\n",
        "    their element-level data types are correct based on a random sample.\n",
        "\n",
        "    This function performs two levels of validation:\n",
        "    1. Schema Existence: Checks if all required columns are present.\n",
        "    2. Type Integrity: Samples non-null values from each column to ensure they match\n",
        "       the expected Python native type (e.g., 'example_id' must be a string,\n",
        "       'tables' must be a list). This detects mixed-type columns that Pandas\n",
        "       might label generically as 'object'.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw TAT-QA input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        ValidationReport: A report containing missing/extra columns and type errors.\n",
        "    \"\"\"\n",
        "    # Initialize the validation report object\n",
        "    report = ValidationReport()\n",
        "\n",
        "    # Define the mapping of required columns to their expected native Python types\n",
        "    # Note: 'answer_unit' is optional (can be None), but if present, must be a string.\n",
        "    # We handle None values by filtering them out before type checking.\n",
        "    column_type_map: Dict[str, type] = {\n",
        "        \"example_id\": str,\n",
        "        \"split\": str,\n",
        "        \"question_text\": str,\n",
        "        \"tables\": list,      # Complex structure (list of dicts)\n",
        "        \"passages\": list,    # Complex structure (list of dicts)\n",
        "        \"answer_type\": str,\n",
        "        \"answer_value\": str, # Raw answers are strings before parsing\n",
        "        \"answer_unit\": str\n",
        "    }\n",
        "\n",
        "    # Extract the set of required column names\n",
        "    required_columns: Set[str] = set(column_type_map.keys())\n",
        "\n",
        "    # Get the actual columns present in the DataFrame\n",
        "    actual_columns: Set[str] = set(df.columns)\n",
        "\n",
        "\n",
        "    # Check for Missing Columns\n",
        "    # Calculate set difference: required - actual\n",
        "    missing = required_columns - actual_columns\n",
        "\n",
        "    # If any required columns are missing, log a critical error and return immediately\n",
        "    # as we cannot validate types for missing columns.\n",
        "    if missing:\n",
        "        report.missing_columns = list(missing)\n",
        "        report.is_valid = False\n",
        "        logger.error(f\"Missing required columns in TAT-QA dataset: {missing}\")\n",
        "        return report\n",
        "\n",
        "\n",
        "    # Check for Extra Columns\n",
        "    # Calculate set difference: actual - required\n",
        "    extra = actual_columns - required_columns\n",
        "\n",
        "    # Log extra columns as warnings (does not invalidate the dataset strictly,\n",
        "    # but indicates schema deviation).\n",
        "    if extra:\n",
        "        report.extra_columns = list(extra)\n",
        "        logger.warning(f\"Extra columns detected in TAT-QA dataset: {extra}\")\n",
        "\n",
        "\n",
        "    # Element-Level Type Validation\n",
        "    # Iterate through each required column to verify data types\n",
        "    for col_name, expected_type in column_type_map.items():\n",
        "        # Extract the series for the current column\n",
        "        series = df[col_name]\n",
        "\n",
        "        # Drop null values (NaN, None) to check only actual data.\n",
        "        # If a column allows nulls (like answer_unit), we only validate the non-null entries.\n",
        "        non_null_series = series.dropna()\n",
        "\n",
        "        # If the column is empty or contains only nulls, we cannot validate types.\n",
        "        # This is technically valid (no invalid types exist).\n",
        "        if non_null_series.empty:\n",
        "            continue\n",
        "\n",
        "        # Select a random sample of up to 5 elements to verify.\n",
        "        # Sampling reduces overhead compared to checking every row, while still\n",
        "        # catching systematic type issues (e.g., integers in a string column).\n",
        "        sample_size = min(5, len(non_null_series))\n",
        "        sample = non_null_series.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        # Iterate through the sample elements\n",
        "        for idx, value in sample.items():\n",
        "            # Check if the value matches the expected native type\n",
        "            if not isinstance(value, expected_type):\n",
        "                # Construct a detailed error message\n",
        "                actual_type_name = type(value).__name__\n",
        "                expected_type_name = expected_type.__name__\n",
        "\n",
        "                # Create the validation error record\n",
        "                error = ValidationError(\n",
        "                    row_index=int(idx), # Cast to int for serializability\n",
        "                    column=col_name,\n",
        "                    error_type=\"dtype_mismatch\",\n",
        "                    message=(\n",
        "                        f\"Column '{col_name}' contains invalid type at index {idx}. \"\n",
        "                        f\"Expected '{expected_type_name}', got '{actual_type_name}'.\"\n",
        "                    ),\n",
        "                    example_id=df.at[idx, \"example_id\"] if \"example_id\" in df.columns else None\n",
        "                )\n",
        "\n",
        "                # Add error to report and log it\n",
        "                report.add_error(error)\n",
        "                logger.error(f\"Type validation failed for column '{col_name}': {error.message}\")\n",
        "\n",
        "                # Break after finding one type error in a column to avoid flooding logs\n",
        "                break\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate content integrity for identifying fields\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_tatqa_identifying_fields(df: pd.DataFrame, report: ValidationReport) -> ValidationReport:\n",
        "    \"\"\"\n",
        "    Validates the integrity of 'example_id', 'split', and 'question_text'.\n",
        "    Checks for nulls, uniqueness within splits, and empty strings.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw TAT-QA DataFrame.\n",
        "        report (ValidationReport): The existing report to append errors to.\n",
        "\n",
        "    Returns:\n",
        "        ValidationReport: The updated validation report.\n",
        "    \"\"\"\n",
        "    # 1. Validate 'example_id'\n",
        "    # Check for null values\n",
        "    null_ids = df[df[\"example_id\"].isnull()]\n",
        "    for idx in null_ids.index:\n",
        "        report.add_error(ValidationError(\n",
        "            row_index=idx,\n",
        "            column=\"example_id\",\n",
        "            error_type=\"null_value\",\n",
        "            message=\"example_id is None or NaN\"\n",
        "        ))\n",
        "\n",
        "    # Check for uniqueness within each split\n",
        "    # Group by split and count example_ids\n",
        "    if \"split\" in df.columns and \"example_id\" in df.columns:\n",
        "        # Filter out nulls before checking duplicates to avoid noise\n",
        "        valid_df = df.dropna(subset=[\"example_id\", \"split\"])\n",
        "\n",
        "        # Find duplicates: boolean mask where True indicates a duplicate\n",
        "        duplicates = valid_df.duplicated(subset=[\"split\", \"example_id\"], keep=False)\n",
        "\n",
        "        if duplicates.any():\n",
        "            dup_rows = valid_df[duplicates]\n",
        "            for idx, row in dup_rows.iterrows():\n",
        "                report.add_error(ValidationError(\n",
        "                    row_index=idx,\n",
        "                    example_id=row[\"example_id\"],\n",
        "                    column=\"example_id\",\n",
        "                    error_type=\"duplicate_id\",\n",
        "                    message=f\"Duplicate example_id '{row['example_id']}' found in split '{row['split']}'\"\n",
        "                ))\n",
        "\n",
        "    # 2. Validate 'split'\n",
        "    # Allowed values set\n",
        "    allowed_splits = {\"train\", \"dev\", \"test\"}\n",
        "\n",
        "    if \"split\" in df.columns:\n",
        "        # Identify rows with invalid split values\n",
        "        # We use apply to check membership for each element\n",
        "        invalid_split_mask = ~df[\"split\"].isin(allowed_splits)\n",
        "        invalid_split_rows = df[invalid_split_mask]\n",
        "\n",
        "        for idx, row in invalid_split_rows.iterrows():\n",
        "            report.add_error(ValidationError(\n",
        "                row_index=idx,\n",
        "                example_id=row.get(\"example_id\"), # Use .get() in case it's null\n",
        "                column=\"split\",\n",
        "                error_type=\"invalid_value\",\n",
        "                message=f\"Invalid split value: '{row['split']}'. Expected one of {allowed_splits}\"\n",
        "            ))\n",
        "\n",
        "    # 3. Validate 'question_text'\n",
        "    if \"question_text\" in df.columns:\n",
        "        # Check for nulls\n",
        "        null_q = df[df[\"question_text\"].isnull()]\n",
        "        for idx in null_q.index:\n",
        "            report.add_error(ValidationError(\n",
        "                row_index=idx,\n",
        "                example_id=df.at[idx, \"example_id\"] if \"example_id\" in df.columns else None,\n",
        "                column=\"question_text\",\n",
        "                error_type=\"null_value\",\n",
        "                message=\"question_text is None\"\n",
        "            ))\n",
        "\n",
        "        # Check for empty or whitespace-only strings\n",
        "        # Ensure we only check strings (exclude the nulls we just found)\n",
        "        non_null_q = df.dropna(subset=[\"question_text\"])\n",
        "        # Strip whitespace and check length\n",
        "        empty_q_mask = non_null_q[\"question_text\"].astype(str).str.strip().str.len() == 0\n",
        "        empty_q_rows = non_null_q[empty_q_mask]\n",
        "\n",
        "        for idx, row in empty_q_rows.iterrows():\n",
        "            report.add_error(ValidationError(\n",
        "                row_index=idx,\n",
        "                example_id=row.get(\"example_id\"),\n",
        "                column=\"question_text\",\n",
        "                error_type=\"empty_string\",\n",
        "                message=\"question_text is empty or whitespace only\"\n",
        "            ))\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate complex column structures\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_tatqa_complex_structures(df: pd.DataFrame, report: ValidationReport) -> ValidationReport:\n",
        "    \"\"\"\n",
        "    Validates the internal structure of 'tables', 'passages', and answer fields.\n",
        "    Enforces schema constraints on nested lists and dictionaries.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw TAT-QA DataFrame.\n",
        "        report (ValidationReport): The existing report to append errors to.\n",
        "\n",
        "    Returns:\n",
        "        ValidationReport: The updated validation report.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to validate a single table dictionary\n",
        "    def validate_single_table(tbl: Any, row_idx: int, ex_id: Optional[str]) -> List[ValidationError]:\n",
        "        errors = []\n",
        "        if not isinstance(tbl, dict):\n",
        "            errors.append(ValidationError(row_idx, \"tables\", \"invalid_type\", f\"Table entry is not a dict: {type(tbl)}\", ex_id))\n",
        "            return errors\n",
        "\n",
        "        # Check required keys\n",
        "        required_keys = {\"table_id\", \"caption\", \"headers\", \"rows\"}\n",
        "        if not required_keys.issubset(tbl.keys()):\n",
        "            missing = required_keys - tbl.keys()\n",
        "            errors.append(ValidationError(row_idx, \"tables\", \"missing_keys\", f\"Table missing keys: {missing}\", ex_id))\n",
        "            return errors\n",
        "\n",
        "        # Validate headers\n",
        "        headers = tbl.get(\"headers\")\n",
        "        if not isinstance(headers, list):\n",
        "            errors.append(ValidationError(row_idx, \"tables\", \"invalid_headers\", \"Headers is not a list\", ex_id))\n",
        "            return errors\n",
        "\n",
        "        # Validate rows\n",
        "        rows = tbl.get(\"rows\")\n",
        "        if not isinstance(rows, list):\n",
        "            errors.append(ValidationError(row_idx, \"tables\", \"invalid_rows\", \"Rows is not a list\", ex_id))\n",
        "            return errors\n",
        "\n",
        "        # Validate row consistency\n",
        "        header_len = len(headers)\n",
        "        for i, r in enumerate(rows):\n",
        "            if not isinstance(r, list):\n",
        "                errors.append(ValidationError(row_idx, \"tables\", \"invalid_row_type\", f\"Row {i} is not a list\", ex_id))\n",
        "                continue\n",
        "            if len(r) != header_len:\n",
        "                errors.append(ValidationError(row_idx, \"tables\", \"row_length_mismatch\",\n",
        "                                              f\"Row {i} length {len(r)} != header length {header_len}\", ex_id))\n",
        "        return errors\n",
        "\n",
        "    # Helper function to validate a single passage dictionary\n",
        "    def validate_single_passage(psg: Any, row_idx: int, ex_id: Optional[str]) -> List[ValidationError]:\n",
        "        errors = []\n",
        "        if not isinstance(psg, dict):\n",
        "            errors.append(ValidationError(row_idx, \"passages\", \"invalid_type\", f\"Passage entry is not a dict: {type(psg)}\", ex_id))\n",
        "            return errors\n",
        "\n",
        "        if \"passage_id\" not in psg or \"text\" not in psg:\n",
        "            errors.append(ValidationError(row_idx, \"passages\", \"missing_keys\", \"Passage missing 'passage_id' or 'text'\", ex_id))\n",
        "            return errors\n",
        "\n",
        "        text = psg.get(\"text\")\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            errors.append(ValidationError(row_idx, \"passages\", \"empty_text\", \"Passage text is empty or not a string\", ex_id))\n",
        "\n",
        "        return errors\n",
        "\n",
        "    # Iterate over rows to validate complex structures\n",
        "    # We use iterrows() here because we need to inspect deep structures which is hard to vectorize\n",
        "    for idx, row in df.iterrows():\n",
        "        ex_id = row.get(\"example_id\")\n",
        "\n",
        "        # 1. Validate 'tables'\n",
        "        tables = row.get(\"tables\")\n",
        "        if not isinstance(tables, list):\n",
        "            report.add_error(ValidationError(idx, \"tables\", \"invalid_type\", f\"Expected list of dicts, got {type(tables)}\", ex_id))\n",
        "        else:\n",
        "            for tbl in tables:\n",
        "                tbl_errors = validate_single_table(tbl, idx, ex_id)\n",
        "                for err in tbl_errors:\n",
        "                    report.add_error(err)\n",
        "\n",
        "        # 2. Validate 'passages'\n",
        "        passages = row.get(\"passages\")\n",
        "        if not isinstance(passages, list):\n",
        "            report.add_error(ValidationError(idx, \"passages\", \"invalid_type\", f\"Expected list of dicts, got {type(passages)}\", ex_id))\n",
        "        else:\n",
        "            for psg in passages:\n",
        "                psg_errors = validate_single_passage(psg, idx, ex_id)\n",
        "                for err in psg_errors:\n",
        "                    report.add_error(err)\n",
        "\n",
        "        # 3. Validate Answer Fields\n",
        "        # answer_type\n",
        "        ans_type = row.get(\"answer_type\")\n",
        "        if not isinstance(ans_type, str):\n",
        "             report.add_error(ValidationError(idx, \"answer_type\", \"invalid_type\", f\"Expected string, got {type(ans_type)}\", ex_id))\n",
        "\n",
        "        # answer_value\n",
        "        ans_val = row.get(\"answer_value\")\n",
        "        if not isinstance(ans_val, str) or not ans_val.strip():\n",
        "             report.add_error(ValidationError(idx, \"answer_value\", \"empty_or_invalid\", \"answer_value is empty or not a string\", ex_id))\n",
        "\n",
        "        # answer_unit (can be None or string)\n",
        "        ans_unit = row.get(\"answer_unit\")\n",
        "        if ans_unit is not None and not isinstance(ans_unit, str):\n",
        "             report.add_error(ValidationError(idx, \"answer_unit\", \"invalid_type\", f\"Expected string or None, got {type(ans_unit)}\", ex_id))\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_tatqa_dataset(tatqa_raw_df: pd.DataFrame) -> ValidationReport:\n",
        "    \"\"\"\n",
        "    Orchestrates the full validation pipeline for the TAT-QA raw dataset.\n",
        "\n",
        "    This function executes three granular validation steps:\n",
        "    1. Column presence and high-level type check.\n",
        "    2. Content integrity check for identifying fields (IDs, splits, questions).\n",
        "    3. Structural validation for complex nested fields (tables, passages, answers).\n",
        "\n",
        "    Args:\n",
        "        tatqa_raw_df (pd.DataFrame): The raw input DataFrame for TAT-QA.\n",
        "\n",
        "    Returns:\n",
        "        ValidationReport: A comprehensive report detailing validity status and any errors found.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting TAT-QA dataset validation...\")\n",
        "\n",
        "    # Step 1: Column Schema Validation\n",
        "    report = validate_tatqa_columns(tatqa_raw_df)\n",
        "\n",
        "    # If critical columns are missing, we stop early to avoid KeyErrors in subsequent steps\n",
        "    if report.missing_columns:\n",
        "        logger.error(\"Critical columns missing. Aborting further validation.\")\n",
        "        return report\n",
        "\n",
        "    # Step 2: Identifying Fields Validation\n",
        "    report = validate_tatqa_identifying_fields(tatqa_raw_df, report)\n",
        "\n",
        "    # Step 3: Complex Structure Validation\n",
        "    report = validate_tatqa_complex_structures(tatqa_raw_df, report)\n",
        "\n",
        "    if report.is_valid:\n",
        "        logger.info(\"TAT-QA dataset validation passed successfully.\")\n",
        "    else:\n",
        "        logger.warning(f\"TAT-QA dataset validation failed with {len(report.errors)} errors.\")\n",
        "\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "Otn_Vy1w4Vqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 – Validate finqa_raw_df Schema and Content Quality\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate finqa_raw_df Schema and Content Quality\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Validate column presence and types\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_finqa_columns(df: pd.DataFrame) -> ValidationReport:\n",
        "    \"\"\"\n",
        "    Validates that the Fin-QA DataFrame contains exactly the required columns and that\n",
        "    their element-level data types are correct based on a random sample.\n",
        "\n",
        "    This function performs two levels of validation:\n",
        "    1. Schema Existence: Checks if all required columns are present.\n",
        "    2. Type Integrity: Samples non-null values from each column to ensure they match\n",
        "       the expected Python native type. This detects mixed-type columns that Pandas\n",
        "       might label generically as 'object'.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw Fin-QA input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        ValidationReport: A report containing missing/extra columns and type errors.\n",
        "    \"\"\"\n",
        "    # Initialize the validation report object\n",
        "    report = ValidationReport()\n",
        "\n",
        "    # Define the mapping of required columns to their expected native Python types\n",
        "    # Fin-QA schema mirrors TAT-QA but is applied to financial report data.\n",
        "    column_type_map: Dict[str, type] = {\n",
        "        \"example_id\": str,\n",
        "        \"split\": str,\n",
        "        \"question_text\": str,\n",
        "        \"tables\": list,      # Complex structure (list of dicts)\n",
        "        \"passages\": list,    # Complex structure (list of dicts)\n",
        "        \"answer_type\": str,\n",
        "        \"answer_value\": str, # Raw answers are strings before parsing\n",
        "        \"answer_unit\": str\n",
        "    }\n",
        "\n",
        "    # Extract the set of required column names\n",
        "    required_columns: Set[str] = set(column_type_map.keys())\n",
        "\n",
        "    # Get the actual columns present in the DataFrame\n",
        "    actual_columns: Set[str] = set(df.columns)\n",
        "\n",
        "\n",
        "    # Step 1: Check for Missing Columns\n",
        "    # Calculate set difference: required - actual\n",
        "    missing = required_columns - actual_columns\n",
        "\n",
        "    # If any required columns are missing, log a critical error and return immediately\n",
        "    if missing:\n",
        "        report.missing_columns = list(missing)\n",
        "        report.is_valid = False\n",
        "        logger.error(f\"Missing required columns in Fin-QA dataset: {missing}\")\n",
        "        return report\n",
        "\n",
        "\n",
        "    # Step 2: Check for Extra Columns\n",
        "    # Calculate set difference: actual - required\n",
        "    extra = actual_columns - required_columns\n",
        "\n",
        "    # Log extra columns as warnings. Fin-QA often has 'program' or 'table_ori'.\n",
        "    if extra:\n",
        "        report.extra_columns = list(extra)\n",
        "        logger.warning(f\"Extra columns detected in Fin-QA dataset: {extra}\")\n",
        "\n",
        "\n",
        "    # Step 3: Element-Level Type Validation\n",
        "    # Iterate through each required column to verify data types\n",
        "    for col_name, expected_type in column_type_map.items():\n",
        "        # Extract the series for the current column\n",
        "        series = df[col_name]\n",
        "\n",
        "        # Drop null values (NaN, None) to check only actual data.\n",
        "        non_null_series = series.dropna()\n",
        "\n",
        "        # If the column is empty or contains only nulls, skip validation.\n",
        "        if non_null_series.empty:\n",
        "            continue\n",
        "\n",
        "        # Select a random sample of up to 5 elements to verify.\n",
        "        sample_size = min(5, len(non_null_series))\n",
        "        sample = non_null_series.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        # Iterate through the sample elements\n",
        "        for idx, value in sample.items():\n",
        "            # Check if the value matches the expected native type\n",
        "            if not isinstance(value, expected_type):\n",
        "                # Construct a detailed error message\n",
        "                actual_type_name = type(value).__name__\n",
        "                expected_type_name = expected_type.__name__\n",
        "\n",
        "                # Create the validation error record\n",
        "                error = ValidationError(\n",
        "                    row_index=int(idx),\n",
        "                    column=col_name,\n",
        "                    error_type=\"dtype_mismatch\",\n",
        "                    message=(\n",
        "                        f\"Column '{col_name}' contains invalid type at index {idx}. \"\n",
        "                        f\"Expected '{expected_type_name}', got '{actual_type_name}'.\"\n",
        "                    ),\n",
        "                    example_id=df.at[idx, \"example_id\"] if \"example_id\" in df.columns else None\n",
        "                )\n",
        "\n",
        "                # Add error to report and log it\n",
        "                report.add_error(error)\n",
        "                logger.error(f\"Type validation failed for column '{col_name}': {error.message}\")\n",
        "\n",
        "                # Break after finding one type error in a column\n",
        "                break\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate content integrity for identifying fields and answer types\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_finqa_content_integrity(df: pd.DataFrame, report: ValidationReport) -> ValidationReport:\n",
        "    \"\"\"\n",
        "    Validates 'example_id', 'split', 'answer_type', and 'answer_value' integrity.\n",
        "    Checks for uniqueness, valid splits, numeric answer types, and parseable answer values.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw Fin-QA DataFrame.\n",
        "        report (ValidationReport): The existing report to append errors to.\n",
        "\n",
        "    Returns:\n",
        "        ValidationReport: The updated validation report.\n",
        "    \"\"\"\n",
        "    # 1. Validate 'example_id'\n",
        "    null_ids = df[df[\"example_id\"].isnull()]\n",
        "    for idx in null_ids.index:\n",
        "        report.add_error(ValidationError(idx, \"example_id\", \"null_value\", \"example_id is None or NaN\"))\n",
        "\n",
        "    if \"split\" in df.columns and \"example_id\" in df.columns:\n",
        "        valid_df = df.dropna(subset=[\"example_id\", \"split\"])\n",
        "        duplicates = valid_df.duplicated(subset=[\"split\", \"example_id\"], keep=False)\n",
        "        if duplicates.any():\n",
        "            dup_rows = valid_df[duplicates]\n",
        "            for idx, row in dup_rows.iterrows():\n",
        "                report.add_error(ValidationError(\n",
        "                    row_index=idx,\n",
        "                    example_id=row[\"example_id\"],\n",
        "                    column=\"example_id\",\n",
        "                    error_type=\"duplicate_id\",\n",
        "                    message=f\"Duplicate example_id '{row['example_id']}' found in split '{row['split']}'\"\n",
        "                ))\n",
        "\n",
        "    # 2. Validate 'split'\n",
        "    allowed_splits = {\"train\", \"dev\", \"test\"}\n",
        "    if \"split\" in df.columns:\n",
        "        invalid_split_mask = ~df[\"split\"].isin(allowed_splits)\n",
        "        for idx, row in df[invalid_split_mask].iterrows():\n",
        "            report.add_error(ValidationError(\n",
        "                row_index=idx,\n",
        "                example_id=row.get(\"example_id\"),\n",
        "                column=\"split\",\n",
        "                error_type=\"invalid_value\",\n",
        "                message=f\"Invalid split value: '{row['split']}'\"\n",
        "            ))\n",
        "\n",
        "    # 3. Validate 'answer_type'\n",
        "    # Fin-QA is predominantly numeric. We expect 'number' or similar.\n",
        "    if \"answer_type\" in df.columns:\n",
        "        # Check for unexpected types. We don't strictly fail, but we log non-number types if they are rare/unexpected.\n",
        "        # Assuming 'number' is the standard.\n",
        "        non_number_mask = df[\"answer_type\"] != \"number\"\n",
        "        # This might be too strict if the dataset has text answers, but per instructions \"Often 'number' in Fin-QA\".\n",
        "        # We will just validate it is a string.\n",
        "        invalid_type_mask = df[\"answer_type\"].apply(lambda x: not isinstance(x, str))\n",
        "        for idx, row in df[invalid_type_mask].iterrows():\n",
        "             report.add_error(ValidationError(\n",
        "                row_index=idx,\n",
        "                example_id=row.get(\"example_id\"),\n",
        "                column=\"answer_type\",\n",
        "                error_type=\"invalid_type\",\n",
        "                message=f\"answer_type is not a string: {type(row['answer_type'])}\"\n",
        "            ))\n",
        "\n",
        "    # 4. Validate 'answer_value' parseability\n",
        "    # We attempt to parse as float to flag potential data quality issues early.\n",
        "    if \"answer_value\" in df.columns:\n",
        "        def is_parseable(val: Any) -> bool:\n",
        "            if not isinstance(val, str): return False\n",
        "            # Remove commas, handle percent\n",
        "            clean_val = val.replace(',', '').replace('%', '').strip()\n",
        "            try:\n",
        "                float(clean_val)\n",
        "                return True\n",
        "            except ValueError:\n",
        "                return False\n",
        "\n",
        "        # We only check rows where answer_type is 'number' to avoid false positives on text answers\n",
        "        number_rows = df[df[\"answer_type\"] == \"number\"]\n",
        "        unparseable_mask = ~number_rows[\"answer_value\"].apply(is_parseable)\n",
        "\n",
        "        for idx, row in number_rows[unparseable_mask].iterrows():\n",
        "            # We log this as a warning/error but don't necessarily fail the whole pipeline\n",
        "            # as Task 5 will handle cleansing. However, validation should report it.\n",
        "            report.add_error(ValidationError(\n",
        "                row_index=idx,\n",
        "                example_id=row.get(\"example_id\"),\n",
        "                column=\"answer_value\",\n",
        "                error_type=\"unparseable_number\",\n",
        "                message=f\"answer_value '{row['answer_value']}' could not be parsed as float\"\n",
        "            ))\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate complex column structures\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_finqa_complex_structures(df: pd.DataFrame, report: ValidationReport) -> ValidationReport:\n",
        "    \"\"\"\n",
        "    Validates the internal structure of 'tables' and 'passages' columns for the Fin-QA dataset.\n",
        "\n",
        "    This function iterates through each row of the DataFrame to enforce strict schema constraints\n",
        "    on nested list-of-dictionary structures. It ensures that 'tables' and 'passages' contain\n",
        "    the required keys and correct data types, logging any deviations as validation errors.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw Fin-QA input DataFrame containing 'tables' and 'passages' columns.\n",
        "        report (ValidationReport): An existing ValidationReport object to which new errors will be appended.\n",
        "\n",
        "    Returns:\n",
        "        ValidationReport: The updated validation report containing any structural errors found.\n",
        "    \"\"\"\n",
        "\n",
        "    def validate_single_table(tbl: Any, row_idx: int, ex_id: Optional[str]) -> List[ValidationError]:\n",
        "        \"\"\"\n",
        "        Validates the structure of a single table dictionary.\n",
        "\n",
        "        Args:\n",
        "            tbl (Any): The table object to validate (expected to be a dict).\n",
        "            row_idx (int): The index of the row in the DataFrame.\n",
        "            ex_id (Optional[str]): The example_id associated with the row.\n",
        "\n",
        "        Returns:\n",
        "            List[ValidationError]: A list of validation errors found within the table structure.\n",
        "        \"\"\"\n",
        "        # Initialize an empty list to collect errors for this specific table\n",
        "        errors: List[ValidationError] = []\n",
        "\n",
        "        # Check if the table entry is a dictionary\n",
        "        if not isinstance(tbl, dict):\n",
        "            # Log error if type is incorrect\n",
        "            errors.append(ValidationError(\n",
        "                row_index=row_idx,\n",
        "                column=\"tables\",\n",
        "                error_type=\"invalid_type\",\n",
        "                message=f\"Table entry is not a dict: {type(tbl)}\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "            return errors\n",
        "\n",
        "        # Define the set of required keys for a table object\n",
        "        required_keys: Set[str] = {\"table_id\", \"caption\", \"headers\", \"rows\"}\n",
        "\n",
        "        # Check if all required keys are present in the table dictionary\n",
        "        if not required_keys.issubset(tbl.keys()):\n",
        "            # Identify missing keys\n",
        "            missing = required_keys - tbl.keys()\n",
        "            # Log error for missing keys\n",
        "            errors.append(ValidationError(\n",
        "                row_index=row_idx,\n",
        "                column=\"tables\",\n",
        "                error_type=\"missing_keys\",\n",
        "                message=f\"Table missing keys: {missing}\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "            return errors\n",
        "\n",
        "        # Retrieve and validate the 'headers' field\n",
        "        headers = tbl.get(\"headers\")\n",
        "        if not isinstance(headers, list):\n",
        "            # Log error if headers is not a list\n",
        "            errors.append(ValidationError(\n",
        "                row_index=row_idx,\n",
        "                column=\"tables\",\n",
        "                error_type=\"invalid_headers\",\n",
        "                message=\"Headers is not a list\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "            return errors\n",
        "\n",
        "        # Retrieve and validate the 'rows' field\n",
        "        rows = tbl.get(\"rows\")\n",
        "        if not isinstance(rows, list):\n",
        "            # Log error if rows is not a list\n",
        "            errors.append(ValidationError(\n",
        "                row_index=row_idx,\n",
        "                column=\"tables\",\n",
        "                error_type=\"invalid_rows\",\n",
        "                message=\"Rows is not a list\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "            return errors\n",
        "\n",
        "        # Determine the expected length of each row based on the headers\n",
        "        header_len = len(headers)\n",
        "\n",
        "        # Iterate through each row to validate its structure and length\n",
        "        for i, r in enumerate(rows):\n",
        "            # Check if the row is a list\n",
        "            if not isinstance(r, list):\n",
        "                # Log error if a row is not a list\n",
        "                errors.append(ValidationError(\n",
        "                    row_index=row_idx,\n",
        "                    column=\"tables\",\n",
        "                    error_type=\"invalid_row_type\",\n",
        "                    message=f\"Row {i} is not a list\",\n",
        "                    example_id=ex_id\n",
        "                ))\n",
        "                continue\n",
        "\n",
        "            # Check if the row length matches the header length\n",
        "            if len(r) != header_len:\n",
        "                # Log error for row length mismatch\n",
        "                errors.append(ValidationError(\n",
        "                    row_index=row_idx,\n",
        "                    column=\"tables\",\n",
        "                    error_type=\"row_length_mismatch\",\n",
        "                    message=f\"Row {i} length {len(r)} != header length {header_len}\",\n",
        "                    example_id=ex_id\n",
        "                ))\n",
        "\n",
        "        return errors\n",
        "\n",
        "    def validate_single_passage(psg: Any, row_idx: int, ex_id: Optional[str]) -> List[ValidationError]:\n",
        "        \"\"\"\n",
        "        Validates the structure of a single passage dictionary.\n",
        "\n",
        "        Args:\n",
        "            psg (Any): The passage object to validate (expected to be a dict).\n",
        "            row_idx (int): The index of the row in the DataFrame.\n",
        "            ex_id (Optional[str]): The example_id associated with the row.\n",
        "\n",
        "        Returns:\n",
        "            List[ValidationError]: A list of validation errors found within the passage structure.\n",
        "        \"\"\"\n",
        "        # Initialize an empty list to collect errors for this specific passage\n",
        "        errors: List[ValidationError] = []\n",
        "\n",
        "        # Check if the passage entry is a dictionary\n",
        "        if not isinstance(psg, dict):\n",
        "            # Log error if type is incorrect\n",
        "            errors.append(ValidationError(\n",
        "                row_index=row_idx,\n",
        "                column=\"passages\",\n",
        "                error_type=\"invalid_type\",\n",
        "                message=f\"Passage entry is not a dict: {type(psg)}\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "            return errors\n",
        "\n",
        "        # Check for required keys 'passage_id' and 'text'\n",
        "        if \"passage_id\" not in psg or \"text\" not in psg:\n",
        "            # Log error for missing keys\n",
        "            errors.append(ValidationError(\n",
        "                row_index=row_idx,\n",
        "                column=\"passages\",\n",
        "                error_type=\"missing_keys\",\n",
        "                message=\"Passage missing 'passage_id' or 'text'\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "            return errors\n",
        "\n",
        "        # Retrieve and validate the 'text' field\n",
        "        text = psg.get(\"text\")\n",
        "        # Ensure text is a string and is not empty or whitespace-only\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            # Log error for invalid text content\n",
        "            errors.append(ValidationError(\n",
        "                row_index=row_idx,\n",
        "                column=\"passages\",\n",
        "                error_type=\"empty_text\",\n",
        "                message=\"Passage text is empty or not a string\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "\n",
        "        return errors\n",
        "\n",
        "    # Iterate over each row in the DataFrame to validate complex structures\n",
        "    for idx, row in df.iterrows():\n",
        "        # Retrieve the example_id for logging purposes\n",
        "        ex_id = row.get(\"example_id\")\n",
        "\n",
        "        # 1. Validate 'tables' column\n",
        "        tables = row.get(\"tables\")\n",
        "        # Check if 'tables' is a list\n",
        "        if not isinstance(tables, list):\n",
        "            # Log error if 'tables' is not a list\n",
        "            report.add_error(ValidationError(\n",
        "                row_index=idx,\n",
        "                column=\"tables\",\n",
        "                error_type=\"invalid_type\",\n",
        "                message=f\"Expected list of dicts, got {type(tables)}\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "        else:\n",
        "            # Iterate through each table in the list and validate it\n",
        "            for tbl in tables:\n",
        "                tbl_errors = validate_single_table(tbl, idx, ex_id)\n",
        "                # Add any found errors to the main report\n",
        "                for err in tbl_errors:\n",
        "                    report.add_error(err)\n",
        "\n",
        "        # 2. Validate 'passages' column\n",
        "        passages = row.get(\"passages\")\n",
        "        # Check if 'passages' is a list\n",
        "        if not isinstance(passages, list):\n",
        "            # Log error if 'passages' is not a list\n",
        "            report.add_error(ValidationError(\n",
        "                row_index=idx,\n",
        "                column=\"passages\",\n",
        "                error_type=\"invalid_type\",\n",
        "                message=f\"Expected list of dicts, got {type(passages)}\",\n",
        "                example_id=ex_id\n",
        "            ))\n",
        "        else:\n",
        "            # Iterate through each passage in the list and validate it\n",
        "            for psg in passages:\n",
        "                psg_errors = validate_single_passage(psg, idx, ex_id)\n",
        "                # Add any found errors to the main report\n",
        "                for err in psg_errors:\n",
        "                    report.add_error(err)\n",
        "\n",
        "    return report\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_finqa_dataset(finqa_raw_df: pd.DataFrame) -> ValidationReport:\n",
        "    \"\"\"\n",
        "    Orchestrates the full validation pipeline for the Fin-QA raw dataset.\n",
        "\n",
        "    Executes granular validation steps:\n",
        "    1. Column presence and high-level type check.\n",
        "    2. Content integrity check for identifying fields and numeric answer validity.\n",
        "    3. Structural validation for complex nested fields (tables, passages).\n",
        "\n",
        "    Args:\n",
        "        finqa_raw_df (pd.DataFrame): The raw input DataFrame for Fin-QA.\n",
        "\n",
        "    Returns:\n",
        "        ValidationReport: A comprehensive report detailing validity status and any errors found.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Fin-QA dataset validation...\")\n",
        "\n",
        "    # Step 1: Column Schema Validation\n",
        "    report = validate_finqa_columns(finqa_raw_df)\n",
        "\n",
        "    if report.missing_columns:\n",
        "        logger.error(\"Critical columns missing in Fin-QA. Aborting further validation.\")\n",
        "        return report\n",
        "\n",
        "    # Step 2: Identifying Fields and Content Validation\n",
        "    report = validate_finqa_content_integrity(finqa_raw_df, report)\n",
        "\n",
        "    # Step 3: Complex Structure Validation\n",
        "    report = validate_finqa_complex_structures(finqa_raw_df, report)\n",
        "\n",
        "    if report.is_valid:\n",
        "        logger.info(\"Fin-QA dataset validation passed successfully.\")\n",
        "    else:\n",
        "        logger.warning(f\"Fin-QA dataset validation failed with {len(report.errors)} errors.\")\n",
        "\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "482myPOZNCwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 – Validate study_config Completeness and Consistency\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Validate study_config Completeness and Consistency\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Inventory all <REQUIRED_BY_IMPLEMENTER> placeholders\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def inventory_placeholders(config: Dict[str, Any], path: str = \"\") -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursively traverses the configuration dictionary to identify all keys\n",
        "    that still hold the placeholder value \"<REQUIRED_BY_IMPLEMENTER>\".\n",
        "\n",
        "    This function handles nested dictionaries and lists, constructing a\n",
        "    dot-separated path for each placeholder found. List indices are\n",
        "    represented as `[index]`.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to inspect.\n",
        "        path (str): The current path to the config node (used for recursion).\n",
        "                    Defaults to empty string.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of paths pointing to unresolved placeholders.\n",
        "                   Example: [\"offline_corpus_config.tokenization_scheme.name\"]\n",
        "    \"\"\"\n",
        "    placeholders = []\n",
        "\n",
        "    for key, value in config.items():\n",
        "        # Construct the current path\n",
        "        current_path = f\"{path}.{key}\" if path else key\n",
        "\n",
        "        if isinstance(value, dict):\n",
        "            # Recurse into nested dictionaries\n",
        "            placeholders.extend(inventory_placeholders(value, current_path))\n",
        "        elif isinstance(value, list):\n",
        "            # Iterate through lists (e.g., list of model configs)\n",
        "            for i, item in enumerate(value):\n",
        "                list_item_path = f\"{current_path}[{i}]\"\n",
        "                if isinstance(item, dict):\n",
        "                    placeholders.extend(inventory_placeholders(item, list_item_path))\n",
        "                elif item == \"<REQUIRED_BY_IMPLEMENTER>\":\n",
        "                    placeholders.append(list_item_path)\n",
        "        elif value == \"<REQUIRED_BY_IMPLEMENTER>\":\n",
        "            # Found a placeholder\n",
        "            placeholders.append(current_path)\n",
        "\n",
        "    return placeholders\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Assign concrete values to required parameters\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def resolve_placeholders(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Assigns concrete, scientifically justifiable default values to all\n",
        "    required parameters identified as placeholders.\n",
        "\n",
        "    This function mutates the configuration dictionary in-place (or returns the mutated version)\n",
        "    to ensure the pipeline is executable. It uses a robust path-based setter to handle\n",
        "    nested keys and list indices.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary containing placeholders.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The fully resolved configuration dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the mapping of config paths to concrete values based on the paper's context\n",
        "    defaults = {\n",
        "        \"offline_corpus_config.tokenization_scheme.name\": \"cl100k_base\",\n",
        "        \"hard_prompt_compression_config.phrase_grouping.parser_library\": \"spacy_en_core_web_trf\",\n",
        "        \"hard_prompt_compression_config.phrase_score_aggregation.chosen_method\": \"mean\",\n",
        "        \"hard_prompt_compression_config.prompt_token_budget\": 1500,\n",
        "        \"ngram_abbreviation_config.ngram_length.default\": 2,\n",
        "        \"ngram_abbreviation_config.dictionary_size_K.default\": 100,\n",
        "        \"numeric_quantization_config.uniform_integer.bit_width_b\": 8,\n",
        "        \"numeric_quantization_config.kmeans_based.num_clusters_k\": 16,\n",
        "        \"preprocessing_config.table_serialization.method\": \"markdown\",\n",
        "        \"llm_config.decoding_parameters.temperature\": 0.0,\n",
        "        \"llm_config.decoding_parameters.top_p\": 1.0,\n",
        "        \"llm_config.decoding_parameters.max_tokens\": 256\n",
        "    }\n",
        "\n",
        "    def set_value_by_path(cfg: Dict[str, Any], path: str, value: Any) -> None:\n",
        "        \"\"\"\n",
        "        Helper to set a value in a nested dict using a dot-separated path.\n",
        "        Handles list indices like 'key[0]'.\n",
        "\n",
        "        Args:\n",
        "            cfg (Dict[str, Any]): The dictionary to modify.\n",
        "            path (str): The dot-separated path to the key.\n",
        "            value (Any): The value to set.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If a path segment cannot be resolved.\n",
        "            IndexError: If a list index is out of bounds.\n",
        "            ValueError: If a path segment is invalid.\n",
        "        \"\"\"\n",
        "        keys = path.split('.')\n",
        "        current = cfg\n",
        "\n",
        "        # Traverse to the parent of the target key\n",
        "        for key in keys[:-1]:\n",
        "            if '[' in key and ']' in key:\n",
        "                # Handle list index: \"key[index]\"\n",
        "                k, idx_str = key[:-1].split('[')\n",
        "                idx = int(idx_str)\n",
        "                current = current[k][idx]\n",
        "            else:\n",
        "                current = current[key]\n",
        "\n",
        "        # Set the value at the target key\n",
        "        last_key = keys[-1]\n",
        "        if '[' in last_key and ']' in last_key:\n",
        "             k, idx_str = last_key[:-1].split('[')\n",
        "             idx = int(idx_str)\n",
        "             current[k][idx] = value\n",
        "        else:\n",
        "            current[last_key] = value\n",
        "\n",
        "    # Apply defaults\n",
        "    for path, value in defaults.items():\n",
        "        try:\n",
        "            set_value_by_path(config, path, value)\n",
        "            logger.info(f\"Resolved placeholder '{path}' to '{value}'\")\n",
        "        except (KeyError, IndexError, ValueError) as e:\n",
        "            logger.warning(f\"Could not resolve path '{path}': {e}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Validate LLM access and API credentials\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_llm_config(config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Validates that LLM configurations are structurally sound and that\n",
        "    API credentials are not left as default placeholders.\n",
        "\n",
        "    This function checks:\n",
        "    1. API credentials are not the default placeholder string.\n",
        "    2. Scorer LLMs are defined and have valid model names.\n",
        "    3. Target LLMs are defined and have valid model names.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The full study configuration.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if LLM config is valid, False otherwise.\n",
        "    \"\"\"\n",
        "    llm_config = config.get(\"llm_config\", {})\n",
        "    credentials = llm_config.get(\"api_credentials\", {})\n",
        "\n",
        "    is_valid = True\n",
        "\n",
        "    # Check credentials\n",
        "    for key, value in credentials.items():\n",
        "        if value == \"<EXTERNAL_SECRET_NOT_IN_PAPER>\":\n",
        "            logger.warning(f\"Credential '{key}' is still set to placeholder. \"\n",
        "                           \"Ensure environment variables or secrets are loaded in production.\")\n",
        "            # In a strict check, we might set is_valid = False, but for this exercise\n",
        "            # we assume secrets are injected at runtime. We flag it.\n",
        "\n",
        "    # Check Scorer LLMs\n",
        "    scorers = llm_config.get(\"scorer_llm_options\", [])\n",
        "    if not scorers:\n",
        "        logger.error(\"No scorer LLMs defined.\")\n",
        "        is_valid = False\n",
        "    for i, scorer in enumerate(scorers):\n",
        "        if not scorer.get(\"model_name\"):\n",
        "            logger.error(f\"Scorer definition at index {i} missing model_name: {scorer}\")\n",
        "            is_valid = False\n",
        "\n",
        "    # Check Target LLMs\n",
        "    targets = llm_config.get(\"target_llms_for_evaluation\", [])\n",
        "    if not targets:\n",
        "        logger.error(\"No target LLMs defined.\")\n",
        "        is_valid = False\n",
        "    for i, target in enumerate(targets):\n",
        "        if not target.get(\"model_name\"):\n",
        "            logger.error(f\"Target definition at index {i} missing model_name: {target}\")\n",
        "            is_valid = False\n",
        "\n",
        "    return is_valid\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_and_update_study_config(study_config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation and resolution of the study configuration.\n",
        "\n",
        "    This function executes the following steps:\n",
        "    1. Inventories all placeholders to identify missing configurations.\n",
        "    2. Resolves placeholders with concrete default values.\n",
        "    3. Validates the structural integrity of LLM configurations.\n",
        "\n",
        "    Args:\n",
        "        study_config (Dict[str, Any]): The initial configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The validated and fully resolved configuration dictionary.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting study_config validation...\")\n",
        "\n",
        "    # Step 1: Inventory\n",
        "    placeholders = inventory_placeholders(study_config)\n",
        "    if placeholders:\n",
        "        logger.info(f\"Found {len(placeholders)} placeholders to resolve.\")\n",
        "    else:\n",
        "        logger.info(\"No placeholders found.\")\n",
        "\n",
        "    # Step 2: Resolve\n",
        "    # We resolve regardless of whether inventory found them, to ensure defaults are enforced\n",
        "    # if the keys exist.\n",
        "    resolved_config = resolve_placeholders(study_config)\n",
        "\n",
        "    # Verify no placeholders remain\n",
        "    remaining_placeholders = inventory_placeholders(resolved_config)\n",
        "    if remaining_placeholders:\n",
        "        logger.error(f\"Unresolved placeholders remain: {remaining_placeholders}\")\n",
        "        # In a strict pipeline, we might raise an error here.\n",
        "\n",
        "    # Step 3: LLM Validation\n",
        "    if not validate_llm_config(resolved_config):\n",
        "        logger.error(\"LLM configuration validation failed.\")\n",
        "    else:\n",
        "        logger.info(\"LLM configuration validation passed.\")\n",
        "\n",
        "    logger.info(\"study_config validation complete.\")\n",
        "    return resolved_config\n"
      ],
      "metadata": {
        "id": "VM2I5Hp4Oxdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 – Cleanse and Handle Missing Entries in tatqa_raw_df\n",
        "\n",
        "@dataclass\n",
        "class CleansingLog:\n",
        "    \"\"\"\n",
        "    Dataclass to track the results of the cleansing process.\n",
        "\n",
        "    Attributes:\n",
        "        initial_rows (int): Number of rows in the raw DataFrame.\n",
        "        rows_dropped_missing_id (int): Rows dropped due to missing/null example_id.\n",
        "        rows_dropped_duplicate_id (int): Rows dropped due to duplicate example_id within a split.\n",
        "        rows_dropped_empty_question (int): Rows dropped due to empty/whitespace-only question_text.\n",
        "        rows_dropped_invalid_tables (int): Rows dropped because 'tables' was not a list or was empty.\n",
        "        rows_dropped_no_valid_content (int): Rows dropped because they had neither valid tables nor valid passages after repair.\n",
        "        final_rows (int): Number of rows in the cleansed DataFrame.\n",
        "        dropped_ids (List[str]): List of example_ids that were dropped.\n",
        "    \"\"\"\n",
        "    initial_rows: int = 0\n",
        "    rows_dropped_missing_id: int = 0\n",
        "    rows_dropped_duplicate_id: int = 0\n",
        "    rows_dropped_empty_question: int = 0\n",
        "    rows_dropped_invalid_tables: int = 0\n",
        "    rows_dropped_no_valid_content: int = 0\n",
        "    final_rows: int = 0\n",
        "    dropped_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Cleanse and Handle Missing Entries in tatqa_raw_df\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Drop rows with critical missing fields\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def drop_critical_missing_rows(df: pd.DataFrame, log: CleansingLog) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Drops rows from the DataFrame that are missing critical identifying information\n",
        "    or essential content fields required for downstream processing.\n",
        "\n",
        "    Criteria for dropping:\n",
        "    1. `example_id` is Null or NaN.\n",
        "    2. `example_id` is a duplicate within its `split`.\n",
        "    3. `question_text` is Null, empty, or whitespace-only.\n",
        "    4. `tables` is Null, not a list, or an empty list.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw TAT-QA DataFrame.\n",
        "        log (CleansingLog): The logging object to update with drop statistics.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered DataFrame with critical issues resolved.\n",
        "    \"\"\"\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # 1. Drop missing example_id\n",
        "    # We capture indices to drop to ensure we can log them if needed,\n",
        "    # but for bulk operations boolean indexing is faster.\n",
        "    missing_id_mask = df[\"example_id\"].isnull()\n",
        "    log.rows_dropped_missing_id = missing_id_mask.sum()\n",
        "    df = df[~missing_id_mask].copy()\n",
        "\n",
        "    # 2. Drop duplicate example_id within split\n",
        "    # We keep the first occurrence and drop subsequent ones.\n",
        "    # Note: We assume 'split' exists and is valid (validated in Task 1).\n",
        "    # If 'split' is missing, we treat global uniqueness or just ignore split grouping for those rows.\n",
        "    # Here we strictly group by split.\n",
        "    duplicate_mask = df.duplicated(subset=[\"split\", \"example_id\"], keep=\"first\")\n",
        "\n",
        "    # Log dropped IDs for duplicates\n",
        "    dropped_dups = df[duplicate_mask][\"example_id\"].tolist()\n",
        "    log.dropped_ids.extend(dropped_dups)\n",
        "    log.rows_dropped_duplicate_id = len(dropped_dups)\n",
        "\n",
        "    df = df[~duplicate_mask].copy()\n",
        "\n",
        "    # 3. Drop empty/whitespace question_text\n",
        "    # Ensure string type before stripping\n",
        "    # We handle NaN questions here as well\n",
        "    def is_valid_question(q: Any) -> bool:\n",
        "        if not isinstance(q, str):\n",
        "            return False\n",
        "        return len(q.strip()) > 0\n",
        "\n",
        "    valid_question_mask = df[\"question_text\"].apply(is_valid_question)\n",
        "    dropped_questions = df[~valid_question_mask][\"example_id\"].tolist()\n",
        "    log.dropped_ids.extend(dropped_questions)\n",
        "    log.rows_dropped_empty_question = len(dropped_questions)\n",
        "\n",
        "    df = df[valid_question_mask].copy()\n",
        "\n",
        "    # 4. Drop invalid 'tables' field\n",
        "    # Must be a non-empty list\n",
        "    def is_valid_tables_field(t: Any) -> bool:\n",
        "        if not isinstance(t, list):\n",
        "            return False\n",
        "        return len(t) > 0\n",
        "\n",
        "    valid_tables_mask = df[\"tables\"].apply(is_valid_tables_field)\n",
        "    dropped_tables = df[~valid_tables_mask][\"example_id\"].tolist()\n",
        "    log.dropped_ids.extend(dropped_tables)\n",
        "    log.rows_dropped_invalid_tables = len(dropped_tables)\n",
        "\n",
        "    df = df[valid_tables_mask].copy()\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Repair or drop malformed table entries\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def repair_malformed_tables(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Iterates through the 'tables' column of each row to validate and repair individual table structures.\n",
        "\n",
        "    Repair Logic:\n",
        "    - A table is discarded if it lacks 'headers' (list of strings) or 'rows' (list of lists).\n",
        "    - Rows within a table are repaired:\n",
        "        - If a row is longer than headers, it is truncated.\n",
        "        - If a row is shorter than headers, it is padded with empty strings (to preserve structure).\n",
        "        - If a row is not a list, it is discarded.\n",
        "    - If a table ends up with no valid rows, it is discarded.\n",
        "    - If an example ends up with no valid tables after repairs, the 'tables' field becomes empty\n",
        "      (which will be caught in the final cleanup step).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame filtered from Step 1.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with repaired 'tables' structures.\n",
        "    \"\"\"\n",
        "\n",
        "    def repair_single_row_tables(tables_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        valid_tables = []\n",
        "\n",
        "        for tbl in tables_list:\n",
        "            # Basic type check (should pass due to Step 1, but defensive coding)\n",
        "            if not isinstance(tbl, dict):\n",
        "                continue\n",
        "\n",
        "            headers = tbl.get(\"headers\")\n",
        "            rows = tbl.get(\"rows\")\n",
        "\n",
        "            # Critical structural check\n",
        "            if not isinstance(headers, list) or not isinstance(rows, list):\n",
        "                continue\n",
        "\n",
        "            # Ensure headers are strings\n",
        "            # If headers contain non-strings, convert to string\n",
        "            clean_headers = [str(h) if h is not None else \"\" for h in headers]\n",
        "            header_len = len(clean_headers)\n",
        "\n",
        "            if header_len == 0:\n",
        "                continue # Empty table headers, useless\n",
        "\n",
        "            repaired_rows = []\n",
        "            for r in rows:\n",
        "                if not isinstance(r, list):\n",
        "                    continue\n",
        "\n",
        "                # Repair length mismatch\n",
        "                current_len = len(r)\n",
        "                if current_len > header_len:\n",
        "                    # Truncate\n",
        "                    new_row = r[:header_len]\n",
        "                    # Ensure elements are strings\n",
        "                    new_row = [str(c) if c is not None else \"\" for c in new_row]\n",
        "                    repaired_rows.append(new_row)\n",
        "                elif current_len < header_len:\n",
        "                    # Pad\n",
        "                    padding = [\"\"] * (header_len - current_len)\n",
        "                    new_row = r + padding\n",
        "                    new_row = [str(c) if c is not None else \"\" for c in new_row]\n",
        "                    repaired_rows.append(new_row)\n",
        "                else:\n",
        "                    # Exact match\n",
        "                    new_row = [str(c) if c is not None else \"\" for c in r]\n",
        "                    repaired_rows.append(new_row)\n",
        "\n",
        "            # Only keep table if it has rows (or if empty tables are allowed?\n",
        "            # Usually empty tables are useless for QA). Let's require at least one row\n",
        "            # or just keep the structure if headers exist.\n",
        "            # TAT-QA usually implies data extraction, so let's keep it if headers exist.\n",
        "            # We update the table dict with repaired content\n",
        "            tbl[\"headers\"] = clean_headers\n",
        "            tbl[\"rows\"] = repaired_rows\n",
        "            valid_tables.append(tbl)\n",
        "\n",
        "        return valid_tables\n",
        "\n",
        "    # Apply the repair function\n",
        "    # We use a lambda wrapper to handle potential non-list inputs if any slipped through (defensive)\n",
        "    df[\"tables\"] = df[\"tables\"].apply(lambda x: repair_single_row_tables(x) if isinstance(x, list) else [])\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Repair or drop malformed passage entries\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def repair_malformed_passages(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Iterates through the 'passages' column to validate and repair passage structures.\n",
        "\n",
        "    Repair Logic:\n",
        "    - A passage is kept only if it is a dict containing 'text' which is a non-empty string.\n",
        "    - 'passage_id' is preserved if present; otherwise generated or ignored based on downstream needs.\n",
        "      (Here we assume it's required for citation, so we check for it).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with repaired tables.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with repaired 'passages' structures.\n",
        "    \"\"\"\n",
        "\n",
        "    def repair_single_row_passages(passages_list: Any) -> List[Dict[str, Any]]:\n",
        "        if not isinstance(passages_list, list):\n",
        "            return []\n",
        "\n",
        "        valid_passages = []\n",
        "        for psg in passages_list:\n",
        "            if not isinstance(psg, dict):\n",
        "                continue\n",
        "\n",
        "            text = psg.get(\"text\")\n",
        "            # Check text validity\n",
        "            if not isinstance(text, str) or not text.strip():\n",
        "                continue\n",
        "\n",
        "            # Ensure passage_id exists (if missing, maybe generate one?\n",
        "            # For now, we skip if critical ID is missing to avoid alignment errors later)\n",
        "            if \"passage_id\" not in psg:\n",
        "                continue\n",
        "\n",
        "            valid_passages.append(psg)\n",
        "\n",
        "        return valid_passages\n",
        "\n",
        "    df[\"passages\"] = df[\"passages\"].apply(repair_single_row_passages)\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def cleanse_tatqa_dataset(tatqa_raw_df: pd.DataFrame) -> Tuple[pd.DataFrame, CleansingLog]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing pipeline for the TAT-QA dataset.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Drop rows with critical missing fields (ID, Question, Tables).\n",
        "    2. Repair internal table structures (headers/rows alignment).\n",
        "    3. Repair passage structures (valid text).\n",
        "    4. Final check: Drop rows that have NO valid tables AND NO valid passages (empty context).\n",
        "\n",
        "    Args:\n",
        "        tatqa_raw_df (pd.DataFrame): The raw input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, CleansingLog]: The cleansed DataFrame and a log of operations.\n",
        "    \"\"\"\n",
        "    log = CleansingLog(initial_rows=len(tatqa_raw_df))\n",
        "    logger.info(f\"Starting TAT-QA cleansing. Initial rows: {len(tatqa_raw_df)}\")\n",
        "\n",
        "    # Step 1: Drop critical missing\n",
        "    df = drop_critical_missing_rows(tatqa_raw_df, log)\n",
        "    logger.info(f\"Rows after dropping critical missing: {len(df)}\")\n",
        "\n",
        "    # Step 2: Repair tables\n",
        "    df = repair_malformed_tables(df)\n",
        "\n",
        "    # Step 3: Repair passages\n",
        "    df = repair_malformed_passages(df)\n",
        "\n",
        "    # Step 4: Final Content Check\n",
        "    # We require at least one valid table OR one valid passage to form a prompt context.\n",
        "    # If both lists are empty, the example is unusable.\n",
        "    def has_valid_content(row: pd.Series) -> bool:\n",
        "        has_tables = len(row[\"tables\"]) > 0\n",
        "        has_passages = len(row[\"passages\"]) > 0\n",
        "        return has_tables or has_passages\n",
        "\n",
        "    valid_content_mask = df.apply(has_valid_content, axis=1)\n",
        "    dropped_no_content = df[~valid_content_mask][\"example_id\"].tolist()\n",
        "\n",
        "    log.dropped_ids.extend(dropped_no_content)\n",
        "    log.rows_dropped_no_valid_content = len(dropped_no_content)\n",
        "\n",
        "    df = df[valid_content_mask].copy()\n",
        "\n",
        "    # Finalize log\n",
        "    log.final_rows = len(df)\n",
        "    logger.info(f\"TAT-QA cleansing complete. Final rows: {len(df)}\")\n",
        "\n",
        "    # Reset index for clean usage, but keep original index for traceability\n",
        "    df = df.reset_index(names=\"original_index\")\n",
        "\n",
        "    return df, log\n"
      ],
      "metadata": {
        "id": "0uKufmoaSGfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 – Cleanse and Handle Missing Entries in finqa_raw_df\n",
        "\n",
        "@dataclass\n",
        "class FinQACleansingLog:\n",
        "    \"\"\"\n",
        "    Dataclass to track the results of the Fin-QA cleansing process.\n",
        "\n",
        "    Attributes:\n",
        "        initial_rows (int): Number of rows in the raw DataFrame.\n",
        "        rows_dropped_missing_id (int): Rows dropped due to missing/null example_id.\n",
        "        rows_dropped_duplicate_id (int): Rows dropped due to duplicate example_id within a split.\n",
        "        rows_dropped_empty_question (int): Rows dropped due to empty/whitespace-only question_text.\n",
        "        rows_dropped_invalid_tables (int): Rows dropped because 'tables' was not a list or was empty.\n",
        "        rows_dropped_no_valid_content (int): Rows dropped because they had no valid tables after repair.\n",
        "        rows_flagged_invalid_numeric (int): Rows flagged (not dropped) because their numeric answer could not be parsed.\n",
        "        final_rows (int): Number of rows in the cleansed DataFrame.\n",
        "        dropped_ids (List[str]): List of example_ids that were dropped.\n",
        "    \"\"\"\n",
        "    initial_rows: int = 0\n",
        "    rows_dropped_missing_id: int = 0\n",
        "    rows_dropped_duplicate_id: int = 0\n",
        "    rows_dropped_empty_question: int = 0\n",
        "    rows_dropped_invalid_tables: int = 0\n",
        "    rows_dropped_no_valid_content: int = 0\n",
        "    rows_flagged_invalid_numeric: int = 0\n",
        "    final_rows: int = 0\n",
        "    dropped_ids: List[str] = field(default_factory=list)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Cleanse and Handle Missing Entries in finqa_raw_df\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Drop rows with critical missing fields\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def drop_critical_missing_rows_finqa(df: pd.DataFrame, log: FinQACleansingLog) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Drops rows from the Fin-QA DataFrame that are missing critical identifying information\n",
        "    or essential content fields required for downstream processing.\n",
        "\n",
        "    Criteria for dropping:\n",
        "    1. `example_id` is Null or NaN.\n",
        "    2. `example_id` is a duplicate within its `split`.\n",
        "    3. `question_text` is Null, empty, or whitespace-only.\n",
        "    4. `tables` is Null, not a list, or an empty list.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw Fin-QA DataFrame.\n",
        "        log (FinQACleansingLog): The logging object to update with drop statistics.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered DataFrame with critical issues resolved.\n",
        "    \"\"\"\n",
        "    # 1. Drop missing example_id\n",
        "    missing_id_mask = df[\"example_id\"].isnull()\n",
        "    log.rows_dropped_missing_id = missing_id_mask.sum()\n",
        "    df = df[~missing_id_mask].copy()\n",
        "\n",
        "    # 2. Drop duplicate example_id within split\n",
        "    # We assume 'split' exists and is valid (validated in Task 2).\n",
        "    duplicate_mask = df.duplicated(subset=[\"split\", \"example_id\"], keep=\"first\")\n",
        "\n",
        "    dropped_dups = df[duplicate_mask][\"example_id\"].tolist()\n",
        "    log.dropped_ids.extend(dropped_dups)\n",
        "    log.rows_dropped_duplicate_id = len(dropped_dups)\n",
        "\n",
        "    df = df[~duplicate_mask].copy()\n",
        "\n",
        "    # 3. Drop empty/whitespace question_text\n",
        "    def is_valid_question(q: Any) -> bool:\n",
        "        if not isinstance(q, str):\n",
        "            return False\n",
        "        return len(q.strip()) > 0\n",
        "\n",
        "    valid_question_mask = df[\"question_text\"].apply(is_valid_question)\n",
        "    dropped_questions = df[~valid_question_mask][\"example_id\"].tolist()\n",
        "    log.dropped_ids.extend(dropped_questions)\n",
        "    log.rows_dropped_empty_question = len(dropped_questions)\n",
        "\n",
        "    df = df[valid_question_mask].copy()\n",
        "\n",
        "    # 4. Drop invalid 'tables' field\n",
        "    # Must be a non-empty list\n",
        "    def is_valid_tables_field(t: Any) -> bool:\n",
        "        if not isinstance(t, list):\n",
        "            return False\n",
        "        return len(t) > 0\n",
        "\n",
        "    valid_tables_mask = df[\"tables\"].apply(is_valid_tables_field)\n",
        "    dropped_tables = df[~valid_tables_mask][\"example_id\"].tolist()\n",
        "    log.dropped_ids.extend(dropped_tables)\n",
        "    log.rows_dropped_invalid_tables = len(dropped_tables)\n",
        "\n",
        "    df = df[valid_tables_mask].copy()\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Validate and repair table structures\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def repair_malformed_tables_finqa(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Iterates through the 'tables' column of each row to validate and repair individual table structures.\n",
        "\n",
        "    Repair Logic:\n",
        "    - A table is discarded if it lacks 'headers' (list of strings) or 'rows' (list of lists).\n",
        "    - Rows within a table are repaired:\n",
        "        - If a row is longer than headers, it is truncated.\n",
        "        - If a row is shorter than headers, it is padded with empty strings.\n",
        "        - If a row is not a list, it is discarded.\n",
        "    - If a table ends up with no valid rows, it is discarded.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame filtered from Step 1.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with repaired 'tables' structures.\n",
        "    \"\"\"\n",
        "\n",
        "    def repair_single_row_tables(tables_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        valid_tables = []\n",
        "\n",
        "        for tbl in tables_list:\n",
        "            if not isinstance(tbl, dict):\n",
        "                continue\n",
        "\n",
        "            headers = tbl.get(\"headers\")\n",
        "            rows = tbl.get(\"rows\")\n",
        "\n",
        "            # Critical structural check\n",
        "            if not isinstance(headers, list) or not isinstance(rows, list):\n",
        "                continue\n",
        "\n",
        "            # Ensure headers are strings\n",
        "            clean_headers = [str(h) if h is not None else \"\" for h in headers]\n",
        "            header_len = len(clean_headers)\n",
        "\n",
        "            if header_len == 0:\n",
        "                continue\n",
        "\n",
        "            repaired_rows = []\n",
        "            for r in rows:\n",
        "                if not isinstance(r, list):\n",
        "                    continue\n",
        "\n",
        "                # Repair length mismatch\n",
        "                current_len = len(r)\n",
        "                if current_len > header_len:\n",
        "                    # Truncate\n",
        "                    new_row = r[:header_len]\n",
        "                    new_row = [str(c) if c is not None else \"\" for c in new_row]\n",
        "                    repaired_rows.append(new_row)\n",
        "                elif current_len < header_len:\n",
        "                    # Pad\n",
        "                    padding = [\"\"] * (header_len - current_len)\n",
        "                    new_row = r + padding\n",
        "                    new_row = [str(c) if c is not None else \"\" for c in new_row]\n",
        "                    repaired_rows.append(new_row)\n",
        "                else:\n",
        "                    # Exact match\n",
        "                    new_row = [str(c) if c is not None else \"\" for c in r]\n",
        "                    repaired_rows.append(new_row)\n",
        "\n",
        "            # Keep table if headers exist and structure is valid.\n",
        "            # Fin-QA tables are critical for numeric reasoning.\n",
        "            tbl[\"headers\"] = clean_headers\n",
        "            tbl[\"rows\"] = repaired_rows\n",
        "            valid_tables.append(tbl)\n",
        "\n",
        "        return valid_tables\n",
        "\n",
        "    # Apply the repair function\n",
        "    df[\"tables\"] = df[\"tables\"].apply(lambda x: repair_single_row_tables(x) if isinstance(x, list) else [])\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Validate answer values for numeric parsing\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_numeric_answers_finqa(df: pd.DataFrame, log: FinQACleansingLog) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates that rows with 'answer_type' == 'number' have 'answer_value' fields\n",
        "    that can be parsed into floats.\n",
        "\n",
        "    This step does NOT drop rows. Instead, it adds a flag column `is_valid_numeric_answer`.\n",
        "    This allows downstream evaluation to exclude invalid rows from numeric accuracy calculations\n",
        "    while preserving the data for other purposes (e.g., text generation training).\n",
        "\n",
        "    Parsing Logic:\n",
        "    - Remove commas (',').\n",
        "    - Remove trailing percent signs ('%').\n",
        "    - Attempt `float()` conversion.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with repaired tables.\n",
        "        log (FinQACleansingLog): Logging object to record invalid numeric answers.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with an additional 'is_valid_numeric_answer' boolean column.\n",
        "    \"\"\"\n",
        "\n",
        "    def check_numeric_validity(row: pd.Series) -> bool:\n",
        "        # If not a number type, we assume it's valid (textual) or irrelevant for numeric checks\n",
        "        # However, Fin-QA is primarily numeric.\n",
        "        if row.get(\"answer_type\") != \"number\":\n",
        "            return True\n",
        "\n",
        "        val = row.get(\"answer_value\")\n",
        "        if not isinstance(val, str):\n",
        "            return False\n",
        "\n",
        "        # Clean string\n",
        "        clean_val = val.replace(',', '').strip()\n",
        "        if clean_val.endswith('%'):\n",
        "            clean_val = clean_val[:-1]\n",
        "\n",
        "        try:\n",
        "            float(clean_val)\n",
        "            return True\n",
        "        except ValueError:\n",
        "            return False\n",
        "\n",
        "    # Apply validation\n",
        "    df[\"is_valid_numeric_answer\"] = df.apply(check_numeric_validity, axis=1)\n",
        "\n",
        "    # Log statistics\n",
        "    # We only count rows where answer_type IS 'number' but validity is False\n",
        "    invalid_numeric_count = len(df[(df[\"answer_type\"] == \"number\") & (~df[\"is_valid_numeric_answer\"])])\n",
        "    log.rows_flagged_invalid_numeric = invalid_numeric_count\n",
        "\n",
        "    if invalid_numeric_count > 0:\n",
        "        logger.warning(f\"Flagged {invalid_numeric_count} rows with invalid numeric answers in Fin-QA.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def cleanse_finqa_dataset(finqa_raw_df: pd.DataFrame) -> Tuple[pd.DataFrame, FinQACleansingLog]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing pipeline for the Fin-QA dataset.\n",
        "\n",
        "    Pipeline:\n",
        "    1. Drop rows with critical missing fields (ID, Question, Tables).\n",
        "    2. Repair internal table structures (headers/rows alignment).\n",
        "    3. Final check: Drop rows that have NO valid tables (Fin-QA relies heavily on tables).\n",
        "    4. Validate numeric answer parseability and flag invalid rows.\n",
        "\n",
        "    Args:\n",
        "        finqa_raw_df (pd.DataFrame): The raw input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, FinQACleansingLog]: The cleansed DataFrame and a log of operations.\n",
        "    \"\"\"\n",
        "    log = FinQACleansingLog(initial_rows=len(finqa_raw_df))\n",
        "    logger.info(f\"Starting Fin-QA cleansing. Initial rows: {len(finqa_raw_df)}\")\n",
        "\n",
        "    # Step 1: Drop critical missing\n",
        "    df = drop_critical_missing_rows_finqa(finqa_raw_df, log)\n",
        "    logger.info(f\"Rows after dropping critical missing: {len(df)}\")\n",
        "\n",
        "    # Step 2: Repair tables\n",
        "    df = repair_malformed_tables_finqa(df)\n",
        "\n",
        "    # Step 3: Final Content Check (Tables are mandatory for Fin-QA)\n",
        "    def has_valid_tables(row: pd.Series) -> bool:\n",
        "        return isinstance(row[\"tables\"], list) and len(row[\"tables\"]) > 0\n",
        "\n",
        "    valid_content_mask = df.apply(has_valid_tables, axis=1)\n",
        "    dropped_no_content = df[~valid_content_mask][\"example_id\"].tolist()\n",
        "\n",
        "    log.dropped_ids.extend(dropped_no_content)\n",
        "    log.rows_dropped_no_valid_content = len(dropped_no_content)\n",
        "\n",
        "    df = df[valid_content_mask].copy()\n",
        "\n",
        "    # Step 4: Validate Numeric Answers (Flagging only)\n",
        "    df = validate_numeric_answers_finqa(df, log)\n",
        "\n",
        "    # Finalize log\n",
        "    log.final_rows = len(df)\n",
        "    logger.info(f\"Fin-QA cleansing complete. Final rows: {len(df)}\")\n",
        "\n",
        "    # Reset index for clean usage, but keep original index for traceability\n",
        "    df = df.reset_index(names=\"original_index\")\n",
        "\n",
        "    return df, log\n"
      ],
      "metadata": {
        "id": "jWVbPylgU-Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 – Normalize Numeric Columns and Answer Representations\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Normalize Numeric Columns and Answer Representations\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Helper Class: FinancialTextNormalizer\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "class FinancialTextNormalizer:\n",
        "    \"\"\"\n",
        "    spaCy-based text normalization for financial documents.\n",
        "\n",
        "    This class provides robust methods for cleaning and normalizing financial text,\n",
        "    specifically targeting currency symbols, unit multipliers (e.g., 'M', 'B'),\n",
        "    percentages, and numeric value extraction. It leverages regex for pattern matching\n",
        "    and spaCy for potential linguistic context (though primarily regex-driven for precision).\n",
        "\n",
        "    Attributes:\n",
        "        nlp (spacy.language.Language): The loaded spaCy language model.\n",
        "        currency_pattern (re.Pattern): Compiled regex for currency symbols.\n",
        "        unit_pattern (re.Pattern): Compiled regex for unit multipliers.\n",
        "        percent_pattern (re.Pattern): Compiled regex for percentage signs.\n",
        "        parentheses_negative_pattern (re.Pattern): Compiled regex for accounting-style negatives.\n",
        "    \"\"\"\n",
        "\n",
        "    # Comprehensive currency symbol registry covering major global currencies\n",
        "    CURRENCY_SYMBOLS: Set[str] = {\n",
        "        '$', '€', '£', '¥', '₹', '₽', '₩', '₪', '₦', '₱', '₡', '₵', '₲', '₴', '₸',\n",
        "        'USD', 'EUR', 'GBP', 'JPY', 'CNY', 'INR', 'CAD', 'AUD', 'CHF', 'SEK', 'NZD',\n",
        "        'MXN', 'SGD', 'HKD', 'NOK', 'KRW', 'TRY', 'RUB', 'BRL', 'ZAR', 'PLN', 'THB',\n",
        "        'IDR', 'HUF', 'CZK', 'ILS', 'CLP', 'PHP', 'AED', 'SAR', 'MYR', 'RON'\n",
        "    }\n",
        "\n",
        "    # Unit multipliers mapping textual representations to their numeric scale factors\n",
        "    UNIT_PATTERNS: Dict[str, float] = {\n",
        "        'million': 1e6, 'millions': 1e6,\n",
        "        'billion': 1e9, 'billions': 1e9,\n",
        "        'trillion': 1e12, 'trillions': 1e12,\n",
        "        'thousand': 1e3, 'thousands': 1e3,\n",
        "        'M': 1e6, 'B': 1e9, 'T': 1e12, 'K': 1e3, 'k': 1e3\n",
        "    }\n",
        "\n",
        "    def __init__(self, spacy_model: str = \"en_core_web_sm\"):\n",
        "        \"\"\"\n",
        "        Initialize the normalizer with a spaCy model.\n",
        "\n",
        "        Args:\n",
        "            spacy_model (str): The name of the spaCy model to load (default: \"en_core_web_sm\").\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.nlp = spacy.load(spacy_model)\n",
        "        except OSError:\n",
        "            logger.warning(f\"spaCy model '{spacy_model}' not found. Downloading...\")\n",
        "            from spacy.cli import download\n",
        "            download(spacy_model)\n",
        "            self.nlp = spacy.load(spacy_model)\n",
        "\n",
        "        self._compile_patterns()\n",
        "\n",
        "    def _compile_patterns(self) -> None:\n",
        "        \"\"\"\n",
        "        Compile regex patterns for currency, units, and numeric values.\n",
        "\n",
        "        This method pre-compiles regular expressions to optimize performance during\n",
        "        repeated normalization calls.\n",
        "        \"\"\"\n",
        "        # Escape symbols for regex safety\n",
        "        currency_symbols_escaped = [re.escape(sym) for sym in self.CURRENCY_SYMBOLS]\n",
        "        # Pattern matches currency symbols at word boundaries or followed by whitespace\n",
        "        self.currency_pattern = re.compile(\n",
        "            r'\\b(?:' + '|'.join(currency_symbols_escaped) + r')\\s*',\n",
        "            re.IGNORECASE\n",
        "        )\n",
        "\n",
        "        # Pattern matches unit names as whole words\n",
        "        unit_names = '|'.join(re.escape(unit) for unit in self.UNIT_PATTERNS.keys())\n",
        "        self.unit_pattern = re.compile(\n",
        "            r'\\b(' + unit_names + r')\\b',\n",
        "            re.IGNORECASE\n",
        "        )\n",
        "\n",
        "        self.percent_pattern = re.compile(r'%')\n",
        "        # Pattern matches numbers enclosed in parentheses, e.g., (1,000.00), typical in accounting\n",
        "        self.parentheses_negative_pattern = re.compile(r'\\(([0-9,\\.]+)\\)')\n",
        "\n",
        "    def remove_currency_symbols(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove currency symbols from text using regex.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text containing potential currency symbols.\n",
        "\n",
        "        Returns:\n",
        "            str: The text with currency symbols removed and whitespace normalized.\n",
        "        \"\"\"\n",
        "        # Note: We rely on regex here as it's more robust for symbols than NER alone for removal\n",
        "        cleaned_text = self.currency_pattern.sub('', text)\n",
        "        return ' '.join(cleaned_text.split())\n",
        "\n",
        "    def _expand_units(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Expand unit multipliers (e.g., \"45.2M\" -> \"45200000\").\n",
        "\n",
        "        This method identifies numeric values followed by unit suffixes and replaces them\n",
        "        with the full numeric representation.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text containing numbers with units.\n",
        "\n",
        "        Returns:\n",
        "            str: The text with units expanded.\n",
        "        \"\"\"\n",
        "        def replace_unit(match: re.Match) -> str:\n",
        "            numeric_str = match.group(1)\n",
        "            unit_str = match.group(2)\n",
        "            try:\n",
        "                # Remove commas before parsing float\n",
        "                value = float(numeric_str.replace(',', ''))\n",
        "                # Retrieve multiplier, defaulting to 1 if not found (case-insensitive fallback)\n",
        "                multiplier = self.UNIT_PATTERNS.get(unit_str, self.UNIT_PATTERNS.get(unit_str.lower(), 1))\n",
        "                expanded_value = value * multiplier\n",
        "                # Return integer string if it's a whole number to avoid .0 artifacts\n",
        "                return str(int(expanded_value) if expanded_value.is_integer() else expanded_value)\n",
        "            except ValueError:\n",
        "                # Return original match if parsing fails\n",
        "                return match.group(0)\n",
        "\n",
        "        # Regex to capture number (group 1) and unit (group 2)\n",
        "        pattern = re.compile(\n",
        "            r'([+-]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d+)?)\\s*(' +\n",
        "            '|'.join(re.escape(u) for u in self.UNIT_PATTERNS.keys()) + r')\\b',\n",
        "            re.IGNORECASE\n",
        "        )\n",
        "        return pattern.sub(replace_unit, text)\n",
        "\n",
        "    def normalize_numeric_text(\n",
        "        self,\n",
        "        text: str,\n",
        "        remove_currency: bool = True,\n",
        "        parse_units: bool = True,\n",
        "        parse_percentages: bool = True\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Normalize financial text by removing currency symbols and standardizing units.\n",
        "\n",
        "        Args:\n",
        "            text (str): The raw input text.\n",
        "            remove_currency (bool): If True, strips currency symbols.\n",
        "            parse_units (bool): If True, expands 'M', 'B', 'million', etc.\n",
        "            parse_percentages (bool): If True, removes '%' signs.\n",
        "\n",
        "        Returns:\n",
        "            str: The normalized text string ready for numeric parsing.\n",
        "        \"\"\"\n",
        "        if remove_currency:\n",
        "            text = self.remove_currency_symbols(text)\n",
        "\n",
        "        # Convert accounting negative format (100) to -100\n",
        "        text = self.parentheses_negative_pattern.sub(r'-\\1', text)\n",
        "\n",
        "        if parse_units:\n",
        "            text = self._expand_units(text)\n",
        "\n",
        "        if parse_percentages:\n",
        "            text = self.percent_pattern.sub('', text)\n",
        "\n",
        "        # Remove commas from numbers (e.g., 1,000 -> 1000)\n",
        "        text = re.sub(r'(\\d),(\\d)', r'\\1\\2', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def parse_to_float(\n",
        "        self,\n",
        "        text: str,\n",
        "        remove_currency: bool = True,\n",
        "        parse_units: bool = True\n",
        "    ) -> Optional[float]:\n",
        "        \"\"\"\n",
        "        Parse a text string to a float value.\n",
        "\n",
        "        This method orchestrates the normalization steps and attempts to convert the\n",
        "        cleaned string into a float. It handles percentages by stripping the sign\n",
        "        but preserving the magnitude (e.g., \"15%\" -> 15.0).\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text containing a numeric value.\n",
        "            remove_currency (bool): Whether to remove currency symbols.\n",
        "            parse_units (bool): Whether to expand unit multipliers.\n",
        "\n",
        "        Returns:\n",
        "            Optional[float]: Parsed numeric value, or None if parsing fails.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return None\n",
        "\n",
        "        # Normalize text, keeping '%' temporarily to detect percentage context if needed\n",
        "        normalized = self.normalize_numeric_text(\n",
        "            text,\n",
        "            remove_currency=remove_currency,\n",
        "            parse_units=parse_units,\n",
        "            parse_percentages=False # Keep % to detect it, but we handle value below\n",
        "        )\n",
        "\n",
        "        # Check if percentage sign exists before stripping\n",
        "        is_percentage = '%' in text\n",
        "\n",
        "        # Strip everything except digits, signs, and decimal points\n",
        "        clean_normalized = re.sub(r'[^\\d+\\-\\.]', '', normalized)\n",
        "\n",
        "        try:\n",
        "            value = float(clean_normalized)\n",
        "            # Note: For TAT-QA/Fin-QA, percentages are often kept as-is (15 not 0.15)\n",
        "            # We will follow the dataset convention of keeping the magnitude.\n",
        "            return value\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Identify numeric columns in tables\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def identify_numeric_columns(\n",
        "    df: pd.DataFrame,\n",
        "    dataset_name: str,\n",
        "    normalizer: FinancialTextNormalizer,\n",
        "    threshold: float = 0.9\n",
        ") -> Dict[Tuple[str, str, str, int], bool]:\n",
        "    \"\"\"\n",
        "    Identifies numeric columns in all tables within the DataFrame.\n",
        "\n",
        "    Iterates through every table in every row of the dataset. For each column, it attempts\n",
        "    to parse all non-empty cells as floats. If the ratio of successfully parsed cells to\n",
        "    total non-empty cells meets or exceeds the threshold, the column is marked as numeric.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing tables.\n",
        "        dataset_name (str): Name of the dataset (\"TAT-QA\" or \"Fin-QA\").\n",
        "        normalizer (FinancialTextNormalizer): Initialized normalizer instance.\n",
        "        threshold (float): Fraction of parseable cells required to mark a column as numeric (default: 0.9).\n",
        "\n",
        "    Returns:\n",
        "        Dict[Tuple[str, str, str, int], bool]: Metadata mapping\n",
        "        (dataset_name, example_id, table_id, column_index) -> is_numeric.\n",
        "    \"\"\"\n",
        "    numeric_metadata: Dict[Tuple[str, str, str, int], bool] = {}\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        example_id = row[\"example_id\"]\n",
        "        tables = row[\"tables\"]\n",
        "\n",
        "        if not isinstance(tables, list):\n",
        "            continue\n",
        "\n",
        "        for table in tables:\n",
        "            if not isinstance(table, dict):\n",
        "                continue\n",
        "\n",
        "            table_id = table.get(\"table_id\", \"unknown\")\n",
        "            headers = table.get(\"headers\", [])\n",
        "            rows = table.get(\"rows\", [])\n",
        "\n",
        "            if not headers or not rows:\n",
        "                continue\n",
        "\n",
        "            num_cols = len(headers)\n",
        "\n",
        "            for col_idx in range(num_cols):\n",
        "                # Extract cells for this column\n",
        "                cells = []\n",
        "                for r in rows:\n",
        "                    if isinstance(r, list) and len(r) > col_idx:\n",
        "                        cells.append(str(r[col_idx]))\n",
        "\n",
        "                # Filter empty cells to compute valid ratio\n",
        "                non_empty_cells = [c for c in cells if c.strip()]\n",
        "\n",
        "                if not non_empty_cells:\n",
        "                    # Empty columns are not numeric\n",
        "                    numeric_metadata[(dataset_name, example_id, table_id, col_idx)] = False\n",
        "                    continue\n",
        "\n",
        "                # Check parseability of each cell\n",
        "                parseable_count = 0\n",
        "                for cell in non_empty_cells:\n",
        "                    if normalizer.parse_to_float(cell) is not None:\n",
        "                        parseable_count += 1\n",
        "\n",
        "                # Determine if column is numeric based on threshold\n",
        "                ratio = parseable_count / len(non_empty_cells)\n",
        "                is_numeric = ratio >= threshold\n",
        "\n",
        "                numeric_metadata[(dataset_name, example_id, table_id, col_idx)] = is_numeric\n",
        "\n",
        "    return numeric_metadata\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Parse answer values to numeric types\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def parse_answers(\n",
        "    df: pd.DataFrame,\n",
        "    normalizer: FinancialTextNormalizer,\n",
        "    numeric_types: Set[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parses 'answer_value' into a new 'answer_numeric' column for rows with numeric answer types.\n",
        "\n",
        "    This function iterates through the DataFrame and attempts to convert the 'answer_value'\n",
        "    to a float if the 'answer_type' matches one of the specified numeric types.\n",
        "    If parsing fails or the type is non-numeric, 'answer_numeric' is set to NaN.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing answer data.\n",
        "        normalizer (FinancialTextNormalizer): Initialized normalizer instance.\n",
        "        numeric_types (Set[str]): Set of answer_type strings that indicate a numeric answer\n",
        "                                  (e.g., {\"number\", \"span_number\"}).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with an added 'answer_numeric' column containing floats or NaN.\n",
        "    \"\"\"\n",
        "\n",
        "    def parse_row_answer(row: pd.Series) -> float:\n",
        "        \"\"\"Helper to parse a single row's answer.\"\"\"\n",
        "        ans_type = row.get(\"answer_type\")\n",
        "        ans_val = row.get(\"answer_value\")\n",
        "\n",
        "        # Skip non-numeric types\n",
        "        if ans_type not in numeric_types:\n",
        "            return np.nan\n",
        "\n",
        "        # Ensure value is a string before parsing\n",
        "        if not isinstance(ans_val, str):\n",
        "            return np.nan\n",
        "\n",
        "        # Attempt parsing; return NaN on failure\n",
        "        parsed = normalizer.parse_to_float(ans_val)\n",
        "        return parsed if parsed is not None else np.nan\n",
        "\n",
        "    # Apply parsing logic row-wise\n",
        "    df[\"answer_numeric\"] = df.apply(parse_row_answer, axis=1)\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Store cleansed DataFrames and metadata\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def save_artifacts(\n",
        "    tatqa_df: pd.DataFrame,\n",
        "    finqa_df: pd.DataFrame,\n",
        "    metadata: Dict[str, Any],\n",
        "    output_dir: str = \"processed_data\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Saves the processed DataFrames and metadata to disk.\n",
        "\n",
        "    Args:\n",
        "        tatqa_df (pd.DataFrame): Processed TAT-QA DataFrame.\n",
        "        finqa_df (pd.DataFrame): Processed Fin-QA DataFrame.\n",
        "        metadata (Dict[str, Any]): Numeric column metadata.\n",
        "        output_dir (str): Directory to save files.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Save DataFrames\n",
        "    tatqa_df.to_pickle(os.path.join(output_dir, \"tatqa_normalized.pkl\"))\n",
        "    finqa_df.to_pickle(os.path.join(output_dir, \"finqa_normalized.pkl\"))\n",
        "\n",
        "    # Save Metadata\n",
        "    # Convert tuple keys to strings for JSON\n",
        "    json_safe_metadata = {}\n",
        "    for k, v in metadata.items():\n",
        "        # k is (dataset, example_id, table_id, col_idx)\n",
        "        key_str = f\"{k[0]}|{k[1]}|{k[2]}|{k[3]}\"\n",
        "        json_safe_metadata[key_str] = v\n",
        "\n",
        "    with open(os.path.join(output_dir, \"numeric_metadata.json\"), \"w\") as f:\n",
        "        json.dump(json_safe_metadata, f, indent=2)\n",
        "\n",
        "    logger.info(f\"Artifacts saved to {output_dir}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def normalize_data_task(\n",
        "    tatqa_df: pd.DataFrame,\n",
        "    finqa_df: pd.DataFrame,\n",
        "    output_dir: str = \"processed_data\"\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
        "    \"\"\"\n",
        "    Orchestrates Task 6: Normalizing numeric columns and answers.\n",
        "\n",
        "    Args:\n",
        "        tatqa_df (pd.DataFrame): Cleansed TAT-QA DataFrame.\n",
        "        finqa_df (pd.DataFrame): Cleansed Fin-QA DataFrame.\n",
        "        output_dir (str): Output directory.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, Dict]: Normalized DataFrames and metadata.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 6: Normalization...\")\n",
        "\n",
        "    normalizer = FinancialTextNormalizer()\n",
        "\n",
        "    # Step 1: Identify numeric columns\n",
        "    logger.info(\"Identifying numeric columns...\")\n",
        "    tatqa_meta = identify_numeric_columns(tatqa_df, \"TAT-QA\", normalizer)\n",
        "    finqa_meta = identify_numeric_columns(finqa_df, \"Fin-QA\", normalizer)\n",
        "\n",
        "    # Merge metadata\n",
        "    full_metadata = {**tatqa_meta, **finqa_meta}\n",
        "\n",
        "    # Step 2: Parse answers\n",
        "    logger.info(\"Parsing answers...\")\n",
        "    # TAT-QA numeric types\n",
        "    tatqa_numeric_types = {\"span_number\", \"arithmetic\", \"count\"}\n",
        "    tatqa_df = parse_answers(tatqa_df, normalizer, tatqa_numeric_types)\n",
        "\n",
        "    # Fin-QA numeric types\n",
        "    finqa_numeric_types = {\"number\"}\n",
        "    finqa_df = parse_answers(finqa_df, normalizer, finqa_numeric_types)\n",
        "\n",
        "    # Step 3: Save\n",
        "    save_artifacts(tatqa_df, finqa_df, full_metadata, output_dir)\n",
        "\n",
        "    logger.info(\"Task 6 complete.\")\n",
        "    return tatqa_df, finqa_df, full_metadata\n"
      ],
      "metadata": {
        "id": "CI9O49ACWl48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 – Collect and Normalize Offline Corpus Documents\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Collect and Normalize Offline Corpus Documents\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Helper: LaTeX Cleaning Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def clean_latex(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes LaTeX markup from text while preserving the content.\n",
        "\n",
        "    This function applies a series of regex substitutions to strip common LaTeX\n",
        "    commands, environments, and delimiters, leaving behind the natural language text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text containing LaTeX markup.\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove comments\n",
        "    text = re.sub(r'%.*', '', text)\n",
        "\n",
        "    # Remove commands but keep content: \\textbf{text} -> text\n",
        "    # This is a simplification; nested braces are hard for regex, but sufficient for corpus stats\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
        "\n",
        "    # Remove standalone commands: \\noindent, \\newpage\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
        "\n",
        "    # Remove math delimiters: $...$, \\[...\\], \\(...\\)\n",
        "    text = re.sub(r'\\$[^$]*\\$', '', text) # Inline math\n",
        "    text = re.sub(r'\\\\\\[.*?\\\\\\]', '', text, flags=re.DOTALL) # Display math\n",
        "    text = re.sub(r'\\\\\\(.*?\\\\\\)', '', text, flags=re.DOTALL) # Inline math alt\n",
        "\n",
        "    # Collapse whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Collect Wikipedia documents\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def collect_wikipedia_documents(\n",
        "    file_path: str,\n",
        "    max_docs: Optional[int] = None\n",
        ") -> Generator[Dict[str, str], None, None]:\n",
        "    \"\"\"\n",
        "    Ingests Wikipedia documents from a JSONL file (e.g., output of WikiExtractor).\n",
        "\n",
        "    Each line in the input file is expected to be a JSON object with 'id', 'url', 'title', and 'text'.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the Wikipedia dump file (can be .gz).\n",
        "        max_docs (Optional[int]): Maximum number of documents to yield.\n",
        "\n",
        "    Yields:\n",
        "        Dict[str, str]: A dictionary conforming to the corpus_document_schema.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Collecting Wikipedia documents from {file_path}...\")\n",
        "    count = 0\n",
        "\n",
        "    # Handle gzip or plain text\n",
        "    open_func = gzip.open if file_path.endswith('.gz') else open\n",
        "\n",
        "    try:\n",
        "        with open_func(file_path, 'rt', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if max_docs and count >= max_docs:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    # WikiExtractor format usually has 'id', 'url', 'title', 'text'\n",
        "                    doc_id = f\"wiki_en_{data.get('id', count)}\"\n",
        "                    text = data.get('text', '')\n",
        "                    title = data.get('title', '')\n",
        "\n",
        "                    # Basic cleaning if text contains HTML/XML artifacts not handled by extractor\n",
        "                    # (Assuming WikiExtractor did most of the work, we just trim)\n",
        "                    text = text.strip()\n",
        "\n",
        "                    if text:\n",
        "                        yield {\n",
        "                            \"doc_id\": doc_id,\n",
        "                            \"source\": \"wikipedia\",\n",
        "                            \"title\": title,\n",
        "                            \"text\": text\n",
        "                        }\n",
        "                        count += 1\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "    except FileNotFoundError:\n",
        "        logger.warning(f\"Wikipedia file {file_path} not found. Skipping.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 2: Collect ShareGPT documents\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def collect_sharegpt_documents(\n",
        "    file_path: str,\n",
        "    max_docs: Optional[int] = None\n",
        ") -> Generator[Dict[str, str], None, None]:\n",
        "    \"\"\"\n",
        "    Ingests ShareGPT conversation logs from a JSON file.\n",
        "\n",
        "    The input file is expected to be a list of conversation objects.\n",
        "    Each conversation has an 'id' and a 'conversations' list of turns.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the ShareGPT JSON file.\n",
        "        max_docs (Optional[int]): Maximum number of conversations to yield.\n",
        "\n",
        "    Yields:\n",
        "        Dict[str, str]: A dictionary conforming to the corpus_document_schema.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Collecting ShareGPT documents from {file_path}...\")\n",
        "    count = 0\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            # Load the entire list (ShareGPT dumps are usually a single JSON list)\n",
        "            # For extremely large files, ijson would be better, but standard json is used here for simplicity\n",
        "            data = json.load(f)\n",
        "\n",
        "            for conv in data:\n",
        "                if max_docs and count >= max_docs:\n",
        "                    break\n",
        "\n",
        "                original_id = conv.get('id', str(count))\n",
        "                doc_id = f\"sharegpt_{original_id}\"\n",
        "\n",
        "                # Concatenate turns\n",
        "                turns = conv.get('conversations', [])\n",
        "                text_parts = []\n",
        "                for turn in turns:\n",
        "                    role = turn.get('from', 'unknown')\n",
        "                    content = turn.get('value', '')\n",
        "                    # Format: \"human: ... \\n gpt: ...\"\n",
        "                    text_parts.append(f\"{role}: {content}\")\n",
        "\n",
        "                full_text = \"\\n\".join(text_parts)\n",
        "\n",
        "                if full_text.strip():\n",
        "                    yield {\n",
        "                        \"doc_id\": doc_id,\n",
        "                        \"source\": \"sharegpt\",\n",
        "                        \"title\": f\"ShareGPT Conversation {original_id}\",\n",
        "                        \"text\": full_text\n",
        "                    }\n",
        "                    count += 1\n",
        "    except FileNotFoundError:\n",
        "        logger.warning(f\"ShareGPT file {file_path} not found. Skipping.\")\n",
        "    except json.JSONDecodeError:\n",
        "        logger.error(f\"Failed to decode JSON from {file_path}.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Collect arXiv documents\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def collect_arxiv_documents(\n",
        "    file_path: str,\n",
        "    max_docs: Optional[int] = None\n",
        ") -> Generator[Dict[str, str], None, None]:\n",
        "    \"\"\"\n",
        "    Ingests arXiv metadata/abstracts from a JSONL file.\n",
        "\n",
        "    Each line is expected to be a JSON object with 'id', 'title', 'abstract'.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the arXiv JSONL file.\n",
        "        max_docs (Optional[int]): Maximum number of documents to yield.\n",
        "\n",
        "    Yields:\n",
        "        Dict[str, str]: A dictionary conforming to the corpus_document_schema.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Collecting arXiv documents from {file_path}...\")\n",
        "    count = 0\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if max_docs and count >= max_docs:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    original_id = data.get('id', str(count))\n",
        "                    doc_id = f\"arxiv_{original_id}\"\n",
        "                    title = data.get('title', '')\n",
        "                    abstract = data.get('abstract', '')\n",
        "\n",
        "                    # Clean LaTeX from abstract\n",
        "                    clean_abstract = clean_latex(abstract)\n",
        "                    clean_title = clean_latex(title)\n",
        "\n",
        "                    # Combine title and abstract\n",
        "                    full_text = f\"{clean_title}\\n{clean_abstract}\"\n",
        "\n",
        "                    if full_text.strip():\n",
        "                        yield {\n",
        "                            \"doc_id\": doc_id,\n",
        "                            \"source\": \"arxiv\",\n",
        "                            \"title\": clean_title,\n",
        "                            \"text\": full_text\n",
        "                        }\n",
        "                        count += 1\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "    except FileNotFoundError:\n",
        "        logger.warning(f\"arXiv file {file_path} not found. Skipping.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def build_offline_corpus(\n",
        "    config: Dict[str, Any],\n",
        "    output_path: str = \"offline_corpus.jsonl\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the collection of documents from multiple sources (Wikipedia, ShareGPT, arXiv)\n",
        "    and normalizes them into a unified JSONL corpus file.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary containing paths and limits.\n",
        "                                 Expected keys: 'offline_corpus_config' -> 'sources'.\n",
        "                                 We assume 'sources' list contains dicts with 'name' and 'path'.\n",
        "        output_path (str): The file path to save the consolidated corpus.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the generated corpus file.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting offline corpus construction...\")\n",
        "\n",
        "    # Extract source configurations (simulated paths if not in config, for robustness)\n",
        "    # In a real scenario, these paths would be in the config.\n",
        "    # Here we default to placeholders or expect them to be injected.\n",
        "    sources_config = config.get(\"offline_corpus_config\", {}).get(\"sources\", [])\n",
        "\n",
        "    # Map source names to collector functions\n",
        "    collectors = {\n",
        "        \"wikipedia\": collect_wikipedia_documents,\n",
        "        \"sharegpt\": collect_sharegpt_documents,\n",
        "        \"arxiv\": collect_arxiv_documents\n",
        "    }\n",
        "\n",
        "    total_docs = 0\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as out_f:\n",
        "        for source_def in sources_config:\n",
        "            name = source_def.get(\"name\")\n",
        "            # We assume the config has been updated with actual file paths in a real run.\n",
        "            # For this implementation, we look for a 'path' key, or default to a local file.\n",
        "            path = source_def.get(\"path\", f\"{name}_dump.jsonl\")\n",
        "            limit = source_def.get(\"max_docs\", None) # Optional limit for testing\n",
        "\n",
        "            if name in collectors:\n",
        "                collector_func = collectors[name]\n",
        "                logger.info(f\"Processing source: {name}\")\n",
        "\n",
        "                for doc in collector_func(path, max_docs=limit):\n",
        "                    # Write to JSONL\n",
        "                    out_f.write(json.dumps(doc) + \"\\n\")\n",
        "                    total_docs += 1\n",
        "            else:\n",
        "                logger.warning(f\"Unknown source type: {name}\")\n",
        "\n",
        "    logger.info(f\"Corpus construction complete. Total documents: {total_docs}\")\n",
        "    logger.info(f\"Corpus saved to: {output_path}\")\n",
        "\n",
        "    return output_path\n"
      ],
      "metadata": {
        "id": "4XeAk-OmaDYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 – Tokenize Offline Corpus and Compute Token Counts\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Tokenize Offline Corpus and Compute Token Counts\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1 & 2: Tokenize and Count (Combined for Efficiency)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def tokenize_and_count_corpus(\n",
        "    corpus_path: str,\n",
        "    encoding_name: str = \"cl100k_base\"\n",
        ") -> Tuple[CounterType[int], int]:\n",
        "    \"\"\"\n",
        "    Streams documents from the corpus file, tokenizes them using either tiktoken or\n",
        "    a HuggingFace tokenizer, and computes global token frequencies.\n",
        "\n",
        "    This function supports dynamic tokenizer selection:\n",
        "    1. Checks if 'encoding_name' is a valid tiktoken encoding.\n",
        "    2. If not, attempts to load it as a HuggingFace tokenizer ID (e.g., \"meta-llama/Llama-2-7b\").\n",
        "    3. Raises ValueError if neither method succeeds.\n",
        "\n",
        "    This flexibility ensures alignment between the offline corpus statistics and the\n",
        "    scorer LLM's tokenization, which is critical for accurate self-information computation.\n",
        "\n",
        "    Args:\n",
        "        corpus_path (str): Path to the JSONL corpus file.\n",
        "        encoding_name (str): The tokenizer identifier. Can be a tiktoken encoding name\n",
        "                             (e.g., \"cl100k_base\") or a HuggingFace model ID.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Counter[int], int]: A tuple containing:\n",
        "            - A Counter mapping token IDs (int) to their frequency (int).\n",
        "            - The total number of tokens (N) observed in the corpus.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the tokenizer cannot be loaded.\n",
        "        FileNotFoundError: If the corpus file does not exist.\n",
        "    \"\"\"\n",
        "    tokenizer = None\n",
        "    is_tiktoken = False\n",
        "\n",
        "    # 1. Attempt to load tiktoken encoding\n",
        "    if tiktoken is not None:\n",
        "        try:\n",
        "            tokenizer = tiktoken.get_encoding(encoding_name)\n",
        "            is_tiktoken = True\n",
        "            logger.info(f\"Loaded tiktoken encoding: {encoding_name}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 2. If not tiktoken, attempt HuggingFace AutoTokenizer\n",
        "    if tokenizer is None:\n",
        "        if AutoTokenizer is not None:\n",
        "            try:\n",
        "                # use_fast=True is recommended for speed\n",
        "                tokenizer = AutoTokenizer.from_pretrained(encoding_name, use_fast=True)\n",
        "                is_tiktoken = False\n",
        "                logger.info(f\"Loaded HuggingFace tokenizer: {encoding_name}\")\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"Failed to load HF tokenizer '{encoding_name}': {e}\")\n",
        "        else:\n",
        "            logger.warning(\"transformers library not installed; skipping HF tokenizer check.\")\n",
        "\n",
        "    # 3. If both fail, raise error\n",
        "    if tokenizer is None:\n",
        "        raise ValueError(\n",
        "            f\"Could not load tokenizer for '{encoding_name}'. \"\n",
        "            \"Ensure it is a valid tiktoken encoding or HuggingFace model ID, \"\n",
        "            \"and that necessary libraries (tiktoken, transformers) are installed.\"\n",
        "        )\n",
        "\n",
        "    token_counts: CounterType[int] = Counter()\n",
        "    total_tokens = 0\n",
        "    doc_count = 0\n",
        "\n",
        "    logger.info(f\"Processing corpus from {corpus_path}...\")\n",
        "\n",
        "    if not os.path.exists(corpus_path):\n",
        "        raise FileNotFoundError(f\"Corpus file not found: {corpus_path}\")\n",
        "\n",
        "    try:\n",
        "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    doc = json.loads(line)\n",
        "                    text = doc.get(\"text\", \"\")\n",
        "\n",
        "                    if not text:\n",
        "                        continue\n",
        "\n",
        "                    # Normalize whitespace (simple collapse) to match preprocessing expectations\n",
        "                    text = \" \".join(text.split())\n",
        "\n",
        "                    # Tokenize based on the loaded backend\n",
        "                    if is_tiktoken:\n",
        "                        # tiktoken encode\n",
        "                        tokens = tokenizer.encode(text, disallowed_special=())\n",
        "                    else:\n",
        "                        # HuggingFace encode\n",
        "                        # add_special_tokens=False to avoid BOS/EOS bias in frequency counts\n",
        "                        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "                    # Update counts\n",
        "                    token_counts.update(tokens)\n",
        "                    total_tokens += len(tokens)\n",
        "                    doc_count += 1\n",
        "\n",
        "                    if doc_count % 10000 == 0:\n",
        "                        logger.info(f\"Processed {doc_count} documents. Total tokens so far: {total_tokens}\")\n",
        "\n",
        "                except json.JSONDecodeError:\n",
        "                    logger.warning(\"Skipping malformed JSON line in corpus.\")\n",
        "                    continue\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing corpus file: {e}\")\n",
        "        raise\n",
        "\n",
        "    logger.info(f\"Tokenization complete. Processed {doc_count} documents.\")\n",
        "    logger.info(f\"Total unique tokens: {len(token_counts)}\")\n",
        "    logger.info(f\"Total token occurrences (N): {total_tokens}\")\n",
        "\n",
        "    return token_counts, total_tokens\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Persist raw token statistics\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def save_token_statistics(\n",
        "    token_counts: CounterType[int],\n",
        "    total_tokens: int,\n",
        "    output_dir: str,\n",
        "    encoding_name: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Saves the token counts and metadata to a JSON file.\n",
        "\n",
        "    Since JSON keys must be strings, token IDs are converted to strings in the output.\n",
        "\n",
        "    Args:\n",
        "        token_counts (Counter[int]): The token frequency mapping.\n",
        "        total_tokens (int): The total count N.\n",
        "        output_dir (str): Directory to save the statistics file.\n",
        "        encoding_name (str): Name of the tokenizer encoding used.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved statistics file.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"token_stats.json\")\n",
        "\n",
        "    # Prepare data structure\n",
        "    # Convert integer keys to strings for JSON serialization\n",
        "    stats_data = {\n",
        "        \"metadata\": {\n",
        "            \"encoding\": encoding_name,\n",
        "            \"total_tokens_N\": total_tokens,\n",
        "            \"unique_tokens\": len(token_counts),\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "        },\n",
        "        \"counts\": {str(k): v for k, v in token_counts.items()}\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Saving token statistics to {output_path}...\")\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(stats_data, f, indent=2)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_corpus_statistics(\n",
        "    config: Dict[str, Any],\n",
        "    corpus_path: str = \"offline_corpus.jsonl\",\n",
        "    output_dir: str = \"corpus_stats\"\n",
        ") -> Tuple[CounterType[int], int]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of static corpus statistics.\n",
        "\n",
        "    1. Loads tokenizer configuration.\n",
        "    2. Streams and tokenizes the corpus to compute counts.\n",
        "    3. Persists the results to disk.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): Study configuration containing tokenizer settings.\n",
        "        corpus_path (str): Path to the input corpus JSONL file.\n",
        "        output_dir (str): Directory to save output stats.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Counter[int], int]: The token counts and total token count N.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting corpus statistics computation...\")\n",
        "\n",
        "    # Extract tokenizer name from config (resolved in Task 3)\n",
        "    encoding_name = config.get(\"offline_corpus_config\", {}).get(\"tokenization_scheme\", {}).get(\"name\", \"cl100k_base\")\n",
        "\n",
        "    # Step 1 & 2: Tokenize and Count\n",
        "    token_counts, total_tokens = tokenize_and_count_corpus(corpus_path, encoding_name)\n",
        "\n",
        "    # Step 3: Save\n",
        "    save_token_statistics(token_counts, total_tokens, output_dir, encoding_name)\n",
        "\n",
        "    logger.info(\"Corpus statistics computation complete.\")\n",
        "    return token_counts, total_tokens\n"
      ],
      "metadata": {
        "id": "1JPl-T5cdgsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 – Compute Token Frequencies and Probabilities\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Compute Token Frequencies and Probabilities\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1 & 2: Compute Frequencies and Probabilities\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_frequencies_and_probabilities(\n",
        "    token_counts: CounterType[int],\n",
        "    total_tokens_N: int\n",
        ") -> Tuple[Dict[int, float], Dict[int, float]]:\n",
        "    \"\"\"\n",
        "    Computes empirical frequencies f(t) and corpus probabilities p(t) for each token.\n",
        "\n",
        "    Equations:\n",
        "        f(t) = Count(t) / N\n",
        "        p(t) = f(t)  (No smoothing applied)\n",
        "\n",
        "    Args:\n",
        "        token_counts (Counter[int]): Mapping of token ID to count.\n",
        "        total_tokens_N (int): Total number of tokens in the corpus.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[int, float], Dict[int, float]]:\n",
        "            - freq: Dictionary mapping token ID to frequency f(t).\n",
        "            - p: Dictionary mapping token ID to probability p(t).\n",
        "    \"\"\"\n",
        "    logger.info(f\"Computing frequencies and probabilities for {len(token_counts)} unique tokens...\")\n",
        "\n",
        "    freq: Dict[int, float] = {}\n",
        "    p: Dict[int, float] = {}\n",
        "\n",
        "    if total_tokens_N == 0:\n",
        "        logger.warning(\"Total token count is 0. Returning empty probabilities.\")\n",
        "        return freq, p\n",
        "\n",
        "    for token_id, count in token_counts.items():\n",
        "        # Compute frequency\n",
        "        f_t = count / total_tokens_N\n",
        "        freq[token_id] = f_t\n",
        "\n",
        "        # Estimate probability (identity mapping per instructions)\n",
        "        p[token_id] = f_t\n",
        "\n",
        "    return freq, p\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Validate probability distribution\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_probability_distribution(p: Dict[int, float]) -> bool:\n",
        "    \"\"\"\n",
        "    Validates that the computed probabilities sum to approximately 1.0.\n",
        "\n",
        "    Args:\n",
        "        p (Dict[int, float]): Dictionary of token probabilities.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid, False otherwise.\n",
        "    \"\"\"\n",
        "    total_prob = sum(p.values())\n",
        "    is_valid = math.isclose(total_prob, 1.0, rel_tol=1e-6)\n",
        "\n",
        "    logger.info(f\"Total probability mass: {total_prob:.8f}\")\n",
        "\n",
        "    if not is_valid:\n",
        "        logger.warning(\"Probability distribution does not sum to 1.0 within tolerance.\")\n",
        "    else:\n",
        "        logger.info(\"Probability distribution is valid.\")\n",
        "\n",
        "    # Inspect top tokens (for debugging/sanity check)\n",
        "    # Define Top K\n",
        "    top_k = 5\n",
        "\n",
        "    # Sort by probability descending\n",
        "    sorted_p = sorted(p.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
        "\n",
        "    logger.info(f\"Top {top_k} tokens by probability (ID: prob): {sorted_p}\")\n",
        "\n",
        "    return is_valid\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_token_probabilities(\n",
        "    token_counts: CounterType[int],\n",
        "    total_tokens_N: int\n",
        ") -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of token probabilities from raw counts.\n",
        "\n",
        "    1. Computes f(t) and p(t).\n",
        "    2. Validates the distribution.\n",
        "    3. Returns the probability map p(t).\n",
        "\n",
        "    Args:\n",
        "        token_counts (Counter[int]): Raw token counts.\n",
        "        total_tokens_N (int): Total tokens.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, float]: The probability map p(t).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting probability computation...\")\n",
        "\n",
        "    # Compute frequencies and probabilities\n",
        "    freq, p = compute_frequencies_and_probabilities(token_counts, total_tokens_N)\n",
        "\n",
        "    # Validate probabilities\n",
        "    validate_probability_distribution(p)\n",
        "\n",
        "    logger.info(\"Probability computation complete.\")\n",
        "    return p\n"
      ],
      "metadata": {
        "id": "UVnukI9ze10b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 – Compute Static Self-Information Scores\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Compute Static Self-Information Scores\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1 & 2: Compute Static Self-Information\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_self_information(p: Dict[int, float]) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Computes the static self-information for each token based on its corpus probability.\n",
        "\n",
        "    Equation:\n",
        "        I(T) = -log2(p(T))\n",
        "\n",
        "    Args:\n",
        "        p (Dict[int, float]): Dictionary mapping token ID to probability p(T).\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, float]: Dictionary mapping token ID to static self-information s_stat(T).\n",
        "    \"\"\"\n",
        "    logger.info(f\"Computing static self-information for {len(p)} tokens...\")\n",
        "\n",
        "    s_stat: Dict[int, float] = {}\n",
        "\n",
        "    for token_id, prob in p.items():\n",
        "        if prob <= 0:\n",
        "            # This should theoretically not happen if p comes from counts > 0\n",
        "            # We assign a large penalty or skip. Here we skip and log warning.\n",
        "            logger.warning(f\"Token {token_id} has non-positive probability {prob}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Compute self-information in bits\n",
        "        info = -math.log2(prob)\n",
        "        s_stat[token_id] = info\n",
        "\n",
        "    return s_stat\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Persist static self-information lookup\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def save_static_scores(\n",
        "    s_stat: Dict[int, float],\n",
        "    output_dir: str,\n",
        "    metadata: Dict[str, Any] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Saves the static self-information scores to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        s_stat (Dict[int, float]): The self-information scores.\n",
        "        output_dir (str): Directory to save the file.\n",
        "        metadata (Dict[str, Any], optional): Additional metadata to save.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved file.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"static_self_information.json\")\n",
        "\n",
        "    # Compute summary stats\n",
        "    scores = list(s_stat.values())\n",
        "    if scores:\n",
        "        stats = {\n",
        "            \"min\": min(scores),\n",
        "            \"max\": max(scores),\n",
        "            \"mean\": sum(scores) / len(scores),\n",
        "            \"count\": len(scores)\n",
        "        }\n",
        "    else:\n",
        "        stats = {}\n",
        "\n",
        "    # Prepare data\n",
        "    data = {\n",
        "        \"metadata\": {\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
        "            \"stats\": stats,\n",
        "            **(metadata or {})\n",
        "        },\n",
        "        # Convert int keys to strings for JSON\n",
        "        \"s_stat\": {str(k): v for k, v in s_stat.items()}\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Saving static self-information to {output_path}...\")\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_static_self_information(\n",
        "    p: Dict[int, float],\n",
        "    output_dir: str = \"corpus_stats\",\n",
        "    extra_metadata: Dict[str, Any] = None\n",
        ") -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation and persistence of static self-information scores.\n",
        "\n",
        "    1. Computes I(T) = -log2(p(T)).\n",
        "    2. Persists the lookup table to disk.\n",
        "\n",
        "    Args:\n",
        "        p (Dict[int, float]): Token probabilities.\n",
        "        output_dir (str): Output directory.\n",
        "        extra_metadata (Dict[str, Any]): Metadata to include in the output file.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, float]: The static self-information map.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting static self-information computation...\")\n",
        "\n",
        "    # Compute self-information score\n",
        "    s_stat = calculate_self_information(p)\n",
        "\n",
        "    # Persist stats\n",
        "    save_static_scores(s_stat, output_dir, extra_metadata)\n",
        "\n",
        "    logger.info(\"Static self-information computation complete.\")\n",
        "    return s_stat\n"
      ],
      "metadata": {
        "id": "H59ogKSpgZ3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 – Define Table Serialization Format\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Define Table Serialization Format\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Choose and formalize serialization method\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def serialize_table_markdown(table: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Serializes a table dictionary into a Markdown-formatted string.\n",
        "\n",
        "    This function converts a structured table dictionary into a string representation\n",
        "    suitable for LLM consumption. It handles captions, headers, and rows, ensuring\n",
        "    proper Markdown syntax (pipes, separator lines). It also sanitizes cell content\n",
        "    by escaping pipes and collapsing newlines to maintain table structure.\n",
        "\n",
        "    Format:\n",
        "    Caption: {caption} (if present)\n",
        "    | Header 1 | Header 2 | ... |\n",
        "    | --- | --- | ... |\n",
        "    | Cell 1,1 | Cell 1,2 | ... |\n",
        "    ...\n",
        "\n",
        "    Args:\n",
        "        table (Dict[str, Any]): Table dictionary containing 'headers', 'rows', and optional 'caption'.\n",
        "                                Expected keys: 'headers' (List[str]), 'rows' (List[List[str]]), 'caption' (str).\n",
        "\n",
        "    Returns:\n",
        "        str: The serialized Markdown table string. Returns an empty string if the table structure is invalid.\n",
        "    \"\"\"\n",
        "    if not isinstance(table, dict):\n",
        "        return \"\"\n",
        "\n",
        "    headers = table.get(\"headers\", [])\n",
        "    rows = table.get(\"rows\", [])\n",
        "    caption = table.get(\"caption\", \"\")\n",
        "\n",
        "    # Basic validation: headers are required to form a table structure\n",
        "    if not headers or not isinstance(headers, list):\n",
        "        return \"\"\n",
        "\n",
        "    # Helper to escape special characters (pipes and newlines) within cell content\n",
        "    def clean_cell(cell: Any) -> str:\n",
        "        s = str(cell) if cell is not None else \"\"\n",
        "        # Escape pipes to prevent breaking Markdown table structure\n",
        "        s = s.replace(\"|\", \"\\\\|\")\n",
        "        # Collapse newlines to spaces to keep rows on single lines\n",
        "        s = s.replace(\"\\n\", \" \")\n",
        "        return s.strip()\n",
        "\n",
        "    # Build the table string parts\n",
        "    parts = []\n",
        "\n",
        "    # 1. Caption\n",
        "    if caption and isinstance(caption, str) and caption.strip():\n",
        "        parts.append(f\"Caption: {clean_cell(caption)}\")\n",
        "\n",
        "    # 2. Headers\n",
        "    clean_headers = [clean_cell(h) for h in headers]\n",
        "    header_row = \"| \" + \" | \".join(clean_headers) + \" |\"\n",
        "    parts.append(header_row)\n",
        "\n",
        "    # 3. Separator Row\n",
        "    # Create a separator line with '---' for each column\n",
        "    separator_row = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
        "    parts.append(separator_row)\n",
        "\n",
        "    # 4. Data Rows\n",
        "    if isinstance(rows, list):\n",
        "        for row in rows:\n",
        "            if not isinstance(row, list):\n",
        "                continue\n",
        "\n",
        "            # Ensure row length matches headers.\n",
        "            # If row is shorter, pad with empty strings.\n",
        "            # If row is longer, truncate (though cleansing should have handled this).\n",
        "            current_row = row[:len(headers)] + [\"\"] * (len(headers) - len(row))\n",
        "\n",
        "            clean_row_cells = [clean_cell(c) for c in current_row]\n",
        "            row_str = \"| \" + \" | \".join(clean_row_cells) + \" |\"\n",
        "            parts.append(row_str)\n",
        "\n",
        "    # Join all parts with newlines\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Apply serialization consistently\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def apply_table_serialization(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies Markdown serialization to all tables in the DataFrame.\n",
        "\n",
        "    This function iterates over the 'tables' column of the DataFrame. For each row,\n",
        "    it serializes the list of table dictionaries into a list of Markdown strings.\n",
        "    The result is stored in a new column 'serialized_tables'.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing a 'tables' column (list of dicts).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame with a new 'serialized_tables' column (list of strings).\n",
        "    \"\"\"\n",
        "    def serialize_row_tables(tables_list: Any) -> List[str]:\n",
        "        \"\"\"Helper to serialize a list of tables for a single row.\"\"\"\n",
        "        if not isinstance(tables_list, list):\n",
        "            return []\n",
        "\n",
        "        serialized_list = []\n",
        "        for t in tables_list:\n",
        "            serialized = serialize_table_markdown(t)\n",
        "            if serialized:\n",
        "                serialized_list.append(serialized)\n",
        "        return serialized_list\n",
        "\n",
        "    # Apply the serialization function to the 'tables' column\n",
        "    # We use a lambda or direct function reference. Here direct reference is cleaner.\n",
        "    df[\"serialized_tables\"] = df[\"tables\"].apply(serialize_row_tables)\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Validate serialization output\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_serialization(df: pd.DataFrame, sample_size: int = 5) -> bool:\n",
        "    \"\"\"\n",
        "    Validates that serialization produces non-empty strings for valid tables.\n",
        "\n",
        "    This function inspects a sample of rows where tables exist to ensure that\n",
        "    the 'serialized_tables' column contains valid Markdown strings (non-empty,\n",
        "    containing pipes). It logs errors if malformed serialization is detected.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with 'serialized_tables' column.\n",
        "        sample_size (int): Number of rows to inspect (default: 5).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if validation passes, False otherwise.\n",
        "    \"\"\"\n",
        "    # Filter for rows that actually have tables\n",
        "    rows_with_tables = df[df[\"serialized_tables\"].map(len) > 0]\n",
        "\n",
        "    if len(rows_with_tables) == 0:\n",
        "        logger.warning(\"No rows with tables found to validate serialization.\")\n",
        "        return True # Technically valid if empty, but worth noting\n",
        "\n",
        "    # Sample rows\n",
        "    sample = rows_with_tables.head(sample_size)\n",
        "\n",
        "    all_valid = True\n",
        "    for idx, row in sample.iterrows():\n",
        "        serialized_list = row[\"serialized_tables\"]\n",
        "        for i, s_table in enumerate(serialized_list):\n",
        "            # Check 1: Non-empty\n",
        "            if not s_table.strip():\n",
        "                logger.error(f\"Empty serialization found for row {idx}, table {i}\")\n",
        "                all_valid = False\n",
        "                continue\n",
        "\n",
        "            # Check 2: Contains pipes (basic Markdown table indicator)\n",
        "            if \"|\" not in s_table:\n",
        "                logger.error(f\"Malformed serialization (no pipes) for row {idx}, table {i}: {s_table[:50]}...\")\n",
        "                all_valid = False\n",
        "                continue\n",
        "\n",
        "            # Check 3: Contains separator line\n",
        "            if \"| --- |\" not in s_table and \"| ---\" not in s_table:\n",
        "                 # Note: Depending on column count, it might be \"| --- |\" or \"| --- | --- |\"\n",
        "                 # We check for the basic separator pattern\n",
        "                 if \"---\" not in s_table:\n",
        "                    logger.error(f\"Malformed serialization (no separator line) for row {idx}, table {i}\")\n",
        "                    all_valid = False\n",
        "\n",
        "    if all_valid:\n",
        "        logger.info(f\"Serialization validation passed on {len(sample)} samples.\")\n",
        "    else:\n",
        "        logger.error(\"Serialization validation failed on one or more samples.\")\n",
        "\n",
        "    return all_valid\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def serialize_tables_task(\n",
        "    tatqa_df: pd.DataFrame,\n",
        "    finqa_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates table serialization for both TAT-QA and Fin-QA datasets.\n",
        "\n",
        "    This function applies the Markdown serialization logic to both DataFrames\n",
        "    and performs validation to ensure the output is correct.\n",
        "\n",
        "    Args:\n",
        "        tatqa_df (pd.DataFrame): TAT-QA DataFrame.\n",
        "        finqa_df (pd.DataFrame): Fin-QA DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: The input DataFrames with a new 'serialized_tables' column.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting table serialization task...\")\n",
        "\n",
        "    # Process TAT-QA\n",
        "    logger.info(\"Serializing TAT-QA tables...\")\n",
        "    tatqa_df = apply_table_serialization(tatqa_df)\n",
        "    if not validate_serialization(tatqa_df):\n",
        "        logger.warning(\"TAT-QA serialization validation reported issues.\")\n",
        "\n",
        "    # Process Fin-QA\n",
        "    logger.info(\"Serializing Fin-QA tables...\")\n",
        "    finqa_df = apply_table_serialization(finqa_df)\n",
        "    if not validate_serialization(finqa_df):\n",
        "        logger.warning(\"Fin-QA serialization validation reported issues.\")\n",
        "\n",
        "    logger.info(\"Table serialization task complete.\")\n",
        "    return tatqa_df, finqa_df\n"
      ],
      "metadata": {
        "id": "o7bav_QQiTT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 – Define QA Prompt Template Structure\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Define QA Prompt Template Structure\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1 & 2: Define Templates for TAT-QA and Fin-QA\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "class PromptTemplateManager:\n",
        "    \"\"\"\n",
        "    Manages prompt templates for different datasets.\n",
        "\n",
        "    Templates are designed to be consistent across datasets while allowing for\n",
        "    specific instructions. They include placeholders for:\n",
        "    - {instructions}: Task-specific instructions.\n",
        "    - {exemplars}: Few-shot examples (optional).\n",
        "    - {tables}: Serialized table content.\n",
        "    - {passages}: Narrative text content.\n",
        "    - {question}: The user query.\n",
        "    \"\"\"\n",
        "\n",
        "    # TAT-QA Template\n",
        "    # Emphasizes extracting information from hybrid context\n",
        "    TATQA_TEMPLATE = (\n",
        "        \"{instructions}\\n\\n\"\n",
        "        \"{exemplars}\"\n",
        "        \"Context:\\n\"\n",
        "        \"{tables}\\n\\n\"\n",
        "        \"{passages}\\n\\n\"\n",
        "        \"Question: {question}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "    TATQA_INSTRUCTIONS = (\n",
        "        \"You are a financial QA assistant. Answer the question based on the following \"\n",
        "        \"tables and passages. Provide the answer directly.\"\n",
        "    )\n",
        "\n",
        "    # Fin-QA Template\n",
        "    # Emphasizes numerical reasoning and calculations\n",
        "    FINQA_TEMPLATE = (\n",
        "        \"{instructions}\\n\\n\"\n",
        "        \"{exemplars}\"\n",
        "        \"Context:\\n\"\n",
        "        \"{tables}\\n\\n\"\n",
        "        \"{passages}\\n\\n\"\n",
        "        \"Question: {question}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "    FINQA_INSTRUCTIONS = (\n",
        "        \"You are a financial expert. Perform any necessary calculations to answer the \"\n",
        "        \"question based on the provided financial data. Provide the numeric answer.\"\n",
        "    )\n",
        "\n",
        "    @classmethod\n",
        "    def get_template(cls, dataset: str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieves the raw template string for a given dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset (str): Name of the dataset (\"TAT-QA\" or \"Fin-QA\").\n",
        "\n",
        "        Returns:\n",
        "            str: The format string with placeholders.\n",
        "        \"\"\"\n",
        "        if dataset == \"TAT-QA\":\n",
        "            return cls.TATQA_TEMPLATE\n",
        "        elif dataset == \"Fin-QA\":\n",
        "            return cls.FINQA_TEMPLATE\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset: {dataset}\")\n",
        "\n",
        "    @classmethod\n",
        "    def get_instructions(cls, dataset: str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieves the default instructions for a given dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset (str): Name of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            str: The instruction text.\n",
        "        \"\"\"\n",
        "        if dataset == \"TAT-QA\":\n",
        "            return cls.TATQA_INSTRUCTIONS\n",
        "        elif dataset == \"Fin-QA\":\n",
        "            return cls.FINQA_INSTRUCTIONS\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset: {dataset}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Document template placeholders (Implemented via Docstrings and Class Structure)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# The PromptTemplateManager class above encapsulates the documentation and structure.\n",
        "# The placeholders are explicitly named in the template strings.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def construct_prompt(\n",
        "    dataset: str,\n",
        "    question: str,\n",
        "    tables_str: str,\n",
        "    passages_str: str,\n",
        "    exemplars_str: str = \"\",\n",
        "    custom_instructions: Optional[str] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Constructs the final prompt string by populating the dataset-specific template.\n",
        "\n",
        "    Args:\n",
        "        dataset (str): \"TAT-QA\" or \"Fin-QA\".\n",
        "        question (str): The user question.\n",
        "        tables_str (str): Serialized tables.\n",
        "        passages_str (str): Concatenated passages.\n",
        "        exemplars_str (str, optional): Formatted few-shot exemplars. Defaults to \"\".\n",
        "        custom_instructions (str, optional): Override default instructions.\n",
        "\n",
        "    Returns:\n",
        "        str: The fully constructed prompt.\n",
        "    \"\"\"\n",
        "    template = PromptTemplateManager.get_template(dataset)\n",
        "    instructions = custom_instructions or PromptTemplateManager.get_instructions(dataset)\n",
        "\n",
        "    # Ensure exemplars have a trailing newline if present to separate from Context\n",
        "    if exemplars_str and not exemplars_str.endswith(\"\\n\"):\n",
        "        exemplars_str += \"\\n\"\n",
        "\n",
        "    prompt = template.format(\n",
        "        instructions=instructions,\n",
        "        exemplars=exemplars_str,\n",
        "        tables=tables_str,\n",
        "        passages=passages_str,\n",
        "        question=question\n",
        "    )\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "qG1hii0Kj5DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 – Define Few-Shot Exemplar Embedding in Prompts\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Define Few-Shot Exemplar Embedding in Prompts\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1 & 2: Define Exemplar Formatting and Placement\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def format_single_exemplar(\n",
        "    exemplar: Dict[str, Any],\n",
        "    index: int,\n",
        "    dataset: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Formats a single few-shot exemplar into a string block.\n",
        "\n",
        "    Structure:\n",
        "    Example {index}:\n",
        "    Context:\n",
        "    {tables}\n",
        "    {passages}\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "\n",
        "    Args:\n",
        "        exemplar (Dict[str, Any]): The exemplar data containing 'question_text',\n",
        "                                   'serialized_tables', 'passages', 'answer_value', 'answer_unit'.\n",
        "        index (int): The 1-based index of the exemplar in the sequence.\n",
        "        dataset (str): \"TAT-QA\" or \"Fin-QA\" (used for minor formatting nuances if any).\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted exemplar string.\n",
        "    \"\"\"\n",
        "    # Extract components\n",
        "    question = exemplar.get(\"question_text\", \"\").strip()\n",
        "\n",
        "    # Tables are already serialized strings in the 'serialized_tables' list\n",
        "    # We join them with newlines\n",
        "    tables_list = exemplar.get(\"serialized_tables\", [])\n",
        "    tables_str = \"\\n\\n\".join(tables_list) if tables_list else \"\"\n",
        "\n",
        "    # Passages are a list of dicts, we need to extract text\n",
        "    passages_list = exemplar.get(\"passages\", [])\n",
        "    passages_str = \"\\n\\n\".join([p.get(\"text\", \"\").strip() for p in passages_list])\n",
        "\n",
        "    # Answer\n",
        "    answer_val = str(exemplar.get(\"answer_value\", \"\")).strip()\n",
        "    answer_unit = exemplar.get(\"answer_unit\")\n",
        "    if answer_unit:\n",
        "        answer_full = f\"{answer_val} {answer_unit}\"\n",
        "    else:\n",
        "        answer_full = answer_val\n",
        "\n",
        "    # Construct block\n",
        "    # We use a separator line at the end to distinguish from the next example\n",
        "    block = (\n",
        "        f\"Example {index}:\\n\"\n",
        "        f\"Context:\\n\"\n",
        "        f\"{tables_str}\\n\\n\"\n",
        "        f\"{passages_str}\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Answer: {answer_full}\"\n",
        "    )\n",
        "\n",
        "    return block\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Ensure no leakage and Orchestrate\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def format_exemplars(\n",
        "    target_example_id: str,\n",
        "    exemplars: List[Dict[str, Any]],\n",
        "    dataset: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Selects and formats a list of exemplars into a single string block for the prompt.\n",
        "\n",
        "    Enforces the constraint that the target example cannot be used as its own exemplar (leakage prevention).\n",
        "\n",
        "    Args:\n",
        "        target_example_id (str): The ID of the example being prompted for.\n",
        "        exemplars (List[Dict[str, Any]]): A list of candidate exemplar dictionaries.\n",
        "        dataset (str): The dataset name.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing all formatted exemplars, separated by horizontal rules.\n",
        "    \"\"\"\n",
        "    formatted_blocks = []\n",
        "    valid_count = 0\n",
        "\n",
        "    for ex in exemplars:\n",
        "        # Leakage check\n",
        "        if str(ex.get(\"example_id\")) == str(target_example_id):\n",
        "            logger.warning(f\"Skipping exemplar {ex.get('example_id')} to prevent leakage for target {target_example_id}.\")\n",
        "            continue\n",
        "\n",
        "        valid_count += 1\n",
        "        block = format_single_exemplar(ex, valid_count, dataset)\n",
        "        formatted_blocks.append(block)\n",
        "\n",
        "    if not formatted_blocks:\n",
        "        return \"\"\n",
        "\n",
        "    # Join blocks with a distinct separator\n",
        "    # We add a header \"Examples:\" at the very top\n",
        "    full_str = \"Examples:\\n\\n\" + \"\\n\\n---\\n\\n\".join(formatted_blocks) + \"\\n\\n---\\n\\n\"\n",
        "\n",
        "    return full_str\n"
      ],
      "metadata": {
        "id": "-L6IBiTzm8R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 – Configure Scorer LLMs and Target LLMs\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Configure Scorer LLMs and Target LLMs\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1 & 2: Define LLM Interfaces (Abstract and Concrete)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "class LLMInterface(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class defining the standardized interface for LLM interactions\n",
        "    within the CompactPrompt compression pipeline.\n",
        "\n",
        "    This interface provides a unified abstraction layer across heterogeneous LLM\n",
        "    providers (OpenAI, Anthropic, Together AI), ensuring consistent methods for:\n",
        "\n",
        "    1. **Token Counting**: Estimating prompt length for context-window management.\n",
        "    2. **Prompting**: Sending chat-formatted messages to the model endpoint.\n",
        "    3. **Response Extraction**: Parsing generated text and log-probabilities.\n",
        "    4. **Dynamic Self-Information Computation**: Converting log-probabilities to\n",
        "       information-theoretic scores in bits, as required by Equation (2) in the\n",
        "       CompactPrompt paper for computing s_dyn(t | c).\n",
        "\n",
        "    The log-probability extraction capability is critical for implementing the\n",
        "    dynamic self-information scoring mechanism described in Section 3.1.2 of the\n",
        "    CompactPrompt paper, where:\n",
        "\n",
        "        s_dyn(t | c) = -log_2(P_model(t | c))\n",
        "\n",
        "    Concrete implementations must handle provider-specific API structures while\n",
        "    exposing a uniform interface to the compression orchestration layer.\n",
        "\n",
        "    Attributes:\n",
        "        None at the abstract level; concrete subclasses define provider-specific\n",
        "        attributes such as API clients and tokenizer instances.\n",
        "\n",
        "    Notes:\n",
        "        - Not all LLM providers expose token-level log-probabilities. The Claude\n",
        "          API, for instance, does not currently support public logprobs access.\n",
        "          Concrete implementations must handle this gracefully by returning None\n",
        "          for the logprobs component when unavailable.\n",
        "        - Tokenizer alignment between the offline corpus and the LLM's native\n",
        "          tokenizer is addressed in Task 14, Step 3.\n",
        "\n",
        "    See Also:\n",
        "        - GPT4oInterface: Concrete implementation for OpenAI GPT-4o family.\n",
        "        - ClaudeInterface: Concrete implementation for Anthropic Claude-3.5.\n",
        "        - LlamaInterface: Concrete implementation for Llama-3.3 via Together AI.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"\n",
        "        Count the number of tokens in the input text using the model's tokenizer.\n",
        "\n",
        "        This method is essential for prompt length estimation and ensuring that\n",
        "        compressed prompts remain within the model's context window constraints.\n",
        "        Accurate token counting is critical for the compression ratio calculations\n",
        "        reported in the CompactPrompt evaluation (Section 5).\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text string to tokenize and count.\n",
        "                Must be a valid UTF-8 encoded string. Empty strings are\n",
        "                permitted and should return 0.\n",
        "\n",
        "        Returns:\n",
        "            int: The total number of tokens in the input text according to\n",
        "                the model's native tokenizer. Returns 0 if tokenization fails\n",
        "                or if the input is empty.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If text is not a string type.\n",
        "            ValueError: If text contains invalid characters that cannot be\n",
        "                tokenized by the model's tokenizer.\n",
        "\n",
        "        Notes:\n",
        "            - Token counts may vary significantly between different tokenizers\n",
        "              (e.g., BPE vs. WordPiece vs. SentencePiece).\n",
        "            - For models where the native tokenizer is not accessible, an\n",
        "              approximation using a similar tokenizer (e.g., tiktoken) may\n",
        "              be employed, with documented variance expectations.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def prompt(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        max_tokens: int = 512,\n",
        "        temperature: float = 0.0,\n",
        "        logprobs: bool = False\n",
        "    ) -> Any:\n",
        "        \"\"\"\n",
        "        Send a chat-formatted prompt to the LLM and retrieve the raw response.\n",
        "\n",
        "        This method implements the core interaction with the LLM API, supporting\n",
        "        both generation tasks (for downstream QA evaluation) and scoring tasks\n",
        "        (for obtaining P_model(t | c) via log-probabilities).\n",
        "\n",
        "        Args:\n",
        "            messages (List[Dict[str, str]]): A list of message dictionaries,\n",
        "                each containing:\n",
        "                - 'role' (str): One of 'system', 'user', or 'assistant'.\n",
        "                - 'content' (str): The message text content.\n",
        "                The list must contain at least one message with role 'user'.\n",
        "\n",
        "            max_tokens (int, optional): Maximum number of tokens to generate\n",
        "                in the response. Defaults to 512. Must be a positive integer\n",
        "                not exceeding the model's maximum generation limit.\n",
        "\n",
        "            temperature (float, optional): Sampling temperature controlling\n",
        "                output randomness. Defaults to 0.0 for deterministic outputs,\n",
        "                which is recommended for reproducible evaluation experiments.\n",
        "                Must be in the range [0.0, 2.0] for most providers.\n",
        "\n",
        "            logprobs (bool, optional): Whether to request token-level log\n",
        "                probabilities in the response. Defaults to False. When True,\n",
        "                the response object will include per-token log-probabilities\n",
        "                required for computing dynamic self-information scores.\n",
        "\n",
        "        Returns:\n",
        "            Any: The raw response object from the provider's API. The exact\n",
        "                type depends on the provider:\n",
        "                - OpenAI: openai.types.chat.ChatCompletion\n",
        "                - Anthropic: anthropic.types.Message\n",
        "                - Together: together.types.ChatCompletion\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If messages is not a list or contains non-dict elements.\n",
        "            ValueError: If messages is empty or lacks a 'user' role message.\n",
        "            ValueError: If max_tokens is not a positive integer.\n",
        "            ValueError: If temperature is outside the valid range.\n",
        "            RuntimeError: If the API call fails due to network or auth errors.\n",
        "\n",
        "        Notes:\n",
        "            - The logprobs parameter may be ignored by providers that do not\n",
        "              support log-probability access (e.g., Anthropic Claude).\n",
        "            - For scoring operations, temperature should be set to 0.0 to\n",
        "              ensure deterministic probability estimates.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def extract_response(\n",
        "        self,\n",
        "        response: Any\n",
        "    ) -> Tuple[str, Optional[List[Dict[str, Any]]]]:\n",
        "        \"\"\"\n",
        "        Extract generated text and log-probabilities from the raw API response.\n",
        "\n",
        "        This method parses the provider-specific response structure to extract:\n",
        "        1. The generated text content for downstream task evaluation.\n",
        "        2. Token-level log-probabilities for dynamic self-information scoring.\n",
        "\n",
        "        Args:\n",
        "            response (Any): The raw response object returned by the prompt()\n",
        "                method. The exact type is provider-specific.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[str, Optional[List[Dict[str, Any]]]]: A tuple containing:\n",
        "\n",
        "                [0] str: The generated text content. For multi-block responses\n",
        "                    (e.g., Anthropic), all text blocks are concatenated.\n",
        "\n",
        "                [1] Optional[List[Dict[str, Any]]]: A list of log-probability\n",
        "                    dictionaries, one per generated token, or None if log-probs\n",
        "                    were not requested or are not supported. Each dictionary\n",
        "                    contains:\n",
        "                    - 'token' (str): The token string.\n",
        "                    - 'logprob' (float): Natural log probability ln(P(t|c)).\n",
        "                    - 'top_logprobs' (List[Dict[str, Any]]): Alternative tokens\n",
        "                      and their log-probabilities (may be empty).\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If response is None or of unexpected type.\n",
        "            AttributeError: If the response object lacks expected attributes.\n",
        "            KeyError: If required fields are missing from the response structure.\n",
        "\n",
        "        Notes:\n",
        "            - Log-probabilities are returned in natural log (base e) as provided\n",
        "              by the API. Use compute_dynamic_self_information() to convert to\n",
        "              bits (base 2) as required by the CompactPrompt scoring equations.\n",
        "            - For providers without logprob support (Claude), this method\n",
        "              returns None for the second tuple element.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def compute_dynamic_self_information(\n",
        "        self,\n",
        "        logprobs_list: List[Dict[str, Any]]\n",
        "    ) -> List[float]:\n",
        "        \"\"\"\n",
        "        Convert natural log probabilities to self-information in bits.\n",
        "\n",
        "        This method implements the conversion from API-provided log-probabilities\n",
        "        (in natural log) to self-information scores in bits, as required by the\n",
        "        dynamic self-information computation in Section 3.1.2 of the CompactPrompt\n",
        "        paper.\n",
        "\n",
        "        The mathematical transformation is:\n",
        "\n",
        "            s_dyn(t | c) = -log_2(P_model(t | c))\n",
        "                         = -ln(P_model(t | c)) / ln(2)\n",
        "\n",
        "        Since API logprobs are provided as ln(P), we compute:\n",
        "\n",
        "            I(t) = -logprob / ln(2)\n",
        "\n",
        "        Args:\n",
        "            logprobs_list (List[Dict[str, Any]]): A list of log-probability\n",
        "                dictionaries as returned by extract_response(). Each dictionary\n",
        "                must contain a 'logprob' key with a float value representing\n",
        "                the natural log probability ln(P(t | c)).\n",
        "\n",
        "        Returns:\n",
        "            List[float]: A list of self-information values in bits, one per\n",
        "                token in the input list. Higher values indicate tokens that\n",
        "                are more surprising (informative) given their context.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If logprobs_list is not a list.\n",
        "            KeyError: If any dictionary in the list lacks a 'logprob' key.\n",
        "            ValueError: If any 'logprob' value is not a valid float.\n",
        "            ValueError: If logprobs_list is empty.\n",
        "\n",
        "        Example:\n",
        "            >>> logprobs = [{'token': 'the', 'logprob': -0.5},\n",
        "            ...             {'token': 'cat', 'logprob': -2.3}]\n",
        "            >>> interface.compute_dynamic_self_information(logprobs)\n",
        "            [0.7213..., 3.3188...]  # Values in bits\n",
        "\n",
        "        Notes:\n",
        "            - The constant ln(2) ≈ 0.693147 is used for the base conversion.\n",
        "            - Self-information values are always non-negative since logprobs\n",
        "              are always ≤ 0 (probabilities are ≤ 1).\n",
        "        \"\"\"\n",
        "        # Validate that logprobs_list is a non-empty list\n",
        "        if not isinstance(logprobs_list, list):\n",
        "            raise TypeError(\n",
        "                f\"logprobs_list must be a list, got {type(logprobs_list).__name__}\"\n",
        "            )\n",
        "\n",
        "        # Validate that the list is not empty\n",
        "        if len(logprobs_list) == 0:\n",
        "            raise ValueError(\"logprobs_list cannot be empty\")\n",
        "\n",
        "        # Compute the natural log of 2 for base conversion (ln(2) ≈ 0.693147)\n",
        "        ln_2 = math.log(2)\n",
        "\n",
        "        # Initialize list to store self-information values in bits\n",
        "        self_info: List[float] = []\n",
        "\n",
        "        # Iterate over each token's log-probability dictionary\n",
        "        for idx, lp in enumerate(logprobs_list):\n",
        "            # Validate that each element is a dictionary\n",
        "            if not isinstance(lp, dict):\n",
        "                raise TypeError(\n",
        "                    f\"Element at index {idx} must be a dict, got {type(lp).__name__}\"\n",
        "                )\n",
        "\n",
        "            # Validate that 'logprob' key exists\n",
        "            if \"logprob\" not in lp:\n",
        "                raise KeyError(\n",
        "                    f\"Element at index {idx} missing required 'logprob' key\"\n",
        "                )\n",
        "\n",
        "            # Extract the natural log probability value\n",
        "            logprob_value = lp[\"logprob\"]\n",
        "\n",
        "            # Validate that logprob is a numeric type\n",
        "            if not isinstance(logprob_value, (int, float)):\n",
        "                raise ValueError(\n",
        "                    f\"logprob at index {idx} must be numeric, got {type(logprob_value).__name__}\"\n",
        "                )\n",
        "\n",
        "            # Convert ln(P) to -log_2(P) using: -ln(P) / ln(2)\n",
        "            # Equation from Section 3.1.2: s_dyn(t|c) = -log_2(P_model(t|c))\n",
        "            self_info_bits = -logprob_value / ln_2\n",
        "\n",
        "            # Append the computed self-information value\n",
        "            self_info.append(self_info_bits)\n",
        "\n",
        "        # Return the list of self-information values in bits\n",
        "        return self_info\n",
        "\n",
        "\n",
        "class GPT4oInterface(LLMInterface):\n",
        "    \"\"\"\n",
        "    Concrete LLM interface implementation for OpenAI's GPT-4o model family.\n",
        "\n",
        "    This class provides access to GPT-4o and GPT-4o-mini models via the OpenAI\n",
        "    API, supporting full log-probability extraction for dynamic self-information\n",
        "    scoring as required by the CompactPrompt pipeline.\n",
        "\n",
        "    The GPT-4o family uses the o200k_base tokenizer (via tiktoken), which must\n",
        "    be aligned with the offline corpus tokenizer as specified in Task 14, Step 3.\n",
        "\n",
        "    Attributes:\n",
        "        client (OpenAI): The initialized OpenAI API client instance.\n",
        "        model (str): The model identifier string (e.g., \"gpt-4o\", \"gpt-4o-mini\").\n",
        "        encoding (tiktoken.Encoding): The tiktoken encoding instance for\n",
        "            the o200k_base vocabulary used by GPT-4o models.\n",
        "\n",
        "    Notes:\n",
        "        - GPT-4o supports token-level log-probabilities via the logprobs parameter.\n",
        "        - The top_logprobs parameter is set to 5 when logprobs are requested,\n",
        "          providing alternative token probabilities for analysis.\n",
        "        - Requires OPENAI_API_KEY environment variable to be set.\n",
        "\n",
        "    Example:\n",
        "        >>> interface = GPT4oInterface(model=\"gpt-4o\")\n",
        "        >>> token_count = interface.count_tokens(\"Hello, world!\")\n",
        "        >>> response = interface.prompt(\n",
        "        ...     messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n",
        "        ...     logprobs=True\n",
        "        ... )\n",
        "        >>> text, logprobs = interface.extract_response(response)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o\") -> None:\n",
        "        \"\"\"\n",
        "        Initialize the GPT-4o interface with API client and tokenizer.\n",
        "\n",
        "        This constructor sets up the OpenAI API client and loads the appropriate\n",
        "        tiktoken encoding for accurate token counting. The o200k_base encoding\n",
        "        is used for all GPT-4o family models.\n",
        "\n",
        "        Args:\n",
        "            model (str, optional): The OpenAI model identifier. Defaults to\n",
        "                \"gpt-4o\". Valid options include:\n",
        "                - \"gpt-4o\": Full GPT-4o model\n",
        "                - \"gpt-4o-mini\": Smaller, faster variant\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If OPENAI_API_KEY environment variable is not set.\n",
        "            RuntimeError: If tiktoken encoding fails to load.\n",
        "\n",
        "        Notes:\n",
        "            - API key is retrieved from the OPENAI_API_KEY environment variable.\n",
        "            - The tiktoken o200k_base encoding provides exact token counts\n",
        "              matching the model's internal tokenization.\n",
        "        \"\"\"\n",
        "        # Validate that model parameter is a non-empty string\n",
        "        if not isinstance(model, str) or len(model.strip()) == 0:\n",
        "            raise TypeError(\"model must be a non-empty string\")\n",
        "\n",
        "        # Retrieve API key from environment variables\n",
        "        api_key: Optional[str] = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "        # Validate that API key is present\n",
        "        if not api_key:\n",
        "            raise ValueError(\n",
        "                \"OPENAI_API_KEY environment variable is not set. \"\n",
        "                \"Please set it to your OpenAI API key.\"\n",
        "            )\n",
        "\n",
        "        # Initialize the OpenAI client with the retrieved API key\n",
        "        self.client: OpenAI = OpenAI(api_key=api_key)\n",
        "\n",
        "        # Store the model identifier for use in API calls\n",
        "        self.model: str = model\n",
        "\n",
        "        # Load the tiktoken encoding for GPT-4o (o200k_base vocabulary)\n",
        "        try:\n",
        "            # o200k_base is the encoding used by GPT-4o family models\n",
        "            self.encoding: tiktoken.Encoding = tiktoken.get_encoding(\"o200k_base\")\n",
        "        except Exception as e:\n",
        "            # Log and re-raise if tokenizer loading fails\n",
        "            logger.error(f\"Failed to load tiktoken encoding 'o200k_base': {e}\")\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to initialize tiktoken encoding: {e}\"\n",
        "            ) from e\n",
        "\n",
        "        # Log successful initialization\n",
        "        logger.info(f\"GPT4oInterface initialized with model: {self.model}\")\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"\n",
        "        Count tokens in the input text using tiktoken's o200k_base encoding.\n",
        "\n",
        "        This method provides exact token counts matching GPT-4o's internal\n",
        "        tokenization, which is essential for accurate compression ratio\n",
        "        calculations and context window management.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to tokenize. Must be a valid string.\n",
        "                Empty strings return 0 tokens.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of tokens in the input text according to the\n",
        "                o200k_base encoding.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If text is not a string.\n",
        "\n",
        "        Example:\n",
        "            >>> interface = GPT4oInterface()\n",
        "            >>> interface.count_tokens(\"Hello, world!\")\n",
        "            4\n",
        "        \"\"\"\n",
        "        # Validate input type\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError(f\"text must be a string, got {type(text).__name__}\")\n",
        "\n",
        "        # Handle empty string case\n",
        "        if len(text) == 0:\n",
        "            return 0\n",
        "\n",
        "        # Encode the text to token IDs using tiktoken\n",
        "        token_ids: List[int] = self.encoding.encode(text)\n",
        "\n",
        "        # Return the count of token IDs\n",
        "        return len(token_ids)\n",
        "\n",
        "    def prompt(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        max_tokens: int = 512,\n",
        "        temperature: float = 0.0,\n",
        "        logprobs: bool = False\n",
        "    ) -> Any:\n",
        "        \"\"\"\n",
        "        Send a chat completion request to the GPT-4o API.\n",
        "\n",
        "        This method wraps the OpenAI chat completions endpoint, supporting\n",
        "        both standard generation and log-probability extraction for dynamic\n",
        "        self-information scoring.\n",
        "\n",
        "        Args:\n",
        "            messages (List[Dict[str, str]]): List of chat messages, each with\n",
        "                'role' and 'content' keys. Must contain at least one message.\n",
        "\n",
        "            max_tokens (int, optional): Maximum tokens to generate. Defaults\n",
        "                to 512. Must be positive and ≤ model's maximum.\n",
        "\n",
        "            temperature (float, optional): Sampling temperature in [0.0, 2.0].\n",
        "                Defaults to 0.0 for deterministic output.\n",
        "\n",
        "            logprobs (bool, optional): Whether to return token log-probabilities.\n",
        "                Defaults to False. When True, top_logprobs is set to 5.\n",
        "\n",
        "        Returns:\n",
        "            Any: OpenAI ChatCompletion response object containing the generated\n",
        "                text and optionally log-probabilities.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If messages is not a list of dictionaries.\n",
        "            ValueError: If messages is empty or malformed.\n",
        "            ValueError: If max_tokens is not positive.\n",
        "            ValueError: If temperature is outside [0.0, 2.0].\n",
        "            RuntimeError: If the API call fails.\n",
        "        \"\"\"\n",
        "        # Validate messages parameter type\n",
        "        if not isinstance(messages, list):\n",
        "            raise TypeError(f\"messages must be a list, got {type(messages).__name__}\")\n",
        "\n",
        "        # Validate messages is non-empty\n",
        "        if len(messages) == 0:\n",
        "            raise ValueError(\"messages list cannot be empty\")\n",
        "\n",
        "        # Validate each message has required keys\n",
        "        for idx, msg in enumerate(messages):\n",
        "            if not isinstance(msg, dict):\n",
        "                raise TypeError(f\"Message at index {idx} must be a dict\")\n",
        "            if \"role\" not in msg or \"content\" not in msg:\n",
        "                raise ValueError(\n",
        "                    f\"Message at index {idx} must have 'role' and 'content' keys\"\n",
        "                )\n",
        "\n",
        "        # Validate max_tokens is a positive integer\n",
        "        if not isinstance(max_tokens, int) or max_tokens <= 0:\n",
        "            raise ValueError(f\"max_tokens must be a positive integer, got {max_tokens}\")\n",
        "\n",
        "        # Validate temperature is within acceptable range\n",
        "        if not isinstance(temperature, (int, float)) or not (0.0 <= temperature <= 2.0):\n",
        "            raise ValueError(f\"temperature must be in [0.0, 2.0], got {temperature}\")\n",
        "\n",
        "        # Determine top_logprobs value based on logprobs flag\n",
        "        # When logprobs=True, request top 5 alternative tokens per position\n",
        "        top_logprobs_value: Optional[int] = 5 if logprobs else None\n",
        "\n",
        "        try:\n",
        "            # Call the OpenAI chat completions API\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                logprobs=logprobs,\n",
        "                top_logprobs=top_logprobs_value\n",
        "            )\n",
        "\n",
        "            # Return the raw response object\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log the error and re-raise with context\n",
        "            logger.error(f\"OpenAI API call failed: {e}\")\n",
        "            raise RuntimeError(f\"OpenAI API call failed: {e}\") from e\n",
        "\n",
        "    def extract_response(\n",
        "        self,\n",
        "        response: Any\n",
        "    ) -> Tuple[str, Optional[List[Dict[str, Any]]]]:\n",
        "        \"\"\"\n",
        "        Extract generated text and log-probabilities from GPT-4o response.\n",
        "\n",
        "        This method parses the OpenAI ChatCompletion response structure to\n",
        "        extract the generated content and, if available, token-level log-\n",
        "        probabilities for dynamic self-information computation.\n",
        "\n",
        "        Args:\n",
        "            response (Any): The ChatCompletion response object returned by\n",
        "                the prompt() method.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[str, Optional[List[Dict[str, Any]]]]: A tuple containing:\n",
        "\n",
        "                [0] str: The generated text from the first choice.\n",
        "\n",
        "                [1] Optional[List[Dict[str, Any]]]: List of logprob dicts if\n",
        "                    logprobs were requested, otherwise None. Each dict contains:\n",
        "                    - 'token' (str): The generated token.\n",
        "                    - 'logprob' (float): Natural log probability.\n",
        "                    - 'top_logprobs' (List[Dict]): Top 5 alternative tokens.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If response is None.\n",
        "            AttributeError: If response lacks expected structure.\n",
        "        \"\"\"\n",
        "        # Validate response is not None\n",
        "        if response is None:\n",
        "            raise TypeError(\"response cannot be None\")\n",
        "\n",
        "        # Validate response has choices attribute\n",
        "        if not hasattr(response, \"choices\") or len(response.choices) == 0:\n",
        "            raise AttributeError(\"response must have non-empty 'choices' attribute\")\n",
        "\n",
        "        # Extract the generated text from the first choice's message content\n",
        "        generated_text: str = response.choices[0].message.content\n",
        "\n",
        "        # Handle case where content might be None\n",
        "        if generated_text is None:\n",
        "            generated_text = \"\"\n",
        "\n",
        "        # Initialize logprobs_data as None (will be populated if available)\n",
        "        logprobs_data: Optional[List[Dict[str, Any]]] = None\n",
        "\n",
        "        # Check if log-probabilities are present in the response\n",
        "        if response.choices[0].logprobs is not None:\n",
        "            # Initialize list to store parsed logprob dictionaries\n",
        "            logprobs_data = []\n",
        "\n",
        "            # Iterate over each token's logprob data in the response\n",
        "            for token_data in response.choices[0].logprobs.content:\n",
        "                # Build the top_logprobs list for alternative tokens\n",
        "                top_logprobs_list: List[Dict[str, Any]] = []\n",
        "\n",
        "                # Check if top_logprobs exists for this token\n",
        "                if token_data.top_logprobs:\n",
        "                    # Extract each alternative token and its log probability\n",
        "                    for alt in token_data.top_logprobs:\n",
        "                        top_logprobs_list.append({\n",
        "                            \"token\": alt.token,\n",
        "                            \"logprob\": alt.logprob\n",
        "                        })\n",
        "\n",
        "                # Append the complete logprob dictionary for this token\n",
        "                logprobs_data.append({\n",
        "                    \"token\": token_data.token,\n",
        "                    \"logprob\": token_data.logprob,\n",
        "                    \"top_logprobs\": top_logprobs_list\n",
        "                })\n",
        "\n",
        "        # Return the extracted text and logprobs tuple\n",
        "        return generated_text, logprobs_data\n",
        "\n",
        "\n",
        "class ClaudeInterface(LLMInterface):\n",
        "    \"\"\"\n",
        "    Concrete LLM interface implementation for Anthropic's Claude-3.5-Sonnet model.\n",
        "\n",
        "    This class provides access to Claude models via the Anthropic API. Note that\n",
        "    Claude does not currently expose token-level log-probabilities through its\n",
        "    public API, which limits its use as a scorer LLM for dynamic self-information\n",
        "    computation. However, Claude remains a valid target LLM for downstream task\n",
        "    evaluation.\n",
        "\n",
        "    Attributes:\n",
        "        client (Anthropic): The initialized Anthropic API client instance.\n",
        "        model (str): The model identifier string (e.g., \"claude-3-5-sonnet-20241022\").\n",
        "        encoding (tiktoken.Encoding): A tiktoken encoding instance (cl100k_base)\n",
        "            used as an approximation for offline token counting when the API\n",
        "            count_tokens endpoint is unavailable.\n",
        "\n",
        "    Notes:\n",
        "        - Claude does NOT support public log-probability access; extract_response()\n",
        "          always returns None for the logprobs component.\n",
        "        - Token counting uses Claude's native count_tokens API when available,\n",
        "          falling back to tiktoken cl100k_base approximation.\n",
        "        - Requires ANTHROPIC_API_KEY environment variable to be set.\n",
        "\n",
        "    Example:\n",
        "        >>> interface = ClaudeInterface(model=\"claude-3-5-sonnet-20241022\")\n",
        "        >>> token_count = interface.count_tokens(\"Hello, world!\")\n",
        "        >>> response = interface.prompt(\n",
        "        ...     messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n",
        "        ... )\n",
        "        >>> text, logprobs = interface.extract_response(response)\n",
        "        >>> assert logprobs is None  # Claude does not support logprobs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"claude-3-5-sonnet-20241022\") -> None:\n",
        "        \"\"\"\n",
        "        Initialize the Claude interface with API client and fallback tokenizer.\n",
        "\n",
        "        This constructor sets up the Anthropic API client and loads a tiktoken\n",
        "        encoding for approximate token counting when the API endpoint is\n",
        "        unavailable.\n",
        "\n",
        "        Args:\n",
        "            model (str, optional): The Anthropic model identifier. Defaults to\n",
        "                \"claude-3-5-sonnet-20241022\".\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If ANTHROPIC_API_KEY environment variable is not set.\n",
        "            RuntimeError: If tiktoken encoding fails to load.\n",
        "\n",
        "        Notes:\n",
        "            - The cl100k_base encoding is used as an approximation for Claude's\n",
        "              tokenization; exact counts are obtained via the API when possible.\n",
        "        \"\"\"\n",
        "        # Validate that model parameter is a non-empty string\n",
        "        if not isinstance(model, str) or len(model.strip()) == 0:\n",
        "            raise TypeError(\"model must be a non-empty string\")\n",
        "\n",
        "        # Retrieve API key from environment variables\n",
        "        api_key: Optional[str] = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "        # Validate that API key is present\n",
        "        if not api_key:\n",
        "            raise ValueError(\n",
        "                \"ANTHROPIC_API_KEY environment variable is not set. \"\n",
        "                \"Please set it to your Anthropic API key.\"\n",
        "            )\n",
        "\n",
        "        # Initialize the Anthropic client with the retrieved API key\n",
        "        self.client: Anthropic = Anthropic(api_key=api_key)\n",
        "\n",
        "        # Store the model identifier for use in API calls\n",
        "        self.model: str = model\n",
        "\n",
        "        # Load tiktoken encoding as fallback for token counting\n",
        "        try:\n",
        "            # cl100k_base provides reasonable approximation for Claude tokenization\n",
        "            self.encoding: tiktoken.Encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        except Exception as e:\n",
        "            # Log and re-raise if tokenizer loading fails\n",
        "            logger.error(f\"Failed to load tiktoken encoding 'cl100k_base': {e}\")\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to initialize tiktoken encoding: {e}\"\n",
        "            ) from e\n",
        "\n",
        "        # Log successful initialization with warning about logprobs limitation\n",
        "        logger.info(\n",
        "            f\"ClaudeInterface initialized with model: {self.model}. \"\n",
        "            \"Note: Claude does not support log-probability extraction.\"\n",
        "        )\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"\n",
        "        Count tokens using Claude's API with tiktoken fallback.\n",
        "\n",
        "        This method attempts to use Anthropic's native count_tokens API endpoint\n",
        "        for exact token counts. If the API call fails, it falls back to tiktoken\n",
        "        cl100k_base approximation.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to tokenize. Must be a valid string.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of tokens in the input text. Exact count from API\n",
        "                when available, approximate count from tiktoken otherwise.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If text is not a string.\n",
        "\n",
        "        Notes:\n",
        "            - API-based counting is preferred for accuracy.\n",
        "            - Tiktoken cl100k_base may undercount or overcount by ~5-10%.\n",
        "        \"\"\"\n",
        "        # Validate input type\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError(f\"text must be a string, got {type(text).__name__}\")\n",
        "\n",
        "        # Handle empty string case\n",
        "        if len(text) == 0:\n",
        "            return 0\n",
        "\n",
        "        try:\n",
        "            # Attempt to use Anthropic's native token counting API\n",
        "            response = self.client.messages.count_tokens(\n",
        "                model=self.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": text}]\n",
        "            )\n",
        "\n",
        "            # Return the exact token count from the API response\n",
        "            return response.input_tokens\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log the fallback to tiktoken\n",
        "            logger.warning(\n",
        "                f\"Claude count_tokens API failed: {e}. \"\n",
        "                \"Falling back to tiktoken approximation.\"\n",
        "            )\n",
        "\n",
        "            # Use tiktoken cl100k_base as approximation\n",
        "            token_ids: List[int] = self.encoding.encode(text)\n",
        "\n",
        "            # Return the approximate token count\n",
        "            return len(token_ids)\n",
        "\n",
        "    def prompt(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        max_tokens: int = 1024,\n",
        "        temperature: float = 0.0,\n",
        "        logprobs: bool = False\n",
        "    ) -> Any:\n",
        "        \"\"\"\n",
        "        Send a message request to the Claude API.\n",
        "\n",
        "        This method wraps the Anthropic messages endpoint. Note that the logprobs\n",
        "        parameter is accepted for interface compatibility but is ignored, as\n",
        "        Claude does not support log-probability extraction.\n",
        "\n",
        "        Args:\n",
        "            messages (List[Dict[str, str]]): List of chat messages with 'role'\n",
        "                and 'content' keys. Must contain at least one message.\n",
        "\n",
        "            max_tokens (int, optional): Maximum tokens to generate. Defaults\n",
        "                to 1024 (higher default for Claude's longer responses).\n",
        "\n",
        "            temperature (float, optional): Sampling temperature in [0.0, 1.0].\n",
        "                Defaults to 0.0 for deterministic output.\n",
        "\n",
        "            logprobs (bool, optional): Ignored parameter (Claude does not\n",
        "                support logprobs). Included for interface compatibility.\n",
        "\n",
        "        Returns:\n",
        "            Any: Anthropic Message response object containing the generated text.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If messages is not a list of dictionaries.\n",
        "            ValueError: If messages is empty or malformed.\n",
        "            ValueError: If max_tokens is not positive.\n",
        "            ValueError: If temperature is outside valid range.\n",
        "            RuntimeError: If the API call fails.\n",
        "        \"\"\"\n",
        "        # Validate messages parameter type\n",
        "        if not isinstance(messages, list):\n",
        "            raise TypeError(f\"messages must be a list, got {type(messages).__name__}\")\n",
        "\n",
        "        # Validate messages is non-empty\n",
        "        if len(messages) == 0:\n",
        "            raise ValueError(\"messages list cannot be empty\")\n",
        "\n",
        "        # Validate each message has required keys\n",
        "        for idx, msg in enumerate(messages):\n",
        "            if not isinstance(msg, dict):\n",
        "                raise TypeError(f\"Message at index {idx} must be a dict\")\n",
        "            if \"role\" not in msg or \"content\" not in msg:\n",
        "                raise ValueError(\n",
        "                    f\"Message at index {idx} must have 'role' and 'content' keys\"\n",
        "                )\n",
        "\n",
        "        # Validate max_tokens is a positive integer\n",
        "        if not isinstance(max_tokens, int) or max_tokens <= 0:\n",
        "            raise ValueError(f\"max_tokens must be a positive integer, got {max_tokens}\")\n",
        "\n",
        "        # Validate temperature is within acceptable range (Claude uses [0, 1])\n",
        "        if not isinstance(temperature, (int, float)) or not (0.0 <= temperature <= 1.0):\n",
        "            raise ValueError(f\"temperature must be in [0.0, 1.0] for Claude, got {temperature}\")\n",
        "\n",
        "        # Log warning if logprobs was requested (not supported by Claude)\n",
        "        if logprobs:\n",
        "            logger.warning(\n",
        "                \"logprobs=True was requested but Claude does not support \"\n",
        "                \"log-probability extraction. This parameter will be ignored.\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # Call the Anthropic messages API (logprobs parameter not passed)\n",
        "            response = self.client.messages.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            # Return the raw response object\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log the error and re-raise with context\n",
        "            logger.error(f\"Anthropic API call failed: {e}\")\n",
        "            raise RuntimeError(f\"Anthropic API call failed: {e}\") from e\n",
        "\n",
        "    def extract_response(\n",
        "        self,\n",
        "        response: Any\n",
        "    ) -> Tuple[str, None]:\n",
        "        \"\"\"\n",
        "        Extract generated text from Claude response (logprobs not available).\n",
        "\n",
        "        This method parses the Anthropic Message response structure to extract\n",
        "        the generated text content. Log-probabilities are not available from\n",
        "        Claude's public API.\n",
        "\n",
        "        Args:\n",
        "            response (Any): The Message response object returned by prompt().\n",
        "\n",
        "        Returns:\n",
        "            Tuple[str, None]: A tuple containing:\n",
        "                [0] str: The concatenated text from all content blocks.\n",
        "                [1] None: Always None (Claude does not support logprobs).\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If response is None.\n",
        "            AttributeError: If response lacks expected structure.\n",
        "        \"\"\"\n",
        "        # Validate response is not None\n",
        "        if response is None:\n",
        "            raise TypeError(\"response cannot be None\")\n",
        "\n",
        "        # Validate response has content attribute\n",
        "        if not hasattr(response, \"content\"):\n",
        "            raise AttributeError(\"response must have 'content' attribute\")\n",
        "\n",
        "        # Initialize generated text accumulator\n",
        "        generated_text: str = \"\"\n",
        "\n",
        "        # Iterate over content blocks in the response\n",
        "        for block in response.content:\n",
        "            # Check if this block contains text content\n",
        "            if hasattr(block, \"type\") and block.type == \"text\":\n",
        "                # Append the text content to the accumulator\n",
        "                generated_text += block.text\n",
        "\n",
        "        # Return text and None for logprobs (Claude does not support logprobs)\n",
        "        return generated_text, None\n",
        "\n",
        "\n",
        "class LlamaInterface(LLMInterface):\n",
        "    \"\"\"\n",
        "    Concrete LLM interface implementation for Llama-3.3-70B-Instruct via Together AI.\n",
        "\n",
        "    This class provides access to Llama models hosted on Together AI's inference\n",
        "    platform. It enforces the use of the correct Hugging Face tokenizer to ensure\n",
        "    perfect alignment between local token counting/indexing and the model's\n",
        "    internal tokenization. This alignment is critical for the accuracy of\n",
        "    dynamic self-information scoring.\n",
        "\n",
        "    Attributes:\n",
        "        client (Together): The initialized Together AI client instance.\n",
        "        model (str): The model identifier on Together AI's platform\n",
        "            (e.g., \"meta-llama/Llama-3.3-70B-Instruct-Turbo\").\n",
        "        tokenizer (AutoTokenizer): The Hugging Face tokenizer for exact token counting.\n",
        "\n",
        "    Notes:\n",
        "        - Requires TOGETHER_API_KEY environment variable.\n",
        "        - Requires HF_TOKEN environment variable for accessing the gated Llama tokenizer.\n",
        "        - Raises RuntimeError if the specific Llama tokenizer cannot be loaded;\n",
        "          does NOT fall back to tiktoken to prevent data corruption.\n",
        "\n",
        "    Example:\n",
        "        >>> interface = LlamaInterface()\n",
        "        >>> token_count = interface.count_tokens(\"Hello, world!\")\n",
        "        >>> response = interface.prompt(\n",
        "        ...     messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n",
        "        ...     logprobs=True\n",
        "        ... )\n",
        "        >>> text, logprobs = interface.extract_response(response)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the Llama interface with Together AI client and HF tokenizer.\n",
        "\n",
        "        Args:\n",
        "            model (str, optional): The model identifier on Together AI.\n",
        "                Defaults to \"meta-llama/Llama-3.3-70B-Instruct-Turbo\".\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If TOGETHER_API_KEY is missing.\n",
        "            ImportError: If 'together' or 'transformers' libraries are missing.\n",
        "            RuntimeError: If the Hugging Face tokenizer cannot be loaded (e.g., invalid HF_TOKEN).\n",
        "        \"\"\"\n",
        "        # Validate dependencies\n",
        "        if Together is None:\n",
        "            raise ImportError(\"The 'together' library is required. Install it via `pip install together`.\")\n",
        "        if AutoTokenizer is None:\n",
        "            raise ImportError(\"The 'transformers' library is required. Install it via `pip install transformers`.\")\n",
        "\n",
        "        # Validate model parameter\n",
        "        if not isinstance(model, str) or not model.strip():\n",
        "            raise TypeError(\"model must be a non-empty string\")\n",
        "\n",
        "        # Retrieve API key\n",
        "        api_key = os.environ.get(\"TOGETHER_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\n",
        "                \"TOGETHER_API_KEY environment variable is not set. \"\n",
        "                \"Please set it to your Together AI API key.\"\n",
        "            )\n",
        "\n",
        "        # Initialize Client\n",
        "        self.client = Together(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "        # Initialize Tokenizer\n",
        "        # We strictly require the correct tokenizer. No fallbacks.\n",
        "        hf_model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Loading tokenizer for {hf_model_id}...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                hf_model_id,\n",
        "                token=hf_token,\n",
        "                use_fast=True\n",
        "            )\n",
        "            logger.info(\"Successfully loaded Llama tokenizer.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load Llama tokenizer: {e}\")\n",
        "            raise RuntimeError(\n",
        "                f\"Could not load tokenizer for '{hf_model_id}'. \"\n",
        "                \"Ensure HF_TOKEN is set and you have access to the gated model. \"\n",
        "                \"Strict token alignment is required; cannot proceed without correct tokenizer.\"\n",
        "            ) from e\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"\n",
        "        Count tokens using the loaded Hugging Face tokenizer.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text.\n",
        "\n",
        "        Returns:\n",
        "            int: The exact number of tokens.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If text is not a string.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            raise TypeError(f\"text must be a string, got {type(text).__name__}\")\n",
        "\n",
        "        if not text:\n",
        "            return 0\n",
        "\n",
        "        # Encode with special tokens to match model behavior\n",
        "        token_ids = self.tokenizer.encode(text, add_special_tokens=True)\n",
        "        return len(token_ids)\n",
        "\n",
        "    def prompt(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        max_tokens: int = 512,\n",
        "        temperature: float = 0.0,\n",
        "        logprobs: bool = False\n",
        "    ) -> Any:\n",
        "        \"\"\"\n",
        "        Send a chat completion request to Together AI.\n",
        "\n",
        "        Args:\n",
        "            messages (List[Dict]): Chat messages.\n",
        "            max_tokens (int): Max tokens to generate.\n",
        "            temperature (float): Sampling temperature.\n",
        "            logprobs (bool): Whether to return log-probabilities.\n",
        "\n",
        "        Returns:\n",
        "            Any: The raw API response object.\n",
        "        \"\"\"\n",
        "        # Input validation\n",
        "        if not isinstance(messages, list) or not messages:\n",
        "            raise ValueError(\"messages must be a non-empty list of dicts\")\n",
        "        if not isinstance(max_tokens, int) or max_tokens <= 0:\n",
        "            raise ValueError(\"max_tokens must be a positive integer\")\n",
        "        if not isinstance(temperature, (int, float)) or temperature < 0:\n",
        "            raise ValueError(\"temperature must be non-negative\")\n",
        "\n",
        "        # Together AI uses 'logprobs' parameter (int 1 for True) in some endpoints,\n",
        "        # but for chat completions, support varies. We assume standard usage.\n",
        "        # Note: As of late 2024, Together's chat endpoint might not return logprobs\n",
        "        # for all models. If strictly needed for scoring, we might use the completion\n",
        "        # endpoint in 'score_prompt_tokens', but here we implement the standard chat interface.\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                logprobs=1 if logprobs else 0\n",
        "            )\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Together AI API call failed: {e}\")\n",
        "            raise RuntimeError(f\"Together AI API call failed: {e}\") from e\n",
        "\n",
        "    def extract_response(\n",
        "        self,\n",
        "        response: Any\n",
        "    ) -> Tuple[str, Optional[List[Dict[str, Any]]]]:\n",
        "        \"\"\"\n",
        "        Extract generated text and log-probabilities from the response.\n",
        "\n",
        "        Args:\n",
        "            response (Any): The API response object.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[str, Optional[List[Dict]]]: Generated text and logprobs list.\n",
        "        \"\"\"\n",
        "        if response is None:\n",
        "            raise TypeError(\"response cannot be None\")\n",
        "\n",
        "        if not hasattr(response, \"choices\") or not response.choices:\n",
        "            raise AttributeError(\"Response missing 'choices'\")\n",
        "\n",
        "        choice = response.choices[0]\n",
        "\n",
        "        # Extract text\n",
        "        if hasattr(choice, \"message\") and choice.message:\n",
        "            text = choice.message.content or \"\"\n",
        "        else:\n",
        "            text = \"\"\n",
        "\n",
        "        # Extract logprobs\n",
        "        logprobs_data = None\n",
        "\n",
        "        # Check for logprobs in the response structure\n",
        "        # Together AI structure for chat logprobs can vary; typically under 'logprobs'\n",
        "        if hasattr(choice, \"logprobs\") and choice.logprobs:\n",
        "            # Assuming OpenAI-like structure if present\n",
        "            if hasattr(choice.logprobs, \"token_logprobs\"):\n",
        "                # Completion-style\n",
        "                tokens = choice.logprobs.tokens\n",
        "                values = choice.logprobs.token_logprobs\n",
        "                logprobs_data = []\n",
        "                for t, lp in zip(tokens, values):\n",
        "                    logprobs_data.append({\n",
        "                        \"token\": t,\n",
        "                        \"logprob\": lp,\n",
        "                        \"top_logprobs\": [] # Often not returned in simple mode\n",
        "                    })\n",
        "            elif hasattr(choice.logprobs, \"content\"):\n",
        "                # Chat-style (like OpenAI)\n",
        "                logprobs_data = []\n",
        "                for item in choice.logprobs.content:\n",
        "                    logprobs_data.append({\n",
        "                        \"token\": item.token,\n",
        "                        \"logprob\": item.logprob,\n",
        "                        \"top_logprobs\": [] # Populate if available\n",
        "                    })\n",
        "\n",
        "        return text, logprobs_data\n",
        "\n",
        "def create_llm_interface(model_name: str) -> LLMInterface:\n",
        "    \"\"\"\n",
        "    Factory function to instantiate the appropriate LLM interface for a given model.\n",
        "\n",
        "    This function provides a centralized mechanism for creating LLM interface\n",
        "    instances, abstracting away provider-specific initialization details and\n",
        "    ensuring consistent model identifier mapping across the CompactPrompt pipeline.\n",
        "\n",
        "    The factory pattern enables seamless switching between scorer and target LLMs\n",
        "    as specified in Task 14, supporting the four models evaluated in the\n",
        "    CompactPrompt paper:\n",
        "    - GPT-4-Omni (OpenAI)\n",
        "    - GPT-4.1-Mini (OpenAI)\n",
        "    - Claude-3.5-Sonnet (Anthropic)\n",
        "    - Llama-3.3-70B-Instruct (Together AI)\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Human-readable model name. Must be one of:\n",
        "            - \"GPT-4o\"\n",
        "            - \"GPT-4o-mini\"\n",
        "            - \"Claude-3.5-Sonnet\"\n",
        "            - \"Llama-3.3-70B-Instruct\"\n",
        "\n",
        "    Returns:\n",
        "        LLMInterface: A concrete LLMInterface implementation instance\n",
        "            configured for the specified model.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If model_name is not a string.\n",
        "        ValueError: If model_name is not recognized.\n",
        "\n",
        "    Example:\n",
        "        >>> scorer = create_llm_interface(\"GPT-4o\")\n",
        "        >>> target = create_llm_interface(\"Claude-3.5-Sonnet\")\n",
        "        >>> token_count = scorer.count_tokens(\"Hello, world!\")\n",
        "\n",
        "    Notes:\n",
        "        - Each interface class handles its own API key retrieval from\n",
        "          environment variables.\n",
        "        - For Claude, note that logprobs are not available; use GPT-4o\n",
        "          or Llama for scoring tasks requiring dynamic self-information.\n",
        "    \"\"\"\n",
        "    # Validate input type\n",
        "    if not isinstance(model_name, str):\n",
        "        raise TypeError(\n",
        "            f\"model_name must be a string, got {type(model_name).__name__}\"\n",
        "        )\n",
        "\n",
        "    # Define mapping from human-readable names to (model_id, interface_class)\n",
        "    model_map: Dict[str, Tuple[str, type]] = {\n",
        "        \"GPT-4o\": (\"gpt-4o\", GPT4oInterface),\n",
        "        \"GPT-4o-mini\": (\"gpt-4o-mini\", GPT4oInterface),\n",
        "        \"Claude-3.5-Sonnet\": (\"claude-3-5-sonnet-20241022\", ClaudeInterface),\n",
        "        \"Llama-3.3-70B-Instruct\": (\n",
        "            \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
        "            LlamaInterface\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Validate that model_name is in the supported set\n",
        "    if model_name not in model_map:\n",
        "        raise ValueError(\n",
        "            f\"Unknown model: '{model_name}'. \"\n",
        "            f\"Supported models: {list(model_map.keys())}\"\n",
        "        )\n",
        "\n",
        "    # Retrieve the model identifier and interface class\n",
        "    model_id, interface_class = model_map[model_name]\n",
        "\n",
        "    # Instantiate the appropriate interface with the model identifier\n",
        "    interface: LLMInterface = interface_class(model=model_id)\n",
        "\n",
        "    # Log the successful creation\n",
        "    logger.info(f\"Created LLM interface for model: {model_name}\")\n",
        "\n",
        "    # Return the configured interface instance\n",
        "    return interface\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Factory and Orchestrator\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def create_llm_interface(model_name: str) -> LLMInterface:\n",
        "    \"\"\"\n",
        "    Factory function to instantiate the appropriate LLM interface based on the model name.\n",
        "\n",
        "    This function serves as a centralized factory for creating concrete LLMInterface\n",
        "    instances. It maps human-readable model names (e.g., \"GPT-4o\") to their\n",
        "    corresponding implementation classes (e.g., GPT4oInterface) and specific model\n",
        "    identifiers required by the provider APIs.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The standardized model name. Must be one of:\n",
        "            - \"GPT-4o\"\n",
        "            - \"GPT-4o-mini\"\n",
        "            - \"Claude-3.5-Sonnet\"\n",
        "            - \"Llama-3.3-70B-Instruct\"\n",
        "\n",
        "    Returns:\n",
        "        LLMInterface: An initialized concrete implementation of LLMInterface\n",
        "            configured for the specified model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the model_name is not recognized in the internal mapping.\n",
        "        TypeError: If model_name is not a string.\n",
        "    \"\"\"\n",
        "    # Validate input type\n",
        "    if not isinstance(model_name, str):\n",
        "        raise TypeError(f\"model_name must be a string, got {type(model_name).__name__}\")\n",
        "\n",
        "    # Normalize model name map\n",
        "    # Maps standardized keys to (API model identifier, Interface Class)\n",
        "    model_map: Dict[str, Tuple[str, type]] = {\n",
        "        \"GPT-4o\": (\"gpt-4o\", GPT4oInterface),\n",
        "        \"GPT-4o-mini\": (\"gpt-4o-mini\", GPT4oInterface),\n",
        "        \"Claude-3.5-Sonnet\": (\"claude-3-5-sonnet-20241022\", ClaudeInterface),\n",
        "        \"Llama-3.3-70B-Instruct\": (\"meta-llama/Llama-3.3-70B-Instruct-Turbo\", LlamaInterface)\n",
        "    }\n",
        "\n",
        "    # Check if the requested model is supported\n",
        "    if model_name not in model_map:\n",
        "        raise ValueError(\n",
        "            f\"Unknown model: '{model_name}'. \"\n",
        "            f\"Supported models: {list(model_map.keys())}\"\n",
        "        )\n",
        "\n",
        "    # Retrieve the API identifier and the class constructor\n",
        "    model_id, interface_class = model_map[model_name]\n",
        "\n",
        "    # Instantiate and return the interface\n",
        "    return interface_class(model=model_id)\n",
        "\n",
        "\n",
        "def configure_llm_resources(study_config: Dict[str, Any]) -> Dict[str, LLMInterface]:\n",
        "    \"\"\"\n",
        "    Orchestrates the configuration of all LLM resources defined in the study configuration.\n",
        "\n",
        "    This function iterates through the configured scorer and target LLMs specified\n",
        "    in the study configuration dictionary. It instantiates the appropriate\n",
        "    LLMInterface for each unique model found, handling normalization of model names\n",
        "    to match the factory's expected keys.\n",
        "\n",
        "    The resulting dictionary serves as a registry of ready-to-use LLM interfaces\n",
        "    for the entire pipeline, ensuring that clients and tokenizers are initialized\n",
        "    only once.\n",
        "\n",
        "    Args:\n",
        "        study_config (Dict[str, Any]): The full study configuration dictionary.\n",
        "            Expected to contain 'llm_config' with 'scorer_llm_options' and\n",
        "            'target_llms_for_evaluation' lists.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, LLMInterface]: A dictionary where keys are standardized model names\n",
        "            (e.g., \"GPT-4o\") and values are the initialized LLMInterface objects.\n",
        "    \"\"\"\n",
        "    logger.info(\"Configuring LLM resources...\")\n",
        "\n",
        "    resources: Dict[str, LLMInterface] = {}\n",
        "    llm_config = study_config.get(\"llm_config\", {})\n",
        "\n",
        "    # Collect all unique model names from scorers and targets to avoid duplicate initialization\n",
        "    model_names: Set[str] = set()\n",
        "\n",
        "    # Process scorer LLMs\n",
        "    for scorer in llm_config.get(\"scorer_llm_options\", []):\n",
        "        raw_name = scorer.get(\"model_name\", \"\")\n",
        "        if not raw_name:\n",
        "            continue\n",
        "\n",
        "        # Normalize raw config names to factory keys\n",
        "        # This logic handles potential case variations or partial matches from the config\n",
        "        name_lower = raw_name.lower()\n",
        "        if \"gpt-4o\" in name_lower and \"mini\" not in name_lower:\n",
        "            key = \"GPT-4o\"\n",
        "        elif \"mini\" in name_lower:\n",
        "            key = \"GPT-4o-mini\"\n",
        "        elif \"claude\" in name_lower:\n",
        "            key = \"Claude-3.5-Sonnet\"\n",
        "        elif \"llama\" in name_lower:\n",
        "            key = \"Llama-3.3-70B-Instruct\"\n",
        "        else:\n",
        "            # Fallback to the raw name if no heuristic matches (will likely raise ValueError in factory)\n",
        "            key = raw_name\n",
        "\n",
        "        model_names.add(key)\n",
        "\n",
        "    # Process target LLMs\n",
        "    for target in llm_config.get(\"target_llms_for_evaluation\", []):\n",
        "        raw_name = target.get(\"model_name\", \"\")\n",
        "        if not raw_name:\n",
        "            continue\n",
        "\n",
        "        # Apply same normalization logic\n",
        "        name_lower = raw_name.lower()\n",
        "        if \"gpt-4o\" in name_lower and \"mini\" not in name_lower:\n",
        "            key = \"GPT-4o\"\n",
        "        elif \"mini\" in name_lower:\n",
        "            key = \"GPT-4o-mini\"\n",
        "        elif \"claude\" in name_lower:\n",
        "            key = \"Claude-3.5-Sonnet\"\n",
        "        elif \"llama\" in name_lower:\n",
        "            key = \"Llama-3.3-70B-Instruct\"\n",
        "        else:\n",
        "            key = raw_name\n",
        "\n",
        "        model_names.add(key)\n",
        "\n",
        "    # Instantiate interfaces for all identified unique models\n",
        "    for name in model_names:\n",
        "        try:\n",
        "            interface = create_llm_interface(name)\n",
        "            resources[name] = interface\n",
        "            logger.info(f\"Successfully configured interface for {name}\")\n",
        "        except ValueError as e:\n",
        "            logger.error(f\"Failed to configure interface for {name}: {e}\")\n",
        "            # We continue to try other models rather than crashing entirely\n",
        "\n",
        "    return resources\n"
      ],
      "metadata": {
        "id": "5ZkwL71uok4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 – Tokenize Prompts for Dynamic Scoring\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Tokenize Prompts for Dynamic Scoring\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Serialize example to prompt string\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def serialize_example_to_prompt(\n",
        "    row: pd.Series,\n",
        "    dataset_name: str,\n",
        "    exemplars_str: str = \"\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Constructs the full prompt string for a single example row.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row from the cleansed DataFrame containing 'question_text',\n",
        "                         'serialized_tables', 'passages'.\n",
        "        dataset_name (str): \"TAT-QA\" or \"Fin-QA\".\n",
        "        exemplars_str (str): Pre-formatted exemplars string (optional).\n",
        "\n",
        "    Returns:\n",
        "        str: The full prompt string ready for tokenization.\n",
        "    \"\"\"\n",
        "    question = row.get(\"question_text\", \"\")\n",
        "\n",
        "    # Tables are already serialized in 'serialized_tables' (list of strings)\n",
        "    tables_list = row.get(\"serialized_tables\", [])\n",
        "    tables_str = \"\\n\\n\".join(tables_list) if isinstance(tables_list, list) else \"\"\n",
        "\n",
        "    # Passages are list of dicts, need to extract text\n",
        "    passages_list = row.get(\"passages\", [])\n",
        "    passages_str = \"\"\n",
        "    if isinstance(passages_list, list):\n",
        "        passages_str = \"\\n\\n\".join([p.get(\"text\", \"\").strip() for p in passages_list if isinstance(p, dict)])\n",
        "\n",
        "    # Use the construct_prompt function from Task 12\n",
        "    # We assume construct_prompt is available in the environment\n",
        "    return construct_prompt(\n",
        "        dataset=dataset_name,\n",
        "        question=question,\n",
        "        tables_str=tables_str,\n",
        "        passages_str=passages_str,\n",
        "        exemplars_str=exemplars_str\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 2 & 3: Tokenize and Compute Offsets\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def tokenize_with_offsets(\n",
        "    text: str,\n",
        "    interface: LLMInterface\n",
        ") -> Tuple[List[int], List[str], List[Tuple[int, int]]]:\n",
        "    \"\"\"\n",
        "    Tokenizes the input text and computes precise character offsets for each token.\n",
        "\n",
        "    This function handles the complexities of different tokenizer implementations:\n",
        "    1. **Hugging Face Tokenizers (e.g., Llama)**: Natively support offset mapping via\n",
        "       `return_offsets_mapping=True`.\n",
        "    2. **Tiktoken (e.g., GPT-4o, Claude approximation)**: Does not natively support\n",
        "       offsets. We implement a robust reconstruction algorithm that decodes tokens\n",
        "       sequentially and maps them back to the original string to determine character spans.\n",
        "\n",
        "    This alignment is critical for Task 19, where dependency parsing (operating on\n",
        "    character spans) must be mapped to LLM tokens for phrase-level pruning.\n",
        "\n",
        "    Args:\n",
        "        text (str): The full prompt text to tokenize.\n",
        "        interface (LLMInterface): The configured LLM interface containing the\n",
        "            appropriate tokenizer (either `tokenizer` for HF or `encoding` for tiktoken).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[int], List[str], List[Tuple[int, int]]]: A tuple containing:\n",
        "            - List[int]: The sequence of token IDs.\n",
        "            - List[str]: The sequence of decoded token strings.\n",
        "            - List[Tuple[int, int]]: A list of (start, end) character offsets for\n",
        "              each token, such that `text[start:end]` corresponds to the token.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the interface does not contain a valid tokenizer.\n",
        "    \"\"\"\n",
        "    token_ids: List[int] = []\n",
        "    token_strings: List[str] = []\n",
        "    offsets: List[Tuple[int, int]] = []\n",
        "\n",
        "    # Case 1: Hugging Face Tokenizer (e.g., Llama)\n",
        "    # These tokenizers provide a built-in method to retrieve offset mappings.\n",
        "    if hasattr(interface, 'tokenizer') and interface.tokenizer is not None:\n",
        "        # Tokenize with offset mapping enabled\n",
        "        # add_special_tokens=False ensures we don't get unexpected BOS/EOS unless intended\n",
        "        encoding = interface.tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "\n",
        "        token_ids = encoding.input_ids\n",
        "        offsets = encoding.offset_mapping\n",
        "\n",
        "        # Convert IDs back to tokens for interpretability and debugging\n",
        "        token_strings = interface.tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    # Case 2: Tiktoken (e.g., GPT-4o, Claude approximation)\n",
        "    # Tiktoken encodes to IDs but does not provide character offsets directly.\n",
        "    # We must reconstruct them by decoding tokens and matching against the input text.\n",
        "    elif hasattr(interface, 'encoding') and interface.encoding is not None:\n",
        "        enc = interface.encoding\n",
        "        token_ids = enc.encode(text)\n",
        "\n",
        "        current_offset = 0\n",
        "\n",
        "        # Iterate through each token ID to reconstruct its string and offset\n",
        "        for tid in token_ids:\n",
        "            # Decode the single token bytes\n",
        "            token_bytes = enc.decode_single_token_bytes(tid)\n",
        "\n",
        "            # Decode bytes to string, handling potential replacement characters\n",
        "            token_str = token_bytes.decode('utf-8', errors='replace')\n",
        "            token_strings.append(token_str)\n",
        "\n",
        "            # Determine the length of the token in the original text\n",
        "            # Note: This assumes the decoded token string length matches the\n",
        "            # character length in the original text. For standard BPE, this holds.\n",
        "            t_len = len(token_str)\n",
        "\n",
        "            # Calculate start and end offsets\n",
        "            start = current_offset\n",
        "            end = current_offset + t_len\n",
        "\n",
        "            # Store the offset tuple\n",
        "            offsets.append((start, end))\n",
        "\n",
        "            # Advance the current offset\n",
        "            current_offset = end\n",
        "\n",
        "    else:\n",
        "        logger.warning(\"No valid tokenizer found in interface. Returning empty lists.\")\n",
        "\n",
        "    return token_ids, token_strings, offsets\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def prepare_dynamic_scoring_inputs(\n",
        "    df: pd.DataFrame,\n",
        "    dataset_name: str,\n",
        "    scorer_interface: LLMInterface,\n",
        "    exemplars_str: str = \"\"\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the preparation of prompts for dynamic scoring.\n",
        "\n",
        "    For each example in the DataFrame:\n",
        "    1. Constructs the full prompt string.\n",
        "    2. Tokenizes the prompt.\n",
        "    3. Computes character offsets.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame (TAT-QA or Fin-QA).\n",
        "        dataset_name (str): Name of the dataset.\n",
        "        scorer_interface (LLMInterface): The interface for the scoring model.\n",
        "        exemplars_str (str): Optional few-shot exemplars string.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A mapping from example_id to a dictionary containing:\n",
        "            - 'prompt_text': The full prompt string.\n",
        "            - 'token_ids': List of token IDs.\n",
        "            - 'token_strings': List of token strings.\n",
        "            - 'offsets': List of (start, end) character offsets.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Preparing dynamic scoring inputs for {dataset_name}...\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        example_id = row.get(\"example_id\")\n",
        "        if not example_id:\n",
        "            continue\n",
        "\n",
        "        # Step 1: Serialize\n",
        "        prompt_text = serialize_example_to_prompt(row, dataset_name, exemplars_str)\n",
        "\n",
        "        # Step 2 & 3: Tokenize and Offsets\n",
        "        token_ids, token_strings, offsets = tokenize_with_offsets(prompt_text, scorer_interface)\n",
        "\n",
        "        results[example_id] = {\n",
        "            \"prompt_text\": prompt_text,\n",
        "            \"token_ids\": token_ids,\n",
        "            \"token_strings\": token_strings,\n",
        "            \"offsets\": offsets\n",
        "        }\n",
        "\n",
        "    logger.info(f\"Prepared inputs for {len(results)} examples.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "PY08OZ4NrXAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 – Query LLM for Conditional Probabilities\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Query LLM for Conditional Probabilities\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Helper: Iterative Scoring for Chat Models\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def score_sequence_iterative(\n",
        "    interface: Any,  # Typed as Any to avoid circular import with LLMInterface class definition\n",
        "    token_ids: List[int],\n",
        "    token_strings: List[str]\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Computes log-probabilities for a token sequence using iterative \"teacher forcing\"\n",
        "    with a chat completion API.\n",
        "\n",
        "    This function approximates the conditional probability P(t_i | t_0...t_{i-1})\n",
        "    by iteratively prompting the model with the prefix t_0...t_{i-1} and checking\n",
        "    the probability assigned to the actual next token t_i in the generation output.\n",
        "\n",
        "    CRITICAL NOTE:\n",
        "    Standard Chat APIs (e.g., OpenAI GPT-4o) do not support scoring the prompt\n",
        "    directly (i.e., no `echo=True` equivalent). This method computes the\n",
        "    *continuation probability*: P(t_i | User: t_0...t_{i-1}). This is a proxy\n",
        "    for the true prompt probability P(t_i | t_0...t_{i-1}) but is the best\n",
        "    available approximation for black-box Chat models.\n",
        "\n",
        "    Algorithm:\n",
        "    1. Initialize logprobs list with 0.0 for the first token (unconditional).\n",
        "    2. Iterate through the sequence from i = 1 to T-1.\n",
        "    3. Construct the context string from tokens t_0...t_{i-1}.\n",
        "    4. Request generation of 1 token with logprobs enabled.\n",
        "    5. Extract the logprob of the target token t_i from the response's top_logprobs.\n",
        "       - Uses robust matching: compares token IDs (if tokenizer available) or\n",
        "         stripped strings to handle whitespace artifacts.\n",
        "    6. If the target token is not in the top-K logprobs, assign a penalty value (-15.0).\n",
        "\n",
        "    Args:\n",
        "        interface (LLMInterface): The LLM interface to use. Must support `prompt`\n",
        "                                  and `extract_response`.\n",
        "        token_ids (List[int]): The sequence of token IDs to score.\n",
        "        token_strings (List[str]): The sequence of token strings (decoded).\n",
        "\n",
        "    Returns:\n",
        "        List[float]: A list of natural log probabilities, one for each token.\n",
        "                     The first token's probability is set to 0.0.\n",
        "    \"\"\"\n",
        "    # Initialize logprobs list; the first token has no preceding context in this scope\n",
        "    logprobs: List[float] = [0.0]\n",
        "\n",
        "    # Iterate through the sequence starting from the second token\n",
        "    for i in range(1, len(token_ids)):\n",
        "        target_token_id = token_ids[i]\n",
        "        target_token_str = token_strings[i]\n",
        "\n",
        "        # Construct context from previous tokens\n",
        "        # We join with empty string as BPE token strings usually include necessary spacing\n",
        "        context_text = \"\".join(token_strings[:i])\n",
        "\n",
        "        # Construct the message payload for the Chat API\n",
        "        messages = [{\"role\": \"user\", \"content\": context_text}]\n",
        "\n",
        "        try:\n",
        "            # Request generation of exactly 1 token with logprobs enabled\n",
        "            # We set temperature to 0.0 for deterministic behavior\n",
        "            response = interface.prompt(\n",
        "                messages=messages,\n",
        "                max_tokens=1,\n",
        "                temperature=0.0,\n",
        "                logprobs=True\n",
        "            )\n",
        "\n",
        "            # Extract the generated text and the logprobs data structure\n",
        "            _, response_logprobs = interface.extract_response(response)\n",
        "\n",
        "            # Handle cases where logprobs are missing or empty\n",
        "            if not response_logprobs:\n",
        "                logger.warning(f\"No logprobs returned for token index {i}. Assigning penalty.\")\n",
        "                logprobs.append(-15.0) # Penalty for missing data\n",
        "                continue\n",
        "\n",
        "            # The API returns a list of generated tokens. We only asked for 1.\n",
        "            # Get the data for the first (and only) generated token position.\n",
        "            gen_token_data = response_logprobs[0]\n",
        "\n",
        "            # Initialize target logprob with a penalty value (representing very low probability)\n",
        "            # -15.0 corresponds to exp(-15) approx 3e-7\n",
        "            target_logprob = -15.0\n",
        "            match_found = False\n",
        "\n",
        "            # Helper to check if a candidate matches the target\n",
        "            def is_match(candidate_str: str) -> bool:\n",
        "                # Method A: Re-encode and compare IDs (Most Rigorous)\n",
        "                if hasattr(interface, 'encoding') and interface.encoding:\n",
        "                    try:\n",
        "                        # Encode candidate string to ID\n",
        "                        cand_ids = interface.encoding.encode(candidate_str)\n",
        "                        # Check if it matches the target ID\n",
        "                        # Note: candidate might encode to multiple tokens if it's long,\n",
        "                        # but here we expect single token generation.\n",
        "                        if len(cand_ids) == 1 and cand_ids[0] == target_token_id:\n",
        "                            return True\n",
        "                    except Exception:\n",
        "                        pass # Fallback to string comparison\n",
        "\n",
        "                if hasattr(interface, 'tokenizer') and interface.tokenizer:\n",
        "                     try:\n",
        "                        cand_ids = interface.tokenizer.encode(candidate_str, add_special_tokens=False)\n",
        "                        if len(cand_ids) == 1 and cand_ids[0] == target_token_id:\n",
        "                            return True\n",
        "                     except Exception:\n",
        "                        pass\n",
        "\n",
        "                # Method B: String comparison (Robust Fallback)\n",
        "                # Compare stripped strings to ignore leading/trailing whitespace differences\n",
        "                # which are common artifacts in tokenization (e.g. \" world\" vs \"world\")\n",
        "                return candidate_str.strip() == target_token_str.strip()\n",
        "\n",
        "            # 1. Check if the top generated token is our target\n",
        "            if is_match(gen_token_data['token']):\n",
        "                target_logprob = gen_token_data['logprob']\n",
        "                match_found = True\n",
        "\n",
        "            # 2. If not, check the top_logprobs list for alternatives\n",
        "            elif 'top_logprobs' in gen_token_data:\n",
        "                for alt in gen_token_data['top_logprobs']:\n",
        "                    if is_match(alt['token']):\n",
        "                        target_logprob = alt['logprob']\n",
        "                        match_found = True\n",
        "                        break\n",
        "\n",
        "            # Append the found logprob (or the penalty if no match found)\n",
        "            logprobs.append(target_logprob)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error scoring token {i} ('{target_token_str}'): {e}\")\n",
        "            logprobs.append(-15.0) # Fallback penalty\n",
        "\n",
        "    return logprobs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Helper: Score Prompt Tokens\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def score_prompt_tokens(\n",
        "    interface: LLMInterface,\n",
        "    text: str,\n",
        "    token_ids: List[int],\n",
        "    token_strings: List[str]\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Obtains the log-probability log(P(t_i | t_0...t_{i-1})) for each token in the text.\n",
        "\n",
        "    This function dispatches the scoring request to the appropriate strategy based on\n",
        "    the specific LLMInterface implementation. It enforces strict alignment between\n",
        "    the locally computed token sequence and the API's response.\n",
        "\n",
        "    Strategies:\n",
        "    1. **Llama (Together AI)**: Uses the `completions` endpoint with `echo=True` and `logprobs=1`.\n",
        "       This is efficient as it scores the entire prompt in one pass.\n",
        "    2. **GPT-4o (OpenAI)**: Uses iterative \"teacher forcing\" via the chat API (implemented\n",
        "       in `score_sequence_iterative`). This is slower but necessary as OpenAI does not\n",
        "       support `echo=True` for chat models.\n",
        "    3. **Claude (Anthropic)**: Returns 0.0s as log-probabilities are not supported.\n",
        "\n",
        "    Args:\n",
        "        interface (LLMInterface): The configured LLM interface.\n",
        "        text (str): The full prompt text.\n",
        "        token_ids (List[int]): The list of token IDs computed locally.\n",
        "        token_strings (List[str]): The list of token strings computed locally.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: A list of natural log probabilities, one for each token.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the number of log-probabilities returned by the API does not\n",
        "            match the number of local tokens (for Llama). This indicates a critical\n",
        "            tokenizer mismatch.\n",
        "    \"\"\"\n",
        "    # Strategy 1: Llama (Together AI) - Efficient `echo=True` scoring\n",
        "    if isinstance(interface, LlamaInterface):\n",
        "        # Verify that the interface has a valid tokenizer loaded\n",
        "        # The remedied LlamaInterface guarantees this in __init__, but we check for safety.\n",
        "        if not hasattr(interface, 'tokenizer') or interface.tokenizer is None:\n",
        "             raise RuntimeError(\n",
        "                 \"LlamaInterface must have a valid HuggingFace tokenizer loaded for scoring. \"\n",
        "                 \"Ensure HF_TOKEN is set and transformers is installed.\"\n",
        "             )\n",
        "\n",
        "        try:\n",
        "            # Access the raw client to use completions endpoint\n",
        "            # Note: LlamaInterface exposes .client (Together instance) and .model\n",
        "            response = interface.client.completions.create(\n",
        "                model=interface.model,\n",
        "                prompt=text,\n",
        "                max_tokens=0,       # We only want to score the prompt, not generate\n",
        "                logprobs=1,         # Request token logprobs\n",
        "                echo=True,          # Echo the prompt to get its scores\n",
        "                temperature=0.0     # Deterministic\n",
        "            )\n",
        "\n",
        "            # Extract logprobs from response\n",
        "            if hasattr(response.choices[0], 'logprobs') and response.choices[0].logprobs:\n",
        "                token_logprobs = response.choices[0].logprobs.token_logprobs\n",
        "\n",
        "                # Strict Length Validation\n",
        "                # We must ensure the API's tokenization matches our local token_ids exactly.\n",
        "                if len(token_logprobs) != len(token_ids):\n",
        "                    raise RuntimeError(\n",
        "                        f\"Token length mismatch for Llama scoring! \"\n",
        "                        f\"Local tokenizer found {len(token_ids)} tokens, \"\n",
        "                        f\"API returned {len(token_logprobs)} logprobs. \"\n",
        "                        \"This indicates a divergence between the local HuggingFace tokenizer \"\n",
        "                        \"and the remote API tokenizer. Scoring cannot proceed safely.\"\n",
        "                    )\n",
        "\n",
        "                # Replace None values (often the first token has None logprob) with 0.0\n",
        "                return [lp if lp is not None else 0.0 for lp in token_logprobs]\n",
        "            else:\n",
        "                logger.warning(\"Llama response did not contain logprobs. Returning 0.0s.\")\n",
        "                return [0.0] * len(token_ids)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Llama completion scoring failed: {e}\")\n",
        "            raise RuntimeError(f\"Llama scoring failed: {e}\") from e\n",
        "\n",
        "    # Strategy 2: GPT-4o (Chat API) - Iterative Scoring\n",
        "    if isinstance(interface, GPT4oInterface):\n",
        "        logger.info(\"Using iterative scoring for GPT-4o (this may take time)...\")\n",
        "        # score_sequence_iterative is assumed to be defined in the scope\n",
        "        return score_sequence_iterative(interface, token_ids, token_strings)\n",
        "\n",
        "    # Strategy 3: Claude - Unsupported\n",
        "    if isinstance(interface, ClaudeInterface):\n",
        "        logger.warning(\"Claude does not support logprobs. Returning 0.0s.\")\n",
        "        return [0.0] * len(token_ids)\n",
        "\n",
        "    # Default Fallback for unknown interfaces\n",
        "    logger.warning(f\"Unknown interface type {type(interface)}. Returning 0.0s.\")\n",
        "    return [0.0] * len(token_ids)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def get_prompt_logprobs_task(\n",
        "    dynamic_inputs: Dict[str, Dict[str, Any]],\n",
        "    scorer_interface: LLMInterface\n",
        ") -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the retrieval of log-probabilities for all prepared prompts.\n",
        "\n",
        "    Iterates through each example's tokenized prompt and queries the scorer LLM\n",
        "    to obtain conditional log-probabilities.\n",
        "\n",
        "    Args:\n",
        "        dynamic_inputs (Dict): Output from Task 15, mapping example_id to\n",
        "                               {'prompt_text', 'token_ids', 'token_strings', ...}.\n",
        "        scorer_interface (LLMInterface): The interface to use for scoring.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[float]]: Mapping example_id -> list of logprobs (natural log).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting dynamic scoring query...\")\n",
        "\n",
        "    results = {}\n",
        "    total_examples = len(dynamic_inputs)\n",
        "    processed = 0\n",
        "\n",
        "    # Iterate through dynamic inputs and score prompts\n",
        "    for example_id, data in dynamic_inputs.items():\n",
        "        prompt_text = data[\"prompt_text\"]\n",
        "        token_ids = data[\"token_ids\"]\n",
        "        token_strings = data[\"token_strings\"]\n",
        "\n",
        "        # Score the prompt\n",
        "        logprobs = score_prompt_tokens(scorer_interface, prompt_text, token_ids, token_strings)\n",
        "\n",
        "        results[example_id] = logprobs\n",
        "        processed += 1\n",
        "\n",
        "        if processed % 10 == 0:\n",
        "            logger.info(f\"Scored {processed}/{total_examples} examples.\")\n",
        "\n",
        "    logger.info(f\"Scoring complete for {len(results)} examples.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "evDZ9dom5F7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 – Compute Dynamic Self-Information\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Compute Dynamic Self-Information\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1: Convert probabilities to self-information\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def convert_logprobs_to_self_info(logprobs: List[float]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Converts natural log probabilities to self-information in bits.\n",
        "\n",
        "    Equation:\n",
        "        s_dyn(t) = -log2(P(t)) = -ln(P(t)) / ln(2)\n",
        "\n",
        "    Args:\n",
        "        logprobs (List[float]): List of natural log probabilities.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: List of self-information scores in bits.\n",
        "    \"\"\"\n",
        "    ln_2 = math.log(2)\n",
        "    # Handle potential None or NaN in input by treating as 0 probability (high info)\n",
        "    # But logprobs should be floats. We assume valid floats from Task 16.\n",
        "\n",
        "    s_dyn = []\n",
        "    for lp in logprobs:\n",
        "        if lp is None or math.isnan(lp):\n",
        "            # Assign a high penalty value for missing info\n",
        "            s_dyn.append(20.0)\n",
        "        else:\n",
        "            # Ensure lp is <= 0 (probability <= 1)\n",
        "            # If lp > 0 due to float error, clamp to 0\n",
        "            clean_lp = min(0.0, lp)\n",
        "            s_dyn.append(-clean_lp / ln_2)\n",
        "\n",
        "    return s_dyn\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 2: Validate dynamic scores\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_dynamic_scores(s_dyn: List[float], example_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the computed self-information scores.\n",
        "\n",
        "    Checks for:\n",
        "    - Non-negativity.\n",
        "    - Finite values.\n",
        "\n",
        "    Args:\n",
        "        s_dyn (List[float]): Self-information scores.\n",
        "        example_id (str): ID for logging.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid.\n",
        "    \"\"\"\n",
        "    if not s_dyn:\n",
        "        logger.warning(f\"Empty s_dyn for {example_id}\")\n",
        "        return False\n",
        "\n",
        "    s_dyn_arr = np.array(s_dyn)\n",
        "\n",
        "    if np.any(s_dyn_arr < 0):\n",
        "        logger.error(f\"Negative self-information detected for {example_id}. Min: {np.min(s_dyn_arr)}\")\n",
        "        return False\n",
        "\n",
        "    if not np.all(np.isfinite(s_dyn_arr)):\n",
        "        logger.error(f\"Non-finite self-information detected for {example_id}.\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_dynamic_scores_task(\n",
        "    logprobs_map: Dict[str, List[float]]\n",
        ") -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of dynamic self-information scores.\n",
        "\n",
        "    Args:\n",
        "        logprobs_map (Dict[str, List[float]]): Mapping example_id -> logprobs.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[float]]: Mapping example_id -> s_dyn scores.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting dynamic self-information computation...\")\n",
        "\n",
        "    s_dyn_map = {}\n",
        "\n",
        "    for example_id, logprobs in logprobs_map.items():\n",
        "        # Step 1: Convert\n",
        "        s_dyn = convert_logprobs_to_self_info(logprobs)\n",
        "\n",
        "        # Step 2: Validate\n",
        "        if validate_dynamic_scores(s_dyn, example_id):\n",
        "            s_dyn_map[example_id] = s_dyn\n",
        "        else:\n",
        "            # Fallback: return zeros or handle error?\n",
        "            # We'll return zeros to allow pipeline to continue, but logged error indicates issue.\n",
        "            s_dyn_map[example_id] = [0.0] * len(logprobs)\n",
        "\n",
        "    logger.info(f\"Computed scores for {len(s_dyn_map)} examples.\")\n",
        "    return s_dyn_map\n"
      ],
      "metadata": {
        "id": "DUkkcGq08CJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 – Compute Relative Difference and Combined Score\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Compute Relative Difference and Combined Score\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1 & 2: Compute Combined Scores\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_combined_scores(\n",
        "    token_ids: List[int],\n",
        "    s_dyn: List[float],\n",
        "    s_stat_lookup: Dict[str, float], # Keys are strings in JSON\n",
        "    delta_threshold: float = 0.1\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Computes the combined importance score C(t) for each token.\n",
        "\n",
        "    Equations:\n",
        "        Delta = |s_dyn - s_stat| / s_stat\n",
        "        C(t) = (s_stat + s_dyn) / 2  if Delta <= 0.1\n",
        "             = s_dyn                 if Delta > 0.1\n",
        "\n",
        "    Args:\n",
        "        token_ids (List[int]): List of token IDs.\n",
        "        s_dyn (List[float]): List of dynamic self-information scores.\n",
        "        s_stat_lookup (Dict[str, float]): Static self-information lookup table.\n",
        "        delta_threshold (float): Threshold for relative difference (default 0.1).\n",
        "\n",
        "    Returns:\n",
        "        List[float]: List of combined scores C(t).\n",
        "    \"\"\"\n",
        "    combined_scores = []\n",
        "\n",
        "    for i, tid in enumerate(token_ids):\n",
        "        # Retrieve static score\n",
        "        # JSON keys are strings, so convert tid to str\n",
        "        s_stat_val = s_stat_lookup.get(str(tid))\n",
        "        s_dyn_val = s_dyn[i]\n",
        "\n",
        "        # Calculate Delta\n",
        "        if s_stat_val is None or s_stat_val == 0:\n",
        "            # If static score is missing or zero, we cannot compute relative difference reliably.\n",
        "            # We default to dynamic score (Delta = infinity).\n",
        "            delta = float('inf')\n",
        "        else:\n",
        "            delta = abs(s_dyn_val - s_stat_val) / s_stat_val\n",
        "\n",
        "        # Fusion Rule\n",
        "        if delta <= delta_threshold:\n",
        "            c_val = (s_stat_val + s_dyn_val) / 2.0\n",
        "        else:\n",
        "            c_val = s_dyn_val\n",
        "\n",
        "        combined_scores.append(c_val)\n",
        "\n",
        "    return combined_scores\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_combined_scores_task(\n",
        "    dynamic_inputs: Dict[str, Dict[str, Any]],\n",
        "    s_dyn_map: Dict[str, List[float]],\n",
        "    s_stat_lookup: Dict[str, float],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of combined importance scores.\n",
        "\n",
        "    Args:\n",
        "        dynamic_inputs (Dict): Contains token_ids per example.\n",
        "        s_dyn_map (Dict): Contains s_dyn scores per example.\n",
        "        s_stat_lookup (Dict): Static scores lookup.\n",
        "        config (Dict): Configuration containing delta threshold.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[float]]: Mapping example_id -> combined scores.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting combined score computation...\")\n",
        "\n",
        "    threshold = config.get(\"hard_prompt_compression_config\", {}).get(\"delta_relative_difference_threshold\", 0.1)\n",
        "    combined_scores_map = {}\n",
        "\n",
        "    # Iterate through the examples\n",
        "    for example_id, inputs in dynamic_inputs.items():\n",
        "        token_ids = inputs[\"token_ids\"]\n",
        "        s_dyn = s_dyn_map.get(example_id)\n",
        "\n",
        "        if not s_dyn:\n",
        "            logger.warning(f\"No dynamic scores found for {example_id}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        if len(token_ids) != len(s_dyn):\n",
        "            logger.error(f\"Length mismatch for {example_id}: tokens={len(token_ids)}, s_dyn={len(s_dyn)}\")\n",
        "\n",
        "            # Truncate to minimum length to proceed safely\n",
        "            min_len = min(len(token_ids), len(s_dyn))\n",
        "            token_ids = token_ids[:min_len]\n",
        "            s_dyn = s_dyn[:min_len]\n",
        "\n",
        "        c_scores = calculate_combined_scores(token_ids, s_dyn, s_stat_lookup, threshold)\n",
        "        combined_scores_map[example_id] = c_scores\n",
        "\n",
        "    logger.info(f\"Computed combined scores for {len(combined_scores_map)} examples.\")\n",
        "    return combined_scores_map\n"
      ],
      "metadata": {
        "id": "Q8GIsRWfSKGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 – Group Tokens into Phrases Using Dependency Parsing\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Group Tokens into Phrases Using Dependency Parsing\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1: Run dependency parser\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def parse_prompt_text(\n",
        "    text: str,\n",
        "    spacy_model_name: str = \"en_core_web_sm\"\n",
        ") -> List[Tuple[int, int, str]]:\n",
        "    \"\"\"\n",
        "    Parses the prompt text using spaCy to identify syntactic phrase boundaries.\n",
        "\n",
        "    This function implements the \"dependency-based phrase grouping\" required for\n",
        "    hard prompt compression. Unlike simple noun chunking, this implementation\n",
        "    extracts a broader set of syntactic units to enable more granular and\n",
        "    comprehensive pruning.\n",
        "\n",
        "    Extracted Phrase Types:\n",
        "    1. **NP (Noun Phrases)**: Extracted via `doc.noun_chunks`. Represents entities\n",
        "       and objects (e.g., \"the total revenue\").\n",
        "    2. **VP (Verb Phrases)**: Identified by `VERB` or `AUX` tokens. We group the\n",
        "       verb with its immediate auxiliaries and negations to form a coherent action\n",
        "       unit (e.g., \"did not increase\").\n",
        "    3. **PP (Prepositional Phrases)**: Identified by `ADP` (adposition) tokens.\n",
        "       We group the preposition with its subtree (often an NP) to capture\n",
        "       contextual modifiers (e.g., \"in 2019\").\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw prompt text to parse.\n",
        "        spacy_model_name (str): The name of the spaCy model to load.\n",
        "            Defaults to \"en_core_web_sm\".\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[int, int, str]]: A list of tuples, each representing a phrase.\n",
        "            Format: (char_start, char_end, label).\n",
        "            The list is sorted by start position.\n",
        "\n",
        "    Notes:\n",
        "        - Overlapping phrases are possible (e.g., a PP containing an NP).\n",
        "          Downstream logic in `map_tokens_to_phrases` handles assignment/partitioning.\n",
        "        - Requires the specified spaCy model to be installed.\n",
        "    \"\"\"\n",
        "    # Load spaCy model\n",
        "    try:\n",
        "        nlp = spacy.load(spacy_model_name)\n",
        "    except OSError:\n",
        "        logger.warning(f\"spaCy model '{spacy_model_name}' not found. Downloading...\")\n",
        "        from spacy.cli import download\n",
        "        download(spacy_model_name)\n",
        "        nlp = spacy.load(spacy_model_name)\n",
        "\n",
        "    # Process text\n",
        "    # Disable unnecessary pipeline components for speed if possible, but we need parser/tagger\n",
        "    doc = nlp(text)\n",
        "\n",
        "    phrases: List[Tuple[int, int, str]] = []\n",
        "\n",
        "    # 1. Extract Noun Phrases (NP)\n",
        "    # spaCy's noun_chunks iterator provides robust NP extraction\n",
        "    for chunk in doc.noun_chunks:\n",
        "        phrases.append((chunk.start_char, chunk.end_char, \"NP\"))\n",
        "\n",
        "    # 2. Extract Verb Phrases (VP) and Prepositional Phrases (PP)\n",
        "    # We iterate over tokens to find heads of these phrases\n",
        "    for token in doc:\n",
        "        # Verb Phrases: Group verb with auxiliaries/particles\n",
        "        if token.pos_ in [\"VERB\", \"AUX\"]:\n",
        "            # We want a compact VP, not the whole clause.\n",
        "            # Strategy: Collect the verb and its immediate children that are aux/neg/prt\n",
        "            vp_tokens = [token]\n",
        "            for child in token.children:\n",
        "                if child.dep_ in [\"aux\", \"auxpass\", \"neg\", \"prt\"]:\n",
        "                    vp_tokens.append(child)\n",
        "\n",
        "            # Determine span of this token group\n",
        "            # Note: These might be non-contiguous in rare cases, but for pruning\n",
        "            # we usually want contiguous spans. We take the min/max extent.\n",
        "            min_i = min(t.i for t in vp_tokens)\n",
        "            max_i = max(t.i for t in vp_tokens)\n",
        "\n",
        "            # Create span from doc\n",
        "            span = doc[min_i : max_i + 1]\n",
        "            phrases.append((span.start_char, span.end_char, \"VP\"))\n",
        "\n",
        "        # Prepositional Phrases: Group preposition with its subtree\n",
        "        elif token.pos_ == \"ADP\":\n",
        "            # A PP is rooted at the ADP. Its subtree usually includes the object.\n",
        "            # We take the full subtree of the ADP as the PP span.\n",
        "            span = list(token.subtree)\n",
        "            if span:\n",
        "                min_i = min(t.i for t in span)\n",
        "                max_i = max(t.i for t in span)\n",
        "\n",
        "                # Create span object to get char offsets\n",
        "                pp_span = doc[min_i : max_i + 1]\n",
        "                phrases.append((pp_span.start_char, pp_span.end_char, \"PP\"))\n",
        "\n",
        "    # 3. Deduplication and Sorting\n",
        "    # We might have duplicates or identical spans with different labels.\n",
        "    # We prioritize labels: NP > VP > PP (arbitrary, but consistent).\n",
        "    # We use a set to remove exact (start, end) duplicates.\n",
        "    unique_phrases = {}\n",
        "    for start, end, label in phrases:\n",
        "        if (start, end) not in unique_phrases:\n",
        "            unique_phrases[(start, end)] = label\n",
        "        else:\n",
        "            # If duplicate span, keep existing (or apply priority logic here)\n",
        "            pass\n",
        "\n",
        "    # Convert back to list and sort by start position\n",
        "    sorted_phrases = sorted(\n",
        "        [(start, end, label) for (start, end), label in unique_phrases.items()],\n",
        "        key=lambda x: (x[0], x[1])\n",
        "    )\n",
        "\n",
        "    return sorted_phrases\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 2 & 3: Map tokens to phrases and resolve\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def map_tokens_to_phrases(\n",
        "    token_offsets: List[Tuple[int, int]],\n",
        "    phrase_spans: List[Tuple[int, int, str]]\n",
        ") -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Maps LLM tokens to phrases based on character overlap.\n",
        "    Ensures every token is assigned to exactly one phrase (partition).\n",
        "\n",
        "    Args:\n",
        "        token_offsets (List[Tuple[int, int]]): List of (start, end) char offsets for each token.\n",
        "        phrase_spans (List[Tuple[int, int, str]]): List of (start, end, label) for candidate phrases.\n",
        "\n",
        "    Returns:\n",
        "        List[List[int]]: A list of phrases, where each phrase is a list of token indices.\n",
        "    \"\"\"\n",
        "    num_tokens = len(token_offsets)\n",
        "    token_to_phrase_idx = [-1] * num_tokens\n",
        "\n",
        "    # 1. Assign tokens to phrases\n",
        "    # We prioritize smaller phrases (more specific) if there's nesting,\n",
        "    # but spacy noun_chunks usually don't overlap.\n",
        "    # However, we must handle the case where a token partially overlaps.\n",
        "    for p_idx, (p_start, p_end, _) in enumerate(phrase_spans):\n",
        "        for t_idx, (t_start, t_end) in enumerate(token_offsets):\n",
        "            # Check overlap\n",
        "            # Token is inside phrase or overlaps significantly\n",
        "            # Simple intersection logic: max(p_start, t_start) < min(p_end, t_end)\n",
        "            if max(p_start, t_start) < min(p_end, t_end):\n",
        "                # If token already assigned, check which phrase is \"better\"\n",
        "                # Here we assume noun chunks are disjoint enough or we take the first one.\n",
        "                # Since we sorted phrases, we can just assign if not assigned,\n",
        "                # or overwrite if we want specific behavior.\n",
        "                # Let's assign if unassigned.\n",
        "                if token_to_phrase_idx[t_idx] == -1:\n",
        "                    token_to_phrase_idx[t_idx] = p_idx\n",
        "                else:\n",
        "                    # Collision: Token overlaps multiple phrases?\n",
        "                    # Spacy chunks shouldn't overlap.\n",
        "                    # If they do, we keep the existing assignment (first one).\n",
        "                    pass\n",
        "\n",
        "    # 2. Group by phrase index\n",
        "    # We need to handle unassigned tokens (-1).\n",
        "    # We will create new singleton phrases for them.\n",
        "    # Map: internal_phrase_id -> list of token_indices\n",
        "    # We use a dictionary to build groups\n",
        "    groups: Dict[int, List[int]] = {}\n",
        "\n",
        "    # Existing phrases\n",
        "    for p_idx in range(len(phrase_spans)):\n",
        "        groups[p_idx] = []\n",
        "\n",
        "    # Singleton counter (start after existing phrases)\n",
        "    next_singleton_id = len(phrase_spans)\n",
        "\n",
        "    for t_idx, p_idx in enumerate(token_to_phrase_idx):\n",
        "        if p_idx != -1:\n",
        "            groups[p_idx].append(t_idx)\n",
        "        else:\n",
        "            # Create singleton phrase\n",
        "            groups[next_singleton_id] = [t_idx]\n",
        "            next_singleton_id += 1\n",
        "\n",
        "    # 3. Convert to list of lists\n",
        "    # Filter out empty groups (e.g. phrases that matched no tokens due to alignment issues)\n",
        "    final_phrases = []\n",
        "\n",
        "    # We want to preserve the order of phrases as they appear in the text.\n",
        "    # We can sort groups by the index of their first token.\n",
        "    sorted_group_keys = sorted(groups.keys(), key=lambda k: groups[k][0] if groups[k] else float('inf'))\n",
        "\n",
        "    for k in sorted_group_keys:\n",
        "        tokens = groups[k]\n",
        "        if tokens:\n",
        "            final_phrases.append(tokens)\n",
        "\n",
        "    return final_phrases\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def group_tokens_into_phrases_task(\n",
        "    dynamic_inputs: Dict[str, Dict[str, Any]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, List[List[int]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the grouping of tokens into phrases for all examples.\n",
        "\n",
        "    Args:\n",
        "        dynamic_inputs (Dict): Contains 'prompt_text' and 'offsets' per example.\n",
        "        config (Dict): Configuration containing parser settings.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[List[int]]]: Mapping example_id -> List of phrases (each a list of token indices).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting phrase grouping...\")\n",
        "\n",
        "    spacy_model = config.get(\"hard_prompt_compression_config\", {}).get(\"phrase_grouping\", {}).get(\"parser_library\", \"en_core_web_sm\")\n",
        "    # Handle \"spacy_\" prefix if present in config value\n",
        "    if spacy_model.startswith(\"spacy_\"):\n",
        "        spacy_model = spacy_model.replace(\"spacy_\", \"\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for example_id, data in dynamic_inputs.items():\n",
        "        text = data[\"prompt_text\"]\n",
        "        offsets = data[\"offsets\"]\n",
        "\n",
        "        # Step 1: Parse\n",
        "        phrase_spans = parse_prompt_text(text, spacy_model)\n",
        "\n",
        "        # Step 2 & 3: Map\n",
        "        phrases = map_tokens_to_phrases(offsets, phrase_spans)\n",
        "\n",
        "        results[example_id] = phrases\n",
        "\n",
        "    logger.info(f\"Phrase grouping complete for {len(results)} examples.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "EUmLdg34UMKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 – Compute Phrase-Level Importance Scores\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Compute Phrase-Level Importance Scores\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 1 & 2: Aggregate scores and count tokens\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def aggregate_phrase_scores(\n",
        "    combined_scores: List[float],\n",
        "    phrases: List[List[int]],\n",
        "    aggregation_method: str = \"mean\"\n",
        ") -> Tuple[List[float], List[int]]:\n",
        "    \"\"\"\n",
        "    Computes importance scores and token counts for each phrase.\n",
        "\n",
        "    Equation:\n",
        "        C(pi_k) = mean({C(t) for t in pi_k})\n",
        "\n",
        "    Args:\n",
        "        combined_scores (List[float]): Token-level combined scores.\n",
        "        phrases (List[List[int]]): List of phrases, where each phrase is a list of token indices.\n",
        "        aggregation_method (str): \"mean\" or \"sum\". Defaults to \"mean\".\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[float], List[int]]:\n",
        "            - List of phrase importance scores.\n",
        "            - List of phrase token counts.\n",
        "    \"\"\"\n",
        "    phrase_scores = []\n",
        "    phrase_counts = []\n",
        "\n",
        "    # Iterate through phrases\n",
        "    for phrase_tokens in phrases:\n",
        "        count = len(phrase_tokens)\n",
        "        phrase_counts.append(count)\n",
        "\n",
        "        if count == 0:\n",
        "            # Should not happen with valid grouping, but handle safely\n",
        "            phrase_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # Gather scores for tokens in this phrase\n",
        "        # Ensure indices are within bounds\n",
        "        scores = []\n",
        "        for tid in phrase_tokens:\n",
        "            if 0 <= tid < len(combined_scores):\n",
        "                scores.append(combined_scores[tid])\n",
        "            else:\n",
        "                logger.warning(f\"Token index {tid} out of bounds for scores list of length {len(combined_scores)}\")\n",
        "                scores.append(0.0)\n",
        "\n",
        "        if not scores:\n",
        "            phrase_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        if aggregation_method == \"sum\":\n",
        "            phrase_scores.append(sum(scores))\n",
        "        else: # mean\n",
        "            phrase_scores.append(sum(scores) / len(scores))\n",
        "\n",
        "    return phrase_scores, phrase_counts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 3: Validate phrase scores\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_phrase_scores(phrase_scores: List[float], example_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Validates computed phrase scores.\n",
        "\n",
        "    Args:\n",
        "        phrase_scores (List[float]): List of scores.\n",
        "        example_id (str): ID for logging.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid.\n",
        "    \"\"\"\n",
        "    for i, score in enumerate(phrase_scores):\n",
        "        if math.isnan(score) or math.isinf(score):\n",
        "            logger.error(f\"Invalid phrase score at index {i} for {example_id}: {score}\")\n",
        "            return False\n",
        "        if score < 0:\n",
        "            logger.warning(f\"Negative phrase score at index {i} for {example_id}: {score}\")\n",
        "            # Negative scores might be possible if logprobs were positive (impossible)\n",
        "            # or if s_stat was negative (impossible). So this is a valid check.\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_phrase_scores_task(\n",
        "    combined_scores_map: Dict[str, List[float]],\n",
        "    phrases_map: Dict[str, List[List[int]]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, List[Any]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of phrase-level scores for all examples.\n",
        "\n",
        "    Args:\n",
        "        combined_scores_map (Dict): Mapping example_id -> token scores.\n",
        "        phrases_map (Dict): Mapping example_id -> phrases.\n",
        "        config (Dict): Configuration containing aggregation method.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict]: Mapping example_id -> {'scores': List[float], 'counts': List[int]}.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting phrase score computation...\")\n",
        "\n",
        "    method = config.get(\"hard_prompt_compression_config\", {}).get(\"phrase_score_aggregation\", {}).get(\"chosen_method\", \"mean\")\n",
        "    results = {}\n",
        "\n",
        "    # Iterate through phrases\n",
        "    for example_id, phrases in phrases_map.items():\n",
        "        combined_scores = combined_scores_map.get(example_id)\n",
        "\n",
        "        if not combined_scores:\n",
        "            logger.warning(f\"No combined scores for {example_id}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Compute aggregate phrase scores\n",
        "        scores, counts = aggregate_phrase_scores(combined_scores, phrases, method)\n",
        "\n",
        "        if validate_phrase_scores(scores, example_id):\n",
        "            results[example_id] = {\n",
        "                \"scores\": scores,\n",
        "                \"counts\": counts\n",
        "            }\n",
        "\n",
        "    logger.info(f\"Computed phrase scores for {len(results)} examples.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "JzR8hjuXYYeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 – Prune Phrases to Enforce Token Budget\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Prune Phrases to Enforce Token Budget\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 1 & 2: Sort and Select Phrases\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def select_phrases_knapsack(\n",
        "    phrase_scores: List[float],\n",
        "    phrase_counts: List[int],\n",
        "    budget: int\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    Selects phrases to retain based on importance scores and a token budget.\n",
        "\n",
        "    Uses a greedy strategy: sort by score descending, then add if it fits.\n",
        "    This approximates the Knapsack problem where value = score and weight = count.\n",
        "\n",
        "    Args:\n",
        "        phrase_scores (List[float]): Importance scores for each phrase.\n",
        "        phrase_counts (List[int]): Token count for each phrase.\n",
        "        budget (int): Maximum allowed tokens.\n",
        "\n",
        "    Returns:\n",
        "        List[int]: Indices of phrases to retain, sorted by original position.\n",
        "    \"\"\"\n",
        "    # Create list of (index, score, count)\n",
        "    # We use index as tie-breaker (keep earlier phrases if scores are equal)\n",
        "    candidates = []\n",
        "    for i, (score, count) in enumerate(zip(phrase_scores, phrase_counts)):\n",
        "        candidates.append((i, score, count))\n",
        "\n",
        "    # Sort by score descending\n",
        "    # Python's sort is stable, so original index order is preserved for ties if we don't specify secondary key.\n",
        "    # But let's be explicit: score desc, then index asc (prefer earlier context)\n",
        "    candidates.sort(key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "    selected_indices = []\n",
        "    current_tokens = 0\n",
        "\n",
        "    for idx, score, count in candidates:\n",
        "        if current_tokens + count <= budget:\n",
        "            selected_indices.append(idx)\n",
        "            current_tokens += count\n",
        "\n",
        "    # Sort indices back to original order to preserve narrative flow\n",
        "    selected_indices.sort()\n",
        "\n",
        "    return selected_indices\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Reconstruct compressed prompt\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def reconstruct_prompt_text(\n",
        "    selected_phrase_indices: List[int],\n",
        "    all_phrases: List[List[int]],\n",
        "    all_token_strings: List[str]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Reconstructs the prompt text from the selected phrases.\n",
        "\n",
        "    Args:\n",
        "        selected_phrase_indices (List[int]): Indices of phrases to keep.\n",
        "        all_phrases (List[List[int]]): List of token indices for each phrase.\n",
        "        all_token_strings (List[str]): List of all token strings in the original prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: The compressed prompt text.\n",
        "    \"\"\"\n",
        "    compressed_tokens = []\n",
        "\n",
        "    for p_idx in selected_phrase_indices:\n",
        "        # Get token indices for this phrase\n",
        "        token_indices = all_phrases[p_idx]\n",
        "\n",
        "        # Get strings\n",
        "        # Note: We assume token_strings are decoded pieces (e.g. \" The\", \" cat\").\n",
        "        # Simply joining them should reconstruct the text segment.\n",
        "        for t_idx in token_indices:\n",
        "            if 0 <= t_idx < len(all_token_strings):\n",
        "                compressed_tokens.append(all_token_strings[t_idx])\n",
        "\n",
        "    # Join tokens\n",
        "    # BPE tokens usually reconstruct by simple concatenation\n",
        "    return \"\".join(compressed_tokens)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def prune_prompt_task(\n",
        "    dynamic_inputs: Dict[str, Dict[str, Any]],\n",
        "    phrase_scores_data: Dict[str, Dict[str, List[Any]]],\n",
        "    phrases_map: Dict[str, List[List[int]]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the pruning of prompts for all examples.\n",
        "\n",
        "    Args:\n",
        "        dynamic_inputs (Dict): Contains 'token_strings' per example.\n",
        "        phrase_scores_data (Dict): Contains 'scores', 'counts' per example.\n",
        "        phrases_map (Dict): Contains list of phrases (token indices) per example.\n",
        "        config (Dict): Configuration containing token budget.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict]: Mapping example_id -> {\n",
        "            'compressed_text': str,\n",
        "            'original_tokens': int,\n",
        "            'compressed_tokens': int,\n",
        "            'compression_ratio': float\n",
        "        }\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting prompt pruning...\")\n",
        "    budget = config.get(\"hard_prompt_compression_config\", {}).get(\"prompt_token_budget\", 1500)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for example_id, p_data in phrase_scores_data.items():\n",
        "        if example_id not in dynamic_inputs or example_id not in phrases_map:\n",
        "            continue\n",
        "\n",
        "        scores = p_data[\"scores\"]\n",
        "        counts = p_data[\"counts\"]\n",
        "        phrases = phrases_map[example_id]\n",
        "        token_strings = dynamic_inputs[example_id][\"token_strings\"]\n",
        "\n",
        "        # Step 1 & 2: Select\n",
        "        selected_indices = select_phrases_knapsack(scores, counts, budget)\n",
        "\n",
        "        # Step 3: Reconstruct\n",
        "        compressed_text = reconstruct_prompt_text(selected_indices, phrases, token_strings)\n",
        "\n",
        "        # Metrics\n",
        "        original_len = sum(counts)\n",
        "        compressed_len = sum(counts[i] for i in selected_indices)\n",
        "        ratio = original_len / compressed_len if compressed_len > 0 else 1.0\n",
        "\n",
        "        results[example_id] = {\n",
        "            \"compressed_text\": compressed_text,\n",
        "            \"original_tokens\": original_len,\n",
        "            \"compressed_tokens\": compressed_len,\n",
        "            \"compression_ratio\": ratio\n",
        "        }\n",
        "\n",
        "    logger.info(f\"Pruning complete for {len(results)} examples.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "NGA5xMDcWXId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 – Extract N-grams from Passages and Compute Frequencies\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Extract N-grams from Passages and Compute Frequencies\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 1 & 2: Extract and Count N-grams\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def count_ngrams_in_passages(\n",
        "    passages_list: List[str],\n",
        "    n: int = 2,\n",
        "    encoding_name: str = \"cl100k_base\"\n",
        ") -> CounterType[Tuple[int, ...]]:\n",
        "    \"\"\"\n",
        "    Extracts n-grams from a list of passage texts and computes their frequencies.\n",
        "\n",
        "    Args:\n",
        "        passages_list (List[str]): List of passage texts.\n",
        "        n (int): N-gram length (default 2).\n",
        "        encoding_name (str): Tokenizer encoding name.\n",
        "\n",
        "    Returns:\n",
        "        Counter[Tuple[int, ...]]: Mapping of n-gram (tuple of token IDs) to count.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Counting {n}-grams in {len(passages_list)} passages...\")\n",
        "\n",
        "    try:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load encoding '{encoding_name}': {e}\")\n",
        "        return Counter()\n",
        "\n",
        "    ngram_counts: CounterType[Tuple[int, ...]] = Counter()\n",
        "\n",
        "    for text in passages_list:\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = encoding.encode(text, disallowed_special=())\n",
        "\n",
        "        if len(tokens) < n:\n",
        "            continue\n",
        "\n",
        "        # Extract n-grams\n",
        "        # Sliding window\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i : i + n])\n",
        "            ngram_counts[ngram] += 1\n",
        "\n",
        "    return ngram_counts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 3: Select top-K n-grams\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def select_top_k_ngrams(\n",
        "    ngram_counts: CounterType[Tuple[int, ...]],\n",
        "    k: int = 100\n",
        ") -> List[Tuple[Tuple[int, ...], int]]:\n",
        "    \"\"\"\n",
        "    Selects the top-K most frequent n-grams.\n",
        "\n",
        "    Args:\n",
        "        ngram_counts (Counter): N-gram counts.\n",
        "        k (int): Number of top n-grams to select.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[Tuple[int, ...], int]]: List of (ngram, count) sorted by count descending.\n",
        "    \"\"\"\n",
        "    return ngram_counts.most_common(k)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def extract_top_ngrams_task(\n",
        "    tatqa_df: pd.DataFrame,\n",
        "    finqa_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[Tuple[Tuple[int, ...], int]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction of top-K n-grams from all passages in both datasets.\n",
        "\n",
        "    Args:\n",
        "        tatqa_df (pd.DataFrame): TAT-QA DataFrame.\n",
        "        finqa_df (pd.DataFrame): Fin-QA DataFrame.\n",
        "        config (Dict): Configuration containing n-gram settings.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[Tuple[int, ...], int]]: Top-K n-grams and their counts.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting n-gram extraction...\")\n",
        "\n",
        "    ngram_config = config.get(\"ngram_abbreviation_config\", {})\n",
        "    n = ngram_config.get(\"ngram_size_G\", {}).get(\"best_performing_value\", 2)\n",
        "    k = ngram_config.get(\"dictionary_size_K\", {}).get(\"default\", 100)\n",
        "\n",
        "    # Collect all passages\n",
        "    all_passages = []\n",
        "\n",
        "    # TAT-QA passages\n",
        "    for passages in tatqa_df[\"passages\"]:\n",
        "        if isinstance(passages, list):\n",
        "            for p in passages:\n",
        "                if isinstance(p, dict):\n",
        "                    all_passages.append(p.get(\"text\", \"\"))\n",
        "\n",
        "    # Fin-QA passages\n",
        "    for passages in finqa_df[\"passages\"]:\n",
        "        if isinstance(passages, list):\n",
        "            for p in passages:\n",
        "                if isinstance(p, dict):\n",
        "                    all_passages.append(p.get(\"text\", \"\"))\n",
        "\n",
        "    # Count\n",
        "    counts = count_ngrams_in_passages(all_passages, n=n)\n",
        "\n",
        "    # Select Top-K\n",
        "    top_k = select_top_k_ngrams(counts, k=k)\n",
        "\n",
        "    logger.info(f\"Extracted top {len(top_k)} {n}-grams.\")\n",
        "    return top_k\n"
      ],
      "metadata": {
        "id": "8SUoCtErdUIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23 – Construct Abbreviation Dictionary\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Construct Abbreviation Dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 1 & 2: Assign placeholders and build dictionary\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def build_abbreviation_maps(\n",
        "    top_k_ngrams: List[Tuple[Tuple[int, ...], int]]\n",
        ") -> Tuple[Dict[Tuple[int, ...], str], Dict[str, Tuple[int, ...]]]:\n",
        "    \"\"\"\n",
        "    Constructs bidirectional mappings between n-grams and placeholder tokens.\n",
        "\n",
        "    Placeholder format: \"[PH{i:03d}]\" (e.g., [PH001], [PH002], ...)\n",
        "\n",
        "    Args:\n",
        "        top_k_ngrams (List[Tuple[Tuple[int, ...], int]]): List of (ngram, count) tuples.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]:\n",
        "            - ngram_to_placeholder: Mapping from n-gram tuple to placeholder string.\n",
        "            - placeholder_to_ngram: Mapping from placeholder string to n-gram tuple.\n",
        "    \"\"\"\n",
        "    ngram_to_ph = {}\n",
        "    ph_to_ngram = {}\n",
        "\n",
        "    for i, (ngram, count) in enumerate(top_k_ngrams):\n",
        "        # 1-based index for readability\n",
        "        placeholder = f\"[PH{i+1:03d}]\"\n",
        "\n",
        "        ngram_to_ph[ngram] = placeholder\n",
        "        ph_to_ngram[placeholder] = ngram\n",
        "\n",
        "    return ngram_to_ph, ph_to_ngram\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 3: Verify uniqueness\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def verify_abbreviation_maps(\n",
        "    ngram_to_ph: Dict[Tuple[int, ...], str],\n",
        "    ph_to_ngram: Dict[str, Tuple[int, ...]]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Verifies that the abbreviation mappings are bijective and unique.\n",
        "\n",
        "    Args:\n",
        "        ngram_to_ph (Dict): N-gram to placeholder map.\n",
        "        ph_to_ngram (Dict): Placeholder to n-gram map.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid.\n",
        "    \"\"\"\n",
        "    if len(ngram_to_ph) != len(ph_to_ngram):\n",
        "        logger.error(\"Mismatch in dictionary lengths.\")\n",
        "        return False\n",
        "\n",
        "    # Check round-trip\n",
        "    for ngram, ph in ngram_to_ph.items():\n",
        "        if ph_to_ngram.get(ph) != ngram:\n",
        "            logger.error(f\"Mapping mismatch for {ph}\")\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def construct_abbreviation_dict_task(\n",
        "    top_k_ngrams: List[Tuple[Tuple[int, ...], int]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the abbreviation dictionary.\n",
        "\n",
        "    Args:\n",
        "        top_k_ngrams (List): Top-K n-grams from Task 22.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary containing:\n",
        "            - 'ngram_to_ph': Forward map.\n",
        "            - 'ph_to_ngram': Reverse map.\n",
        "            - 'metadata': Count of entries.\n",
        "    \"\"\"\n",
        "    logger.info(\"Constructing abbreviation dictionary...\")\n",
        "\n",
        "    # Build abbreviation maps\n",
        "    ngram_to_ph, ph_to_ngram = build_abbreviation_maps(top_k_ngrams)\n",
        "\n",
        "    if verify_abbreviation_maps(ngram_to_ph, ph_to_ngram):\n",
        "        logger.info(f\"Successfully constructed dictionary with {len(ngram_to_ph)} entries.\")\n",
        "    else:\n",
        "        logger.error(\"Dictionary construction failed validation.\")\n",
        "        # Return empty or raise error? We return what we have but logged error.\n",
        "\n",
        "    return {\n",
        "        \"ngram_to_ph\": ngram_to_ph,\n",
        "        \"ph_to_ngram\": ph_to_ngram,\n",
        "        \"metadata\": {\"count\": len(ngram_to_ph)}\n",
        "    }\n"
      ],
      "metadata": {
        "id": "CwqL_cbxissq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24 – Apply Abbreviation to Passage\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Apply Abbreviation to Passages\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 1 & 2: Apply Abbreviation\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def abbreviate_text(\n",
        "    text: str,\n",
        "    active_ngrams: Set[Tuple[int, ...]],\n",
        "    ngram_to_ph: Dict[Tuple[int, ...], str],\n",
        "    n: int,\n",
        "    encoding: Any\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Applies n-gram abbreviation to a text string using a greedy left-to-right strategy.\n",
        "\n",
        "    This function tokenizes the input text, scans for occurrences of active n-grams,\n",
        "    and replaces them with their corresponding placeholder tokens. Non-matching tokens\n",
        "    are decoded back to strings. The result is a single string containing a mix of\n",
        "    original text and placeholders.\n",
        "\n",
        "    Args:\n",
        "        text (str): The original passage text.\n",
        "        active_ngrams (Set[Tuple[int, ...]]): A set of n-gram tuples (token IDs) to be replaced.\n",
        "        ngram_to_ph (Dict[Tuple[int, ...], str]): Mapping from n-gram tuple to placeholder string.\n",
        "        n (int): The length of the n-grams (e.g., 2 for bigrams).\n",
        "        encoding (tiktoken.Encoding): The tokenizer used for encoding/decoding.\n",
        "\n",
        "    Returns:\n",
        "        str: The abbreviated text string.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Tokenize the text\n",
        "    # disallowed_special=() ensures we process all text content safely\n",
        "    tokens = encoding.encode(text, disallowed_special=())\n",
        "\n",
        "    output_parts: List[str] = []\n",
        "    i = 0\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    while i < num_tokens:\n",
        "        match_found = False\n",
        "\n",
        "        # Check if an n-gram can start at this position\n",
        "        if i <= num_tokens - n:\n",
        "            # Extract candidate n-gram\n",
        "            candidate = tuple(tokens[i : i + n])\n",
        "\n",
        "            # Check if candidate is in the active set\n",
        "            if candidate in active_ngrams:\n",
        "                # Found a match: append placeholder\n",
        "                placeholder = ngram_to_ph[candidate]\n",
        "                output_parts.append(placeholder)\n",
        "\n",
        "                # Skip n tokens\n",
        "                i += n\n",
        "                match_found = True\n",
        "\n",
        "        if not match_found:\n",
        "            # No match: decode the current token and append\n",
        "            # decode_single_token_bytes returns bytes, we decode to utf-8\n",
        "            token_bytes = encoding.decode_single_token_bytes(tokens[i])\n",
        "            token_str = token_bytes.decode('utf-8', errors='replace')\n",
        "            output_parts.append(token_str)\n",
        "            i += 1\n",
        "\n",
        "    # Join all parts to form the abbreviated string\n",
        "    return \"\".join(output_parts)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 3: Verify reversibility\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def verify_reversibility(\n",
        "    original_text: str,\n",
        "    abbreviated_text: str,\n",
        "    ph_to_ngram: Dict[str, Tuple[int, ...]],\n",
        "    encoding: Any\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Verifies that the abbreviated text can be reconstructed to match the original text.\n",
        "\n",
        "    This function attempts to reverse the abbreviation process by replacing placeholders\n",
        "    with their original n-gram text. It compares the reconstructed text with the original\n",
        "    text, normalizing whitespace to account for minor tokenization artifacts.\n",
        "\n",
        "    Args:\n",
        "        original_text (str): The original passage text.\n",
        "        abbreviated_text (str): The text after abbreviation.\n",
        "        ph_to_ngram (Dict[str, Tuple[int, ...]]): Mapping from placeholder string to n-gram tuple.\n",
        "        encoding (tiktoken.Encoding): The tokenizer used for decoding n-grams.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the reconstructed text matches the original (normalized), False otherwise.\n",
        "    \"\"\"\n",
        "    reconstructed_text = abbreviated_text\n",
        "\n",
        "    # Iterate through placeholders and replace them\n",
        "    # Note: We assume placeholders are unique strings like \"[PH001]\"\n",
        "    for ph, ngram in ph_to_ngram.items():\n",
        "        if ph in reconstructed_text:\n",
        "            # Decode the original n-gram tokens to text\n",
        "            ngram_text = encoding.decode(list(ngram))\n",
        "            reconstructed_text = reconstructed_text.replace(ph, ngram_text)\n",
        "\n",
        "    # Normalize whitespace for comparison\n",
        "    # Tokenization/detokenization can sometimes alter spacing (e.g. \"word .\" vs \"word.\")\n",
        "    norm_orig = \" \".join(original_text.split())\n",
        "    norm_recon = \" \".join(reconstructed_text.split())\n",
        "\n",
        "    return norm_orig == norm_recon\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def apply_abbreviation_task(\n",
        "    tatqa_df: pd.DataFrame,\n",
        "    finqa_df: pd.DataFrame,\n",
        "    abbrev_dict: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the application of n-gram abbreviation to passages in both datasets.\n",
        "\n",
        "    This function:\n",
        "    1. Determines the active set of n-grams (Top-T) based on configuration.\n",
        "    2. Iterates through all passages in TAT-QA and Fin-QA.\n",
        "    3. Applies abbreviation to each passage.\n",
        "    4. Stores the abbreviated text in a new column 'abbreviated_passages'.\n",
        "\n",
        "    Args:\n",
        "        tatqa_df (pd.DataFrame): TAT-QA DataFrame containing 'passages'.\n",
        "        finqa_df (pd.DataFrame): Fin-QA DataFrame containing 'passages'.\n",
        "        abbrev_dict (Dict[str, Any]): Dictionary containing 'ngram_to_ph' and 'ph_to_ngram'.\n",
        "        config (Dict[str, Any]): Configuration dictionary with 'ngram_abbreviation_config'.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: The input DataFrames with a new 'abbreviated_passages' column.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting abbreviation application task...\")\n",
        "\n",
        "    # Extract mappings\n",
        "    ngram_to_ph = abbrev_dict[\"ngram_to_ph\"]\n",
        "    ph_to_ngram = abbrev_dict[\"ph_to_ngram\"]\n",
        "\n",
        "    # Extract configuration parameters\n",
        "    ngram_config = config.get(\"ngram_abbreviation_config\", {})\n",
        "    T = ngram_config.get(\"top_n_T\", {}).get(\"best_performing_value\", 3)\n",
        "    n = ngram_config.get(\"ngram_size_G\", {}).get(\"best_performing_value\", 2)\n",
        "    encoding_name = config.get(\"offline_corpus_config\", {}).get(\"tokenization_scheme\", {}).get(\"name\", \"cl100k_base\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    try:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load encoding '{encoding_name}': {e}. Falling back to cl100k_base.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    # Determine active n-grams (Top-T)\n",
        "    # We assume placeholders are named sequentially [PH001], [PH002], etc.\n",
        "    active_ngrams: Set[Tuple[int, ...]] = set()\n",
        "    for i in range(1, T + 1):\n",
        "        ph = f\"[PH{i:03d}]\"\n",
        "        if ph in ph_to_ngram:\n",
        "            active_ngrams.add(ph_to_ngram[ph])\n",
        "\n",
        "    logger.info(f\"Active n-grams count (T={T}): {len(active_ngrams)}\")\n",
        "\n",
        "    # Helper to process a list of passages\n",
        "    def process_passages_list(passages_list: Any) -> List[Dict[str, Any]]:\n",
        "        if not isinstance(passages_list, list):\n",
        "            return []\n",
        "\n",
        "        new_passages = []\n",
        "        for p in passages_list:\n",
        "            if not isinstance(p, dict):\n",
        "                continue\n",
        "\n",
        "            original_text = p.get(\"text\", \"\")\n",
        "            # Apply abbreviation\n",
        "            abbr_text = abbreviate_text(original_text, active_ngrams, ngram_to_ph, n, encoding)\n",
        "\n",
        "            # Create a copy of the passage dict with updated text\n",
        "            new_p = p.copy()\n",
        "            new_p[\"text\"] = abbr_text\n",
        "            new_passages.append(new_p)\n",
        "\n",
        "        return new_passages\n",
        "\n",
        "    # Apply to TAT-QA\n",
        "    logger.info(\"Abbreviating TAT-QA passages...\")\n",
        "    tatqa_df[\"abbreviated_passages\"] = tatqa_df[\"passages\"].apply(process_passages_list)\n",
        "\n",
        "    # Apply to Fin-QA\n",
        "    logger.info(\"Abbreviating Fin-QA passages...\")\n",
        "    finqa_df[\"abbreviated_passages\"] = finqa_df[\"passages\"].apply(process_passages_list)\n",
        "\n",
        "    logger.info(\"Abbreviation task complete.\")\n",
        "    return tatqa_df, finqa_df\n"
      ],
      "metadata": {
        "id": "4Puy3ZZukutx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25 – Identify Numeric Columns and Extract Values for Quantization\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Identify Numeric Columns and Extract Values for Quantization\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 1 & 2: Extract Values\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def extract_column_values(\n",
        "    df: pd.DataFrame,\n",
        "    dataset_name: str,\n",
        "    numeric_metadata: Dict[Tuple[str, str, str, int], bool],\n",
        "    normalizer: Any # FinancialTextNormalizer\n",
        ") -> Dict[Tuple[str, str, str, int], List[float]]:\n",
        "    \"\"\"\n",
        "    Extracts parsed float values from columns identified as numeric.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset DataFrame.\n",
        "        dataset_name (str): \"TAT-QA\" or \"Fin-QA\".\n",
        "        numeric_metadata (Dict): Metadata indicating which columns are numeric.\n",
        "        normalizer (FinancialTextNormalizer): Parser instance.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Mapping from column key to list of parsed float values.\n",
        "    \"\"\"\n",
        "    extracted_values = {}\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        example_id = row[\"example_id\"]\n",
        "        tables = row[\"tables\"]\n",
        "\n",
        "        if not isinstance(tables, list):\n",
        "            continue\n",
        "\n",
        "        for table in tables:\n",
        "            if not isinstance(table, dict):\n",
        "                continue\n",
        "\n",
        "            table_id = table.get(\"table_id\", \"unknown\")\n",
        "            rows = table.get(\"rows\", [])\n",
        "\n",
        "            # Check which columns in this table are numeric\n",
        "            # We iterate through known numeric columns for this table\n",
        "            # Optimization: iterate through columns in the table and check metadata\n",
        "            if not rows:\n",
        "                continue\n",
        "\n",
        "            num_cols = len(rows[0]) # Assume consistent row length from cleansing\n",
        "\n",
        "            for col_idx in range(num_cols):\n",
        "                key = (dataset_name, example_id, table_id, col_idx)\n",
        "\n",
        "                # Only process if marked numeric\n",
        "                if not numeric_metadata.get(key, False):\n",
        "                    continue\n",
        "\n",
        "                # Extract values\n",
        "                col_values = []\n",
        "                for r in rows:\n",
        "                    if col_idx < len(r):\n",
        "                        val_str = str(r[col_idx])\n",
        "                        parsed = normalizer.parse_to_float(val_str)\n",
        "                        if parsed is not None:\n",
        "                            col_values.append(parsed)\n",
        "\n",
        "                if col_values:\n",
        "                    extracted_values[key] = col_values\n",
        "\n",
        "    return extracted_values\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def extract_numeric_values_task(\n",
        "    tatqa_df: pd.DataFrame,\n",
        "    finqa_df: pd.DataFrame,\n",
        "    numeric_metadata: Dict[Tuple[str, str, str, int], bool]\n",
        ") -> Dict[Tuple[str, str, str, int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction of numeric values for quantization.\n",
        "\n",
        "    Args:\n",
        "        tatqa_df (pd.DataFrame): TAT-QA DataFrame.\n",
        "        finqa_df (pd.DataFrame): Fin-QA DataFrame.\n",
        "        numeric_metadata (Dict): Metadata from Task 6.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Mapping column key -> {'values': List[float], 'count': int}.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting numeric value extraction...\")\n",
        "\n",
        "    # We need the normalizer from Task 6\n",
        "    # Assuming FinancialTextNormalizer is available in scope\n",
        "    normalizer = FinancialTextNormalizer()\n",
        "\n",
        "    # Extract TAT-QA\n",
        "    tatqa_values = extract_column_values(tatqa_df, \"TAT-QA\", numeric_metadata, normalizer)\n",
        "\n",
        "    # Extract Fin-QA\n",
        "    finqa_values = extract_column_values(finqa_df, \"Fin-QA\", numeric_metadata, normalizer)\n",
        "\n",
        "    # Merge and format\n",
        "    all_values = {**tatqa_values, **finqa_values}\n",
        "\n",
        "    final_output = {}\n",
        "    for key, vals in all_values.items():\n",
        "        final_output[key] = {\n",
        "            \"values\": vals,\n",
        "            \"count\": len(vals)\n",
        "        }\n",
        "\n",
        "    logger.info(f\"Extracted values for {len(final_output)} numeric columns.\")\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "qmNcsweppEH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26 – Apply Uniform Integer Quantization\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Apply Uniform Integer Quantization\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 1 & 2: Compute Range and Encode\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def quantize_column_uniform(\n",
        "    values: List[float],\n",
        "    bit_width: int = 8\n",
        ") -> Tuple[List[int], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Applies uniform integer quantization to a list of float values.\n",
        "\n",
        "    Equations:\n",
        "        L = 2^b\n",
        "        q_i = round((x_i - min_x) / (max_x - min_x) * (L - 1))\n",
        "        epsilon_max = (max_x - min_x) / (L - 1)\n",
        "\n",
        "    Args:\n",
        "        values (List[float]): List of numeric values.\n",
        "        bit_width (int): Bit width b (default 8).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[int], Dict]:\n",
        "            - List of quantized integer codes.\n",
        "            - Metadata dictionary (min_x, max_x, L, epsilon_max).\n",
        "    \"\"\"\n",
        "    if not values:\n",
        "        return [], {\"min_x\": 0.0, \"max_x\": 0.0, \"L\": 0, \"epsilon_max\": 0.0}\n",
        "\n",
        "    min_x = min(values)\n",
        "    max_x = max(values)\n",
        "    L = 2 ** bit_width\n",
        "\n",
        "    # Handle constant column\n",
        "    if max_x == min_x:\n",
        "        # All values map to 0\n",
        "        codes = [0] * len(values)\n",
        "        epsilon_max = 0.0\n",
        "        return codes, {\n",
        "            \"min_x\": min_x,\n",
        "            \"max_x\": max_x,\n",
        "            \"L\": L,\n",
        "            \"epsilon_max\": epsilon_max\n",
        "        }\n",
        "\n",
        "    # Compute codes\n",
        "    codes = []\n",
        "    denominator = max_x - min_x\n",
        "    scale = L - 1\n",
        "\n",
        "    for x in values:\n",
        "        normalized = (x - min_x) / denominator\n",
        "        q = int(round(normalized * scale))\n",
        "        # Clip to ensure bounds (floating point errors might push slightly outside)\n",
        "        q = max(0, min(scale, q))\n",
        "        codes.append(q)\n",
        "\n",
        "    epsilon_max = denominator / scale\n",
        "\n",
        "    return codes, {\n",
        "        \"min_x\": min_x,\n",
        "        \"max_x\": max_x,\n",
        "        \"L\": L,\n",
        "        \"epsilon_max\": epsilon_max\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 3: Compute reconstruction (Helper)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def reconstruct_uniform(\n",
        "    codes: List[int],\n",
        "    metadata: Dict[str, float]\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Reconstructs approximate values from quantized codes.\n",
        "\n",
        "    Equation:\n",
        "        x_hat = min_x + (q_i / (L - 1)) * (max_x - min_x)\n",
        "\n",
        "    Args:\n",
        "        codes (List[int]): Quantized codes.\n",
        "        metadata (Dict): Metadata from quantization.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: Reconstructed values.\n",
        "    \"\"\"\n",
        "    # Extract values\n",
        "    min_x = metadata[\"min_x\"]\n",
        "    max_x = metadata[\"max_x\"]\n",
        "    L = int(metadata[\"L\"])\n",
        "\n",
        "    if max_x == min_x:\n",
        "        return [min_x] * len(codes)\n",
        "\n",
        "    # Compute scale and range\n",
        "    scale = L - 1\n",
        "    range_x = max_x - min_x\n",
        "\n",
        "    reconstructed = []\n",
        "\n",
        "    # Compute q\n",
        "    for q in codes:\n",
        "        val = min_x + (q / scale) * range_x\n",
        "        reconstructed.append(val)\n",
        "\n",
        "    return reconstructed\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def apply_uniform_quantization_task(\n",
        "    extracted_values: Dict[Tuple[str, str, str, int], Dict[str, Any]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[Tuple[str, str, str, int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates uniform quantization for all extracted numeric columns.\n",
        "\n",
        "    Args:\n",
        "        extracted_values (Dict): Output from Task 25.\n",
        "        config (Dict): Configuration containing bit width.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Mapping column key -> {\n",
        "            'codes': List[int],\n",
        "            'metadata': Dict\n",
        "        }\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting uniform quantization...\")\n",
        "\n",
        "    bit_width = config.get(\"numeric_quantization_config\", {}).get(\"uniform_integer\", {}).get(\"bit_width_b\", 8)\n",
        "    results = {}\n",
        "\n",
        "    # Iterate through extracted values\n",
        "    for key, data in extracted_values.items():\n",
        "        values = data[\"values\"]\n",
        "\n",
        "        # Quantize column data\n",
        "        codes, meta = quantize_column_uniform(values, bit_width)\n",
        "\n",
        "        results[key] = {\n",
        "            \"codes\": codes,\n",
        "            \"metadata\": meta\n",
        "        }\n",
        "\n",
        "    logger.info(f\"Quantized {len(results)} columns.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "EhpJqN6zrHQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27 – (Optional) Apply K-Means-Based Quantization\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: (Optional) Apply K-Means-Based Quantization\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 1 & 2: Run K-Means and Encode\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def quantize_column_kmeans(\n",
        "    values: List[float],\n",
        "    k: int = 16\n",
        ") -> Tuple[List[int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Applies K-Means quantization to a list of float values.\n",
        "\n",
        "    Args:\n",
        "        values (List[float]): List of numeric values.\n",
        "        k (int): Number of clusters (default 16).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[int], Dict]:\n",
        "            - List of quantized integer codes (cluster indices).\n",
        "            - Metadata dictionary (centroids, k, mse).\n",
        "    \"\"\"\n",
        "    if not values:\n",
        "        return [], {\"centroids\": [], \"k\": 0, \"mse\": 0.0}\n",
        "\n",
        "    # Adjust k if we have fewer unique values than k\n",
        "    unique_vals = len(set(values))\n",
        "    if unique_vals < k:\n",
        "        k = unique_vals\n",
        "\n",
        "    if k == 0: # Should not happen if values is not empty\n",
        "         return [], {\"centroids\": [], \"k\": 0, \"mse\": 0.0}\n",
        "\n",
        "    # Reshape for sklearn\n",
        "    X = np.array(values).reshape(-1, 1)\n",
        "\n",
        "    # Fit KMeans\n",
        "    # n_init='auto' is default in newer sklearn, but explicit 10 is safe\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    codes = kmeans.labels_.tolist()\n",
        "    centroids = kmeans.cluster_centers_.flatten().tolist()\n",
        "\n",
        "    # Compute MSE\n",
        "    # inertia_ is sum of squared distances to nearest centroid\n",
        "    mse = kmeans.inertia_ / len(values)\n",
        "\n",
        "    return codes, {\n",
        "        \"centroids\": centroids,\n",
        "        \"k\": k,\n",
        "        \"mse\": mse\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 3: Reconstruction (Helper)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def reconstruct_kmeans(\n",
        "    codes: List[int],\n",
        "    metadata: Dict[str, Any]\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Reconstructs values from k-means codes.\n",
        "\n",
        "    Args:\n",
        "        codes (List[int]): Cluster indices.\n",
        "        metadata (Dict): Metadata containing centroids.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: Reconstructed values (centroids).\n",
        "    \"\"\"\n",
        "    centroids = metadata[\"centroids\"]\n",
        "    if not centroids:\n",
        "        return []\n",
        "\n",
        "    return [centroids[c] for c in codes]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def apply_kmeans_quantization_task(\n",
        "    extracted_values: Dict[Tuple[str, str, str, int], Dict[str, Any]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[Tuple[str, str, str, int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates K-Means quantization for all extracted numeric columns.\n",
        "\n",
        "    Args:\n",
        "        extracted_values (Dict): Output from Task 25.\n",
        "        config (Dict): Configuration containing k.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Mapping column key -> {\n",
        "            'codes': List[int],\n",
        "            'metadata': Dict\n",
        "        }\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting K-Means quantization...\")\n",
        "\n",
        "    k = config.get(\"numeric_quantization_config\", {}).get(\"kmeans_based\", {}).get(\"num_clusters_k\", 16)\n",
        "    results = {}\n",
        "\n",
        "    # Iterate through extracted values\n",
        "    for key, data in extracted_values.items():\n",
        "        values = data[\"values\"]\n",
        "\n",
        "        # Quantize column means\n",
        "        codes, meta = quantize_column_kmeans(values, k)\n",
        "\n",
        "        results[key] = {\n",
        "            \"codes\": codes,\n",
        "            \"metadata\": meta\n",
        "        }\n",
        "\n",
        "    logger.info(f\"Quantized {len(results)} columns using K-Means.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "C2g9jE23mxGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28 – Embed Candidate Examples for Few-Shot Selection\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 28: Embed Candidate Examples for Few-Shot Selection\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 1: Define textual representation\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def construct_embedding_text(row: pd.Series) -> str:\n",
        "    \"\"\"\n",
        "    Constructs the text representation of an example for embedding.\n",
        "\n",
        "    Format:\n",
        "    Question: {question}\n",
        "    Context:\n",
        "    {serialized_tables}\n",
        "    {passages}\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): DataFrame row.\n",
        "\n",
        "    Returns:\n",
        "        str: Text to embed.\n",
        "    \"\"\"\n",
        "    question = row.get(\"question_text\", \"\").strip()\n",
        "\n",
        "    tables = row.get(\"serialized_tables\", [])\n",
        "    tables_str = \"\\n\".join(tables) if isinstance(tables, list) else \"\"\n",
        "\n",
        "    passages = row.get(\"passages\", [])\n",
        "    passages_str = \"\"\n",
        "    if isinstance(passages, list):\n",
        "        passages_str = \"\\n\".join([p.get(\"text\", \"\").strip() for p in passages if isinstance(p, dict)])\n",
        "\n",
        "    return f\"Question: {question}\\nContext:\\n{tables_str}\\n{passages_str}\"\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 2: Compute embeddings\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_embeddings(\n",
        "    texts: List[str],\n",
        "    model_name: str = \"all-mpnet-base-v2\",\n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes embeddings for a list of texts using SentenceTransformers.\n",
        "\n",
        "    Args:\n",
        "        texts (List[str]): List of texts.\n",
        "        model_name (str): Model name.\n",
        "        batch_size (int): Batch size.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Embedding matrix of shape (N, 768).\n",
        "    \"\"\"\n",
        "    logger.info(f\"Loading embedding model: {model_name}\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    logger.info(f\"Encoding {len(texts)} texts...\")\n",
        "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def embed_examples_task(\n",
        "    df: pd.DataFrame,\n",
        "    dataset_name: str,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"embeddings\"\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the embedding of examples.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        dataset_name (str): Dataset name.\n",
        "        config (Dict): Configuration.\n",
        "        output_dir (str): Directory to save embeddings.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, List[str]]: Embeddings matrix and list of example IDs.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Starting embedding task for {dataset_name}...\")\n",
        "\n",
        "    # Step 1: Construct texts\n",
        "    texts = df.apply(construct_embedding_text, axis=1).tolist()\n",
        "    example_ids = df[\"example_id\"].tolist()\n",
        "\n",
        "    # Step 2: Compute\n",
        "    model_name = config.get(\"fewshot_selection_config\", {}).get(\"embedding_model\", {}).get(\"name\", \"all-mpnet-base-v2\")\n",
        "    embeddings = compute_embeddings(texts, model_name)\n",
        "\n",
        "    # Step 3: Save\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    np.save(os.path.join(output_dir, f\"{dataset_name}_embeddings.npy\"), embeddings)\n",
        "    with open(os.path.join(output_dir, f\"{dataset_name}_ids.json\"), \"w\") as f:\n",
        "        json.dump(example_ids, f)\n",
        "\n",
        "    logger.info(f\"Embeddings saved to {output_dir}\")\n",
        "    return embeddings, example_ids\n"
      ],
      "metadata": {
        "id": "lRhJpbSRwR77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 29 – Select Optimal Cluster Count via Silhouette Score\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 29: Select Optimal Cluster Count via Silhouette Score\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Step 1 & 2: Run K-Means and Compute Silhouette\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_silhouette_scores(\n",
        "    embeddings: np.ndarray,\n",
        "    k_range: range\n",
        ") -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Computes average silhouette scores for a range of cluster counts k.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Embedding matrix of shape (N, D).\n",
        "        k_range (range): Range of k values to test.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, float]: Mapping from k to average silhouette score.\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "\n",
        "    # Ensure we have enough samples for clustering\n",
        "    n_samples = embeddings.shape[0]\n",
        "    if n_samples < 2:\n",
        "        logger.warning(\"Not enough samples for clustering.\")\n",
        "        return {}\n",
        "\n",
        "    # Adjust range if n_samples is small\n",
        "    valid_k_range = [k for k in k_range if k < n_samples]\n",
        "\n",
        "    for k in valid_k_range:\n",
        "        # Fit KMeans\n",
        "        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Compute Silhouette\n",
        "        # This can be slow for very large N\n",
        "        score = silhouette_score(embeddings, labels, metric='euclidean')\n",
        "        scores[k] = score\n",
        "\n",
        "        logger.info(f\"k={k}: Silhouette Score = {score:.4f}\")\n",
        "\n",
        "    return scores\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Step 3: Select optimal k\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def select_best_k(scores: Dict[int, float]) -> int:\n",
        "    \"\"\"\n",
        "    Selects the k with the maximum silhouette score.\n",
        "\n",
        "    Args:\n",
        "        scores (Dict[int, float]): Mapping k -> score.\n",
        "\n",
        "    Returns:\n",
        "        int: The optimal k.\n",
        "    \"\"\"\n",
        "    if not scores:\n",
        "        return 5 # Default fallback\n",
        "\n",
        "    best_k = max(scores, key=scores.get)\n",
        "    logger.info(f\"Optimal k selected: {best_k} (Score: {scores[best_k]:.4f})\")\n",
        "    return best_k\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def select_optimal_k_task(\n",
        "    embeddings: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Orchestrates the selection of the optimal cluster count.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Embedding matrix.\n",
        "        config (Dict): Configuration containing k range.\n",
        "\n",
        "    Returns:\n",
        "        int: Optimal k.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting optimal k selection...\")\n",
        "\n",
        "    k_cfg = config.get(\"fewshot_selection_config\", {}).get(\"kmeans_candidate_k_range\", {})\n",
        "    start = k_cfg.get(\"start\", 5)\n",
        "    end = k_cfg.get(\"end\", 50)\n",
        "\n",
        "    # Range is inclusive in config description, python range is exclusive at end\n",
        "    k_range = range(start, end + 1)\n",
        "\n",
        "    scores = compute_silhouette_scores(embeddings, k_range)\n",
        "    best_k = select_best_k(scores)\n",
        "\n",
        "    return best_k\n"
      ],
      "metadata": {
        "id": "TZaM02fwySk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 30 – Select Representative Exemplars from Clusters\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 30: Select Representative Exemplars from Clusters\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Step 1 & 2: Run K-Means and Find Prototypes\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def find_cluster_prototypes(\n",
        "    embeddings: np.ndarray,\n",
        "    k: int\n",
        ") -> Tuple[List[int], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Runs K-Means and identifies the index of the sample closest to each centroid.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Embedding matrix.\n",
        "        k (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[int], np.ndarray]:\n",
        "            - List of indices of prototype samples (one per cluster).\n",
        "            - Array of cluster labels for all samples.\n",
        "    \"\"\"\n",
        "    if k < 1:\n",
        "        return [], np.array([])\n",
        "\n",
        "    # Fit KMeans\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    # Find closest sample to each centroid\n",
        "    # pairwise_distances_argmin_min returns (indices, distances)\n",
        "    closest_indices, _ = pairwise_distances_argmin_min(centroids, embeddings)\n",
        "\n",
        "    return closest_indices.tolist(), labels\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Step 3: Select top prototypes\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def select_top_prototypes(\n",
        "    prototype_indices: List[int],\n",
        "    labels: np.ndarray,\n",
        "    num_exemplars: int = 3\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    Selects prototypes from the largest clusters.\n",
        "\n",
        "    Args:\n",
        "        prototype_indices (List[int]): Indices of prototypes (aligned with cluster ID 0..k-1).\n",
        "        labels (np.ndarray): Cluster labels for all samples.\n",
        "        num_exemplars (int): Number of exemplars to select.\n",
        "\n",
        "    Returns:\n",
        "        List[int]: Indices of the selected representative exemplars.\n",
        "    \"\"\"\n",
        "    # Count cluster sizes\n",
        "    # labels are 0..k-1\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    cluster_sizes = dict(zip(unique, counts))\n",
        "\n",
        "    # Sort cluster IDs by size descending\n",
        "    sorted_clusters = sorted(cluster_sizes.keys(), key=lambda c: cluster_sizes[c], reverse=True)\n",
        "\n",
        "    # Select top clusters\n",
        "    selected_clusters = sorted_clusters[:num_exemplars]\n",
        "\n",
        "    # Get corresponding prototypes\n",
        "    # prototype_indices[j] is the prototype for cluster j\n",
        "    selected_indices = [prototype_indices[c] for c in selected_clusters]\n",
        "\n",
        "    return selected_indices\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def select_representative_exemplars_task(\n",
        "    embeddings: np.ndarray,\n",
        "    example_ids: List[str],\n",
        "    k_star: int,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Orchestrates the selection of representative few-shot exemplars.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Embedding matrix.\n",
        "        example_ids (List[str]): List of example IDs corresponding to rows.\n",
        "        k_star (int): Optimal cluster count.\n",
        "        config (Dict): Configuration.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of selected example_ids.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Selecting representative exemplars with k={k_star}...\")\n",
        "\n",
        "    num_exemplars = config.get(\"fewshot_selection_config\", {}).get(\"num_exemplars_per_prompt\", 3)\n",
        "\n",
        "    # Step 1 & 2: Prototypes\n",
        "    proto_indices, labels = find_cluster_prototypes(embeddings, k_star)\n",
        "\n",
        "    # Step 3: Selection\n",
        "    selected_indices = select_top_prototypes(proto_indices, labels, num_exemplars)\n",
        "\n",
        "    # Map back to IDs\n",
        "    selected_ids = [example_ids[i] for i in selected_indices]\n",
        "\n",
        "    logger.info(f\"Selected {len(selected_ids)} exemplars: {selected_ids}\")\n",
        "    return selected_ids\n"
      ],
      "metadata": {
        "id": "8mdcEgY00qpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 31 – Compute Embeddings for Semantic Similarity Evaluation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 31: Compute Embeddings for Semantic Similarity Evaluation\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 31, Step 1: Define pairs\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def align_prompt_pairs(\n",
        "    dynamic_inputs: Dict[str, Dict[str, Any]],\n",
        "    pruned_results: Dict[str, Dict[str, Any]]\n",
        ") -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Aligns original and compressed prompts by example_id.\n",
        "\n",
        "    Args:\n",
        "        dynamic_inputs (Dict): Original prompt data.\n",
        "        pruned_results (Dict): Compressed prompt data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], List[str], List[str]]:\n",
        "            - List of original prompt texts.\n",
        "            - List of compressed prompt texts.\n",
        "            - List of corresponding example IDs.\n",
        "    \"\"\"\n",
        "    original_texts = []\n",
        "    compressed_texts = []\n",
        "    ids = []\n",
        "\n",
        "    # Intersection of keys\n",
        "    common_ids = sorted(list(set(dynamic_inputs.keys()) & set(pruned_results.keys())))\n",
        "\n",
        "    for eid in common_ids:\n",
        "        orig = dynamic_inputs[eid][\"prompt_text\"]\n",
        "        comp = pruned_results[eid][\"compressed_text\"]\n",
        "\n",
        "        original_texts.append(orig)\n",
        "        compressed_texts.append(comp)\n",
        "        ids.append(eid)\n",
        "\n",
        "    return original_texts, compressed_texts, ids\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 31, Step 2 & 3: Compute and Store (Orchestrator)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_similarity_embeddings_task(\n",
        "    dynamic_inputs: Dict[str, Dict[str, Any]],\n",
        "    pruned_results: Dict[str, Dict[str, Any]],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"similarity_embeddings\"\n",
        ") -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of embeddings for semantic similarity.\n",
        "\n",
        "    Args:\n",
        "        dynamic_inputs (Dict): Original prompts.\n",
        "        pruned_results (Dict): Compressed prompts.\n",
        "        config (Dict): Configuration.\n",
        "        output_dir (str): Output directory.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "            - Matrix U (original embeddings).\n",
        "            - Matrix V (compressed embeddings).\n",
        "            - List of example IDs.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting similarity embedding computation...\")\n",
        "\n",
        "    # Step 1: Align\n",
        "    orig_texts, comp_texts, ids = align_prompt_pairs(dynamic_inputs, pruned_results)\n",
        "\n",
        "    if not ids:\n",
        "        logger.warning(\"No common examples found for similarity computation.\")\n",
        "        return np.array([]), np.array([]), []\n",
        "\n",
        "    # Step 2: Compute\n",
        "    # Reuse compute_embeddings from Task 28\n",
        "    # We assume compute_embeddings is available in scope\n",
        "    model_name = config.get(\"semantic_evaluation_config\", {}).get(\"embedding_model\", {}).get(\"name\", \"all-mpnet-base-v2\")\n",
        "\n",
        "    logger.info(\"Embedding original prompts...\")\n",
        "    U = compute_embeddings(orig_texts, model_name)\n",
        "\n",
        "    logger.info(\"Embedding compressed prompts...\")\n",
        "    V = compute_embeddings(comp_texts, model_name)\n",
        "\n",
        "    # Step 3: Save\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    np.save(os.path.join(output_dir, \"U_original.npy\"), U)\n",
        "    np.save(os.path.join(output_dir, \"V_compressed.npy\"), V)\n",
        "    with open(os.path.join(output_dir, \"ids.json\"), \"w\") as f:\n",
        "        json.dump(ids, f)\n",
        "\n",
        "    logger.info(f\"Similarity embeddings saved to {output_dir}\")\n",
        "    return U, V, ids\n"
      ],
      "metadata": {
        "id": "88Kxr9H24MNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 32 – Compute Cosine Similarities and Summary Statistics\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 32: Compute Cosine Similarities and Summary Statistics\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Step 1: Compute Cosine Similarity\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_cosine_similarities(\n",
        "    U: np.ndarray,\n",
        "    V: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between corresponding rows of two matrices.\n",
        "\n",
        "    Equation:\n",
        "        s_i = (u_i . v_i) / (||u_i|| * ||v_i||)\n",
        "\n",
        "    Args:\n",
        "        U (np.ndarray): Matrix of original embeddings (N, D).\n",
        "        V (np.ndarray): Matrix of compressed embeddings (N, D).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array of cosine similarities (N,).\n",
        "    \"\"\"\n",
        "    if U.shape != V.shape:\n",
        "        raise ValueError(f\"Shape mismatch: U={U.shape}, V={V.shape}\")\n",
        "\n",
        "    # Compute dot products\n",
        "    # element-wise multiply then sum along axis 1\n",
        "    dot_products = np.sum(U * V, axis=1)\n",
        "\n",
        "    # Compute norms\n",
        "    norm_u = np.linalg.norm(U, axis=1)\n",
        "    norm_v = np.linalg.norm(V, axis=1)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    # Replace 0 norms with 1 (similarity will be 0)\n",
        "    norm_u[norm_u == 0] = 1.0\n",
        "    norm_v[norm_v == 0] = 1.0\n",
        "\n",
        "    similarities = dot_products / (norm_u * norm_v)\n",
        "\n",
        "    # Clip to [-1, 1] to handle floating point errors\n",
        "    similarities = np.clip(similarities, -1.0, 1.0)\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Step 2 & 3: Compute Statistics and Thresholds\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def analyze_similarity_stats(\n",
        "    similarities: np.ndarray,\n",
        "    threshold: float = 0.92\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes summary statistics and threshold violations.\n",
        "\n",
        "    Args:\n",
        "        similarities (np.ndarray): Array of similarity scores.\n",
        "        threshold (float): Safety threshold (default 0.92).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: Dictionary of statistics.\n",
        "    \"\"\"\n",
        "    if len(similarities) == 0:\n",
        "        return {}\n",
        "\n",
        "    stats = {\n",
        "        \"mean_similarity\": float(np.mean(similarities)),\n",
        "        \"median_similarity\": float(np.median(similarities)),\n",
        "        \"p05_similarity\": float(np.percentile(similarities, 5)),\n",
        "        \"min_similarity\": float(np.min(similarities)),\n",
        "        \"max_similarity\": float(np.max(similarities)),\n",
        "        \"std_dev\": float(np.std(similarities)),\n",
        "        \"fraction_below_threshold\": float(np.mean(similarities < threshold))\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def evaluate_semantic_similarity_task(\n",
        "    U: np.ndarray,\n",
        "    V: np.ndarray,\n",
        "    example_ids: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the semantic similarity evaluation.\n",
        "\n",
        "    Args:\n",
        "        U (np.ndarray): Original embeddings.\n",
        "        V (np.ndarray): Compressed embeddings.\n",
        "        example_ids (List[str]): Example IDs.\n",
        "        config (Dict): Configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Results containing per-example scores and aggregate stats.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting semantic similarity evaluation...\")\n",
        "\n",
        "    threshold = config.get(\"semantic_evaluation_config\", {}).get(\"interpretive_thresholds\", {}).get(\"semantic_safety_similarity_min\", 0.92)\n",
        "\n",
        "    # Step 1: Compute\n",
        "    similarities = compute_cosine_similarities(U, V)\n",
        "\n",
        "    # Step 2 & 3: Analyze\n",
        "    stats = analyze_similarity_stats(similarities, threshold)\n",
        "\n",
        "    # Map scores to IDs\n",
        "    per_example_scores = {eid: float(score) for eid, score in zip(example_ids, similarities)}\n",
        "\n",
        "    # Identify flagged examples\n",
        "    flagged_ids = [eid for eid, score in per_example_scores.items() if score < threshold]\n",
        "\n",
        "    logger.info(f\"Mean Similarity: {stats['mean_similarity']:.4f}\")\n",
        "    logger.info(f\"5th Percentile: {stats['p05_similarity']:.4f}\")\n",
        "    logger.info(f\"Flagged {len(flagged_ids)} examples below {threshold}\")\n",
        "\n",
        "    return {\n",
        "        \"stats\": stats,\n",
        "        \"per_example_scores\": per_example_scores,\n",
        "        \"flagged_ids\": flagged_ids\n",
        "    }\n"
      ],
      "metadata": {
        "id": "wCGt_cpu6le_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 33 – Recruit and Calibrate Human Annotators\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 33: Recruit and Calibrate Human Annotators (LLM Proxies)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 33, Step 1: Create Annotation Prompt Template\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def get_annotation_prompt_template() -> str:\n",
        "    \"\"\"\n",
        "    Returns the prompt template for LLM-based semantic equivalence evaluation.\n",
        "\n",
        "    This template is designed to be neutral and unbiased. It presents two text\n",
        "    segments labeled \"Text A\" and \"Text B\" without identifying which is the\n",
        "    original and which is the compressed version. This blinding prevents the\n",
        "    evaluator model from biasing its score based on the label.\n",
        "\n",
        "    The prompt instructs the model to act as a human annotator and rate the\n",
        "    semantic equivalence on a 1-5 scale, focusing on financial facts and logic.\n",
        "\n",
        "    Returns:\n",
        "        str: The prompt template string with placeholders `{text_a}` and `{text_b}`.\n",
        "    \"\"\"\n",
        "    return \"\"\"You are an expert financial analyst acting as a data quality evaluator.\n",
        "            Your task is to rate the semantic equivalence between two text segments, labeled \"Text A\" and \"Text B\".\n",
        "\n",
        "            Rating Scale:\n",
        "            1: Completely different meaning. The texts are unrelated or contradict each other.\n",
        "            2: Mostly different. Key financial figures or logic present in one are missing or altered significantly in the other.\n",
        "            3: Similar but noticeable differences. Some nuance is lost, or minor details are inconsistent between the texts.\n",
        "            4: Mostly identical. The core meaning and all key figures are preserved; only minor stylistic changes or negligible omissions exist.\n",
        "            5: Completely identical meaning. Both texts convey exactly the same information with no loss of semantic content.\n",
        "\n",
        "            Input:\n",
        "            Text A:\n",
        "            {text_a}\n",
        "\n",
        "            Text B:\n",
        "            {text_b}\n",
        "\n",
        "            Instructions:\n",
        "            - Compare Text A and Text B for semantic equivalence.\n",
        "            - Focus on financial facts (numbers, dates, entities, direction of change).\n",
        "            - Ignore minor grammatical fluency issues if the meaning remains clear.\n",
        "            - Do not assume one text is the \"ground truth\"; evaluate their mutual consistency.\n",
        "            - Provide your response in strict JSON format with keys \"score\" (integer 1-5) and \"reasoning\" (string).\n",
        "\n",
        "            Response Format:\n",
        "            {\n",
        "              \"score\": <int>,\n",
        "              \"reasoning\": \"<string>\"\n",
        "            }\n",
        "            \"\"\"\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 33, Step 2: Initialize LLM Agents\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def initialize_proxy_annotators() -> Dict[str, LLMInterface]:\n",
        "    \"\"\"\n",
        "    Initializes the LLM agents that will serve as human proxies.\n",
        "\n",
        "    Models:\n",
        "    - Agent 1: OpenAI \"gpt-5.1-high\"\n",
        "    - Agent 2: Anthropic \"claude-opus-4-5-20251101\"\n",
        "    - Agent 3: Anthropic \"claude-sonnet-4-5-20250929\"\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, LLMInterface]: Mapping of agent ID to initialized interface.\n",
        "    \"\"\"\n",
        "    agents = {}\n",
        "\n",
        "    # Define configurations\n",
        "    configs = [\n",
        "        (\"Agent_GPT5\", \"gpt-5.1-high\", GPT4oInterface), # Using GPT4oInterface for OpenAI protocol\n",
        "        (\"Agent_ClaudeOpus\", \"claude-opus-4-5-20251101\", ClaudeInterface),\n",
        "        (\"Agent_ClaudeSonnet\", \"claude-sonnet-4-5-20250929\", ClaudeInterface)\n",
        "    ]\n",
        "\n",
        "    for agent_id, model_name, interface_cls in configs:\n",
        "        try:\n",
        "            # Instantiate interface\n",
        "            # The interface classes defined in Task 14 take 'model' as an init argument.\n",
        "            agent = interface_cls(model=model_name)\n",
        "            agents[agent_id] = agent\n",
        "            logger.info(f\"Initialized {agent_id} with model {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize {agent_id}: {e}\")\n",
        "\n",
        "    return agents\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 33, Step 3: Perform Calibration\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_cohen_kappa(rater1_scores: List[int], rater2_scores: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Calculates Cohen's Kappa for inter-rater agreement between two raters.\n",
        "\n",
        "    Cohen's Kappa measures the agreement between two raters who each classify N items\n",
        "    into C mutually exclusive categories. It accounts for the agreement occurring by chance.\n",
        "\n",
        "    Formula:\n",
        "        kappa = (p_o - p_e) / (1 - p_e)\n",
        "    Where:\n",
        "        p_o = Relative observed agreement among raters.\n",
        "        p_e = Hypothetical probability of chance agreement.\n",
        "\n",
        "    Args:\n",
        "        rater1_scores (List[int]): List of scores from rater 1.\n",
        "        rater2_scores (List[int]): List of scores from rater 2.\n",
        "\n",
        "    Returns:\n",
        "        float: The Cohen's Kappa score. Returns 0.0 if lists are empty or invalid.\n",
        "    \"\"\"\n",
        "    if len(rater1_scores) != len(rater2_scores) or not rater1_scores:\n",
        "        return 0.0\n",
        "\n",
        "    # Confusion Matrix\n",
        "    # Categories are 1, 2, 3, 4, 5\n",
        "    categories = [1, 2, 3, 4, 5]\n",
        "    n = len(rater1_scores)\n",
        "    matrix = {c1: {c2: 0 for c2 in categories} for c1 in categories}\n",
        "\n",
        "    for s1, s2 in zip(rater1_scores, rater2_scores):\n",
        "        if s1 in categories and s2 in categories:\n",
        "            matrix[s1][s2] += 1\n",
        "\n",
        "    # Observed Agreement (p_o)\n",
        "    # Sum of diagonal elements / Total items\n",
        "    agreement_count = sum(matrix[c][c] for c in categories)\n",
        "    p_o = agreement_count / n\n",
        "\n",
        "    # Expected Agreement (p_e)\n",
        "    # Sum of (prob r1 chooses c * prob r2 chooses c) for all c\n",
        "    p_e = 0.0\n",
        "    for c in categories:\n",
        "        count_r1 = sum(matrix[c][k] for k in categories) # Row sum\n",
        "        count_r2 = sum(matrix[k][c] for k in categories) # Col sum\n",
        "        prob_r1 = count_r1 / n\n",
        "        prob_r2 = count_r2 / n\n",
        "        p_e += prob_r1 * prob_r2\n",
        "\n",
        "    if p_e == 1.0:\n",
        "        return 1.0 # Perfect agreement by chance (all same values)\n",
        "\n",
        "    kappa = (p_o - p_e) / (1 - p_e)\n",
        "    return kappa\n",
        "\n",
        "def run_calibration_scoring(\n",
        "    agents: Dict[str, LLMInterface],\n",
        "    calibration_pairs: List[Dict[str, str]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs the calibration phase where all agents score the same set of pairs.\n",
        "\n",
        "    This function iterates through the calibration dataset, queries each proxy agent,\n",
        "    and computes the inter-annotator agreement using Cohen's Kappa. High agreement\n",
        "    indicates that the agents (and the prompt instructions) are aligned on the\n",
        "    definition of semantic equivalence.\n",
        "\n",
        "    Args:\n",
        "        agents (Dict[str, LLMInterface]): Initialized LLM agents.\n",
        "        calibration_pairs (List[Dict[str, str]]): List of dicts with 'original' and 'compressed' text.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Calibration results including raw scores and the average Kappa agreement metric.\n",
        "    \"\"\"\n",
        "    template = get_annotation_prompt_template()\n",
        "    results = []\n",
        "\n",
        "    logger.info(f\"Starting calibration scoring for {len(calibration_pairs)} pairs...\")\n",
        "\n",
        "    for i, pair in enumerate(calibration_pairs):\n",
        "        orig = pair[\"original\"]\n",
        "        comp = pair[\"compressed\"]\n",
        "        pair_id = pair.get(\"id\", f\"calib_{i}\")\n",
        "\n",
        "        # Use neutral labels as per the remedied template\n",
        "        prompt_text = template.format(text_a=orig, text_b=comp)\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "\n",
        "        pair_results = {\"pair_id\": pair_id, \"scores\": {}}\n",
        "\n",
        "        for agent_id, agent in agents.items():\n",
        "            try:\n",
        "                # Call LLM with temperature 0.0 for consistent evaluation\n",
        "                response = agent.prompt(messages=messages, temperature=0.0)\n",
        "                text_response, _ = agent.extract_response(response)\n",
        "\n",
        "                # Robust JSON Parsing\n",
        "                try:\n",
        "                    start = text_response.find(\"{\")\n",
        "                    end = text_response.rfind(\"}\") + 1\n",
        "                    if start != -1 and end != -1:\n",
        "                        json_str = text_response[start:end]\n",
        "                        data = json.loads(json_str)\n",
        "                        score = int(data[\"score\"])\n",
        "                        # Clamp score to valid range 1-5\n",
        "                        score = max(1, min(5, score))\n",
        "                        pair_results[\"scores\"][agent_id] = score\n",
        "                    else:\n",
        "                        logger.warning(f\"No JSON found in response from {agent_id} for {pair_id}\")\n",
        "                        pair_results[\"scores\"][agent_id] = None\n",
        "                except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
        "                    logger.warning(f\"Failed to parse response from {agent_id} for {pair_id}: {e}\")\n",
        "                    pair_results[\"scores\"][agent_id] = None\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Agent {agent_id} failed on {pair_id}: {e}\")\n",
        "                pair_results[\"scores\"][agent_id] = None\n",
        "\n",
        "        results.append(pair_results)\n",
        "\n",
        "    # Compute Agreement (Average Pairwise Cohen's Kappa)\n",
        "    agent_ids = list(agents.keys())\n",
        "    vectors = {aid: [] for aid in agent_ids}\n",
        "\n",
        "    # Collect valid scores aligned by index\n",
        "    valid_indices = []\n",
        "    for idx, res in enumerate(results):\n",
        "        scores = res[\"scores\"]\n",
        "        # Only include pairs where ALL agents provided a score\n",
        "        if all(scores.get(aid) is not None for aid in agent_ids):\n",
        "            for aid in agent_ids:\n",
        "                vectors[aid].append(scores[aid])\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    kappa_scores = []\n",
        "    if len(valid_indices) > 1:\n",
        "        for i in range(len(agent_ids)):\n",
        "            for j in range(i + 1, len(agent_ids)):\n",
        "                a1 = agent_ids[i]\n",
        "                a2 = agent_ids[j]\n",
        "                v1 = vectors[a1]\n",
        "                v2 = vectors[a2]\n",
        "\n",
        "                kappa = calculate_cohen_kappa(v1, v2)\n",
        "                kappa_scores.append(kappa)\n",
        "\n",
        "    avg_kappa = sum(kappa_scores) / len(kappa_scores) if kappa_scores else 0.0\n",
        "\n",
        "    logger.info(f\"Calibration complete. Average Cohen's Kappa: {avg_kappa:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"raw_results\": results,\n",
        "        \"agreement_metric\": float(avg_kappa),\n",
        "        \"num_valid_pairs\": len(valid_indices)\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 33, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def run_llm_calibration_task(\n",
        "    tatqa_pairs: List[Dict[str, str]],\n",
        "    finqa_pairs: List[Dict[str, str]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the LLM-based annotator recruitment and calibration.\n",
        "\n",
        "    1. Initializes proxy agents.\n",
        "    2. Selects calibration sample (15 from each dataset).\n",
        "    3. Runs scoring.\n",
        "    4. Reports agreement.\n",
        "\n",
        "    Args:\n",
        "        tatqa_pairs (List[Dict]): Available TAT-QA pairs (original, compressed).\n",
        "        finqa_pairs (List[Dict]): Available Fin-QA pairs.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Calibration report.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting LLM proxy calibration...\")\n",
        "\n",
        "    # Step 1: Initialize\n",
        "    agents = initialize_proxy_annotators()\n",
        "\n",
        "    # Step 2: Sample\n",
        "    # We need 15 from each.\n",
        "    sample_tatqa = random.sample(tatqa_pairs, min(15, len(tatqa_pairs)))\n",
        "    sample_finqa = random.sample(finqa_pairs, min(15, len(finqa_pairs)))\n",
        "    calibration_set = sample_tatqa + sample_finqa\n",
        "    random.shuffle(calibration_set)\n",
        "\n",
        "    logger.info(f\"Calibration set size: {len(calibration_set)}\")\n",
        "\n",
        "    # Step 3: Run\n",
        "    calibration_results = run_calibration_scoring(agents, calibration_set)\n",
        "\n",
        "    logger.info(f\"Calibration complete. Agreement: {calibration_results['agreement_metric']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"agents\": list(agents.keys()),\n",
        "        \"results\": calibration_results\n",
        "    }\n"
      ],
      "metadata": {
        "id": "g6L-JGeB9zdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 34 – Conduct Main Human Evaluation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 34: Conduct Main Human Evaluation (LLM Proxies)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 34, Step 1: Sample evaluation pairs\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def sample_evaluation_pairs(\n",
        "    tatqa_pairs: List[Dict[str, str]],\n",
        "    finqa_pairs: List[Dict[str, str]],\n",
        "    total_samples: int = 90\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Samples pairs for the main evaluation from TAT-QA and Fin-QA datasets.\n",
        "\n",
        "    This function attempts to sample an equal number of pairs from each dataset.\n",
        "    If a dataset has fewer pairs than required, it takes all available pairs\n",
        "    and fills the remainder from the other dataset if possible.\n",
        "\n",
        "    Args:\n",
        "        tatqa_pairs (List[Dict]): List of TAT-QA pairs (dicts with 'original', 'compressed').\n",
        "        finqa_pairs (List[Dict]): List of Fin-QA pairs (dicts with 'original', 'compressed').\n",
        "        total_samples (int): Total number of pairs to sample (default 90).\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of sampled pairs with metadata (id, dataset).\n",
        "    \"\"\"\n",
        "    n_tatqa = total_samples // 2\n",
        "    n_finqa = total_samples - n_tatqa\n",
        "\n",
        "    # Adjust if one dataset is too small\n",
        "    if len(tatqa_pairs) < n_tatqa:\n",
        "        n_tatqa = len(tatqa_pairs)\n",
        "        n_finqa = min(len(finqa_pairs), total_samples - n_tatqa)\n",
        "    elif len(finqa_pairs) < n_finqa:\n",
        "        n_finqa = len(finqa_pairs)\n",
        "        n_tatqa = min(len(tatqa_pairs), total_samples - n_finqa)\n",
        "\n",
        "    sample_t = random.sample(tatqa_pairs, n_tatqa)\n",
        "    sample_f = random.sample(finqa_pairs, n_finqa)\n",
        "\n",
        "    # Add metadata\n",
        "    for p in sample_t:\n",
        "        p[\"dataset\"] = \"TAT-QA\"\n",
        "    for p in sample_f:\n",
        "        p[\"dataset\"] = \"Fin-QA\"\n",
        "\n",
        "    combined = sample_t + sample_f\n",
        "    random.shuffle(combined)\n",
        "\n",
        "    # Add unique pair IDs\n",
        "    for i, pair in enumerate(combined):\n",
        "        pair[\"id\"] = f\"eval_{i:03d}\"\n",
        "\n",
        "    logger.info(f\"Sampled {len(combined)} pairs for evaluation ({len(sample_t)} TAT-QA, {len(sample_f)} Fin-QA).\")\n",
        "    return combined\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 34, Step 2: Collect ratings\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def collect_proxy_ratings(\n",
        "    agents: Dict[str, LLMInterface],\n",
        "    pairs: List[Dict[str, Any]]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Collects semantic equivalence ratings from all proxy agents for the given evaluation pairs.\n",
        "\n",
        "    This function implements a randomized evaluation protocol to mitigate position bias.\n",
        "    For each pair, the order of presentation (\"Text A\" vs \"Text B\") is randomized.\n",
        "    The function queries each agent, parses the JSON response, and records the score,\n",
        "    reasoning, and the specific mapping used for that instance.\n",
        "\n",
        "    Args:\n",
        "        agents (Dict[str, LLMInterface]): Dictionary of initialized proxy agents.\n",
        "        pairs (List[Dict[str, Any]]): List of evaluation pairs containing 'id', 'original', 'compressed'.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of result dictionaries. Each dictionary contains:\n",
        "            - pair_id, dataset, example_id\n",
        "            - mapping: Which text was presented as Text A (\"original\" or \"compressed\")\n",
        "            - scores: Dict mapping agent_id to integer score (1-5)\n",
        "            - reasoning: Dict mapping agent_id to reasoning string\n",
        "    \"\"\"\n",
        "    template = get_annotation_prompt_template()\n",
        "    results = []\n",
        "\n",
        "    logger.info(f\"Starting data collection for {len(pairs)} pairs...\")\n",
        "\n",
        "    for i, pair in enumerate(pairs):\n",
        "        pair_id = pair[\"id\"]\n",
        "        orig_text = pair[\"original\"]\n",
        "        comp_text = pair[\"compressed\"]\n",
        "\n",
        "        # Randomize presentation order to prevent position bias\n",
        "        # swap = True means Text A is Compressed, Text B is Original\n",
        "        swap = random.choice([True, False])\n",
        "\n",
        "        if swap:\n",
        "            text_a = comp_text\n",
        "            text_b = orig_text\n",
        "            mapping = \"compressed_first\"\n",
        "        else:\n",
        "            text_a = orig_text\n",
        "            text_b = comp_text\n",
        "            mapping = \"original_first\"\n",
        "\n",
        "        # Construct prompt with neutral labels\n",
        "        prompt_text = template.format(text_a=text_a, text_b=text_b)\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "\n",
        "        pair_result = {\n",
        "            \"pair_id\": pair_id,\n",
        "            \"dataset\": pair.get(\"dataset\", \"unknown\"),\n",
        "            \"example_id\": pair.get(\"example_id\", \"unknown\"),\n",
        "            \"mapping\": mapping,\n",
        "            \"scores\": {},\n",
        "            \"reasoning\": {}\n",
        "        }\n",
        "\n",
        "        for agent_id, agent in agents.items():\n",
        "            try:\n",
        "                # Call LLM with temperature 0.0 for deterministic evaluation\n",
        "                response = agent.prompt(messages=messages, temperature=0.0)\n",
        "                text_response, _ = agent.extract_response(response)\n",
        "\n",
        "                # Robust JSON Parsing\n",
        "                try:\n",
        "                    # Find the JSON object within the response\n",
        "                    start = text_response.find(\"{\")\n",
        "                    end = text_response.rfind(\"}\") + 1\n",
        "                    if start != -1 and end != -1:\n",
        "                        json_str = text_response[start:end]\n",
        "                        data = json.loads(json_str)\n",
        "\n",
        "                        score = int(data[\"score\"])\n",
        "                        # Clamp score to valid range 1-5\n",
        "                        score = max(1, min(5, score))\n",
        "\n",
        "                        pair_result[\"scores\"][agent_id] = score\n",
        "                        pair_result[\"reasoning\"][agent_id] = data.get(\"reasoning\", \"\")\n",
        "                    else:\n",
        "                        logger.warning(f\"No JSON found in response from {agent_id} for {pair_id}\")\n",
        "                        pair_result[\"scores\"][agent_id] = None\n",
        "                        pair_result[\"reasoning\"][agent_id] = \"Parse Error: No JSON found\"\n",
        "\n",
        "                except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
        "                    logger.warning(f\"Agent {agent_id} failed JSON parse on {pair_id}: {e}\")\n",
        "                    pair_result[\"scores\"][agent_id] = None\n",
        "                    pair_result[\"reasoning\"][agent_id] = f\"Parse Error: {str(e)}\"\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Agent {agent_id} API error on {pair_id}: {e}\")\n",
        "                pair_result[\"scores\"][agent_id] = None\n",
        "                pair_result[\"reasoning\"][agent_id] = f\"API Error: {str(e)}\"\n",
        "\n",
        "        results.append(pair_result)\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            logger.info(f\"Collected ratings for {i + 1}/{len(pairs)} pairs.\")\n",
        "\n",
        "    logger.info(\"Data collection complete.\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 34, Step 3: Store raw ratings (Orchestrator)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def run_main_evaluation_task(\n",
        "    tatqa_pairs: List[Dict[str, str]],\n",
        "    finqa_pairs: List[Dict[str, str]],\n",
        "    agents: Dict[str, LLMInterface],\n",
        "    output_dir: str = \"human_eval_results\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the main human evaluation task using LLM proxies.\n",
        "\n",
        "    1. Samples evaluation pairs from both datasets.\n",
        "    2. Collects ratings from all proxy agents.\n",
        "    3. Persists the raw ratings to disk.\n",
        "\n",
        "    Args:\n",
        "        tatqa_pairs (List[Dict]): List of TAT-QA pairs.\n",
        "        finqa_pairs (List[Dict]): List of Fin-QA pairs.\n",
        "        agents (Dict): Dictionary of initialized proxy agents.\n",
        "        output_dir (str): Directory to save results.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: The list of raw rating results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting main evaluation...\")\n",
        "\n",
        "    # Step 1: Sample\n",
        "    eval_pairs = sample_evaluation_pairs(tatqa_pairs, finqa_pairs)\n",
        "\n",
        "    # Step 2: Collect\n",
        "    ratings = collect_proxy_ratings(agents, eval_pairs)\n",
        "\n",
        "    # Step 3: Save\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"raw_ratings.json\")\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(ratings, f, indent=2)\n",
        "\n",
        "    logger.info(f\"Evaluation complete. Ratings saved to {output_path}\")\n",
        "    return ratings\n"
      ],
      "metadata": {
        "id": "6nlc02-EblRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 35 – Aggregate Human Ratings and Compare to Embeddings\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 35: Aggregate Human Ratings and Compare to Embeddings\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 35, Step 1 & 2: Aggregate Ratings\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def aggregate_ratings(\n",
        "    raw_ratings: List[Dict[str, Any]]\n",
        ") -> Tuple[Dict[str, float], float]:\n",
        "    \"\"\"\n",
        "    Aggregates raw ratings per pair and computes the global mean.\n",
        "\n",
        "    Args:\n",
        "        raw_ratings (List[Dict]): List of rating records from Task 34.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, float], float]:\n",
        "            - Mapping pair_id -> mean_rating.\n",
        "            - Global mean rating.\n",
        "    \"\"\"\n",
        "    pair_means = {}\n",
        "    all_scores = []\n",
        "\n",
        "    for record in raw_ratings:\n",
        "        pair_id = record[\"pair_id\"]\n",
        "        scores = [s for s in record[\"scores\"].values() if s is not None]\n",
        "\n",
        "        if scores:\n",
        "            mean_score = float(np.mean(scores))\n",
        "            pair_means[pair_id] = mean_score\n",
        "            all_scores.extend(scores)\n",
        "        else:\n",
        "            logger.warning(f\"No valid scores for pair {pair_id}\")\n",
        "\n",
        "    global_mean = float(np.mean(all_scores)) if all_scores else 0.0\n",
        "\n",
        "    logger.info(f\"Aggregated ratings for {len(pair_means)} pairs. Global Mean: {global_mean:.4f}\")\n",
        "    return pair_means, global_mean\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 35, Step 3: Compare to Cosine Similarity\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compare_ratings_to_similarity(\n",
        "    raw_ratings: List[Dict[str, Any]],\n",
        "    pair_means: Dict[str, float],\n",
        "    similarity_scores: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compares human ratings to embedding cosine similarities.\n",
        "\n",
        "    Identifies mismatches:\n",
        "    - Case A: High Similarity (>= 0.92) but Low Rating (< 3).\n",
        "    - Case B: Low Similarity (< 0.92) but High Rating (>= 4).\n",
        "\n",
        "    Args:\n",
        "        raw_ratings (List[Dict]): To map pair_id to example_id.\n",
        "        pair_means (Dict): Mean human rating per pair.\n",
        "        similarity_scores (Dict): Cosine similarity per example_id.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Analysis results including mismatch counts and lists.\n",
        "    \"\"\"\n",
        "    mismatches_A = [] # High Sim, Low Rating\n",
        "    mismatches_B = [] # Low Sim, High Rating\n",
        "\n",
        "    # Map pair_id to example_id\n",
        "    pair_to_example = {r[\"pair_id\"]: r.get(\"example_id\") for r in raw_ratings}\n",
        "\n",
        "    for pair_id, rating in pair_means.items():\n",
        "        example_id = pair_to_example.get(pair_id)\n",
        "\n",
        "        if not example_id or example_id not in similarity_scores:\n",
        "            continue\n",
        "\n",
        "        sim = similarity_scores[example_id]\n",
        "\n",
        "        # Case A\n",
        "        if sim >= 0.92 and rating < 3.0:\n",
        "            mismatches_A.append({\n",
        "                \"pair_id\": pair_id,\n",
        "                \"example_id\": example_id,\n",
        "                \"similarity\": sim,\n",
        "                \"rating\": rating\n",
        "            })\n",
        "\n",
        "        # Case B\n",
        "        if sim < 0.92 and rating >= 4.0:\n",
        "            mismatches_B.append({\n",
        "                \"pair_id\": pair_id,\n",
        "                \"example_id\": example_id,\n",
        "                \"similarity\": sim,\n",
        "                \"rating\": rating\n",
        "            })\n",
        "\n",
        "    total_pairs = len(pair_means)\n",
        "    frac_A = len(mismatches_A) / total_pairs if total_pairs > 0 else 0.0\n",
        "    frac_B = len(mismatches_B) / total_pairs if total_pairs > 0 else 0.0\n",
        "\n",
        "    logger.info(f\"Mismatch Case A (High Sim, Low Rating): {len(mismatches_A)} ({frac_A:.2%})\")\n",
        "    logger.info(f\"Mismatch Case B (Low Sim, High Rating): {len(mismatches_B)} ({frac_B:.2%})\")\n",
        "\n",
        "    return {\n",
        "        \"mismatches_A\": mismatches_A,\n",
        "        \"mismatches_B\": mismatches_B,\n",
        "        \"fraction_A\": frac_A,\n",
        "        \"fraction_B\": frac_B\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 35, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def analyze_human_vs_embedding_task(\n",
        "    raw_ratings: List[Dict[str, Any]],\n",
        "    similarity_results: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the comparison between human ratings and embedding similarities.\n",
        "\n",
        "    Args:\n",
        "        raw_ratings (List[Dict]): Output from Task 34.\n",
        "        similarity_results (Dict): Output from Task 32 (contains 'per_example_scores').\n",
        "\n",
        "    Returns:\n",
        "        Dict: Comprehensive analysis report.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting human vs embedding analysis...\")\n",
        "\n",
        "    # Step 1 & 2\n",
        "    pair_means, global_mean = aggregate_ratings(raw_ratings)\n",
        "\n",
        "    # Step 3\n",
        "    sim_scores = similarity_results.get(\"per_example_scores\", {})\n",
        "    comparison = compare_ratings_to_similarity(raw_ratings, pair_means, sim_scores)\n",
        "\n",
        "    return {\n",
        "        \"global_mean_rating\": global_mean,\n",
        "        \"pair_means\": pair_means,\n",
        "        \"comparison_analysis\": comparison\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ryu2E0xlbpWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 36 – Design Orchestrator Function Responsibilities\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 36: Design Orchestrator Function Responsibilities\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 36, Step 1: Define Pipeline Stages\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "class PipelineStage(Enum):\n",
        "    \"\"\"\n",
        "    Enumeration of distinct stages in the CompactPrompt pipeline.\n",
        "\n",
        "    This enum defines the sequential processing steps required to transform raw input\n",
        "    into a compressed prompt and obtain model predictions. It serves as a control\n",
        "    structure for the orchestrator to manage execution flow and logging.\n",
        "\n",
        "    Attributes:\n",
        "        INPUT_PROCESSING: Retrieval and validation of raw example data.\n",
        "        SERIALIZATION: Conversion of structured data (tables) to string format.\n",
        "        DYNAMIC_SCORING: Computation of token-level importance via scorer LLM.\n",
        "        HARD_PRUNING: Removal of low-importance phrases based on combined scores.\n",
        "        DATA_COMPRESSION: Application of n-gram abbreviation and numeric quantization.\n",
        "        EXEMPLAR_SELECTION: Retrieval of few-shot examples (random or representative).\n",
        "        PROMPT_ASSEMBLY: Construction of the final prompt string.\n",
        "        INFERENCE: Querying the target LLM for the final answer.\n",
        "        METRICS_COMPUTATION: Calculation of compression ratios and accuracy.\n",
        "    \"\"\"\n",
        "    INPUT_PROCESSING = \"input_processing\"\n",
        "    SERIALIZATION = \"serialization\"\n",
        "    DYNAMIC_SCORING = \"dynamic_scoring\"\n",
        "    HARD_PRUNING = \"hard_pruning\"\n",
        "    DATA_COMPRESSION = \"data_compression\"\n",
        "    EXEMPLAR_SELECTION = \"exemplar_selection\"\n",
        "    PROMPT_ASSEMBLY = \"prompt_assembly\"\n",
        "    INFERENCE = \"inference\"\n",
        "    METRICS_COMPUTATION = \"metrics_computation\"\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 36, Step 2 & 3: Define Configuration and Condition Logic\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    \"\"\"\n",
        "    Configuration object for a single execution of the CompactPrompt pipeline.\n",
        "\n",
        "    This class parses the experimental condition string (e.g., \"compressed_plus_data\")\n",
        "    and sets boolean flags to control which pipeline stages are executed. It encapsulates\n",
        "    all hyperparameters required for compression algorithms, ensuring consistent\n",
        "    configuration across the pipeline.\n",
        "\n",
        "    Attributes:\n",
        "        condition (str): The experimental condition identifier. Must be one of the\n",
        "            predefined conditions (e.g., \"baseline\", \"compressed_prompt\").\n",
        "        target_llm (str): Identifier for the target LLM used for inference.\n",
        "        scorer_llm (str): Identifier for the scorer LLM used for dynamic self-information.\n",
        "        ngram_params (Dict[str, int]): Parameters for n-gram abbreviation.\n",
        "            Expected keys: 'top_n_T' (int), 'ngram_size_G' (int).\n",
        "            Defaults to {'top_n_T': 3, 'ngram_size_G': 2}.\n",
        "        quantization_params (Dict[str, Any]): Parameters for numeric quantization.\n",
        "            Expected keys: 'bit_width_b' (int), 'num_clusters_k' (int).\n",
        "            Defaults to empty dict (uses global defaults).\n",
        "\n",
        "        # Derived Flags (set in __post_init__)\n",
        "        use_hard_pruning (bool): Whether to apply phrase-level pruning (Task 21).\n",
        "        use_abbreviation (bool): Whether to apply n-gram abbreviation (Task 24).\n",
        "        use_quantization (bool): Whether to apply numeric quantization (Task 26/27).\n",
        "        exemplar_mode (str): Strategy for few-shot examples (\"none\", \"random\", \"representative\").\n",
        "        add_dict_context (bool): Whether to append the abbreviation dictionary to the prompt.\n",
        "    \"\"\"\n",
        "    condition: str\n",
        "    target_llm: str\n",
        "    scorer_llm: str\n",
        "    ngram_params: Dict[str, int] = field(default_factory=lambda: {\"top_n_T\": 3, \"ngram_size_G\": 2})\n",
        "    quantization_params: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    # Derived flags (initialized in post_init based on condition string)\n",
        "    use_hard_pruning: bool = field(init=False)\n",
        "    use_abbreviation: bool = field(init=False)\n",
        "    use_quantization: bool = field(init=False)\n",
        "    exemplar_mode: str = field(init=False)\n",
        "    add_dict_context: bool = field(init=False)\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Parses the condition string to set control flags for the pipeline.\n",
        "\n",
        "        This method interprets the semantic components of the condition string\n",
        "        to enable or disable specific compression modules.\n",
        "        \"\"\"\n",
        "        c = self.condition\n",
        "\n",
        "        # Hard Pruning: Enabled if \"compressed\" is in the condition name\n",
        "        # (e.g., \"compressed_prompt\", \"compressed_plus_data\")\n",
        "        self.use_hard_pruning = \"compressed\" in c\n",
        "\n",
        "        # Data Compression: Enabled if \"plus_data\" is present\n",
        "        # This implies both n-gram abbreviation and numeric quantization\n",
        "        self.use_abbreviation = \"plus_data\" in c\n",
        "        self.use_quantization = \"plus_data\" in c\n",
        "\n",
        "        # Added Context: Enabled if \"added_context\" is present\n",
        "        # Appends the abbreviation dictionary to the prompt context\n",
        "        self.add_dict_context = \"added_context\" in c\n",
        "\n",
        "        # Exemplar Mode: Determine few-shot strategy\n",
        "        if \"plus_3_random\" in c:\n",
        "            self.exemplar_mode = \"random\"\n",
        "        elif \"plus_3_representative\" in c:\n",
        "            self.exemplar_mode = \"representative\"\n",
        "        else:\n",
        "            self.exemplar_mode = \"none\"\n",
        "\n",
        "        logger.debug(f\"Pipeline Config Initialized: {self}\")\n"
      ],
      "metadata": {
        "id": "d4d8Gm2LsgSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 37 – Define Orchestrator Inputs and Outputs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 37, Step 1: Specify Input Parameters\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class OrchestratorInput:\n",
        "    \"\"\"\n",
        "    Encapsulates all input parameters required for a single execution of the\n",
        "    CompactPrompt orchestrator.\n",
        "\n",
        "    This dataclass serves as the primary data transfer object (DTO) for passing\n",
        "    configuration and context into the orchestrator function. It ensures type safety\n",
        "    and provides default values for optional compression parameters.\n",
        "\n",
        "    Attributes:\n",
        "        example_id (str): Unique identifier for the example being processed.\n",
        "        dataset (str): Name of the dataset (\"TAT-QA\" or \"Fin-QA\").\n",
        "        condition (str): Experimental condition identifier (e.g., \"compressed_plus_data\").\n",
        "        target_llm (str): Identifier for the target LLM used for inference.\n",
        "        scorer_llm (str): Identifier for the scorer LLM used for dynamic self-information.\n",
        "        ngram_params (Dict[str, int]): Parameters for n-gram abbreviation.\n",
        "            Expected keys: 'top_n_T' (int), 'ngram_size_G' (int).\n",
        "            Defaults to {'top_n_T': 3, 'ngram_size_G': 2}.\n",
        "        quantization_params (Dict[str, Any]): Parameters for numeric quantization.\n",
        "            Expected keys: 'bit_width_b' (int), 'num_clusters_k' (int).\n",
        "            Defaults to empty dict (uses global defaults).\n",
        "    \"\"\"\n",
        "    example_id: str\n",
        "    dataset: str\n",
        "    condition: str\n",
        "    target_llm: str\n",
        "    scorer_llm: str\n",
        "    ngram_params: Dict[str, int] = field(default_factory=lambda: {\"top_n_T\": 3, \"ngram_size_G\": 2})\n",
        "    quantization_params: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 37, Step 2: Specify Output Artifacts\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class OrchestratorOutput:\n",
        "    \"\"\"\n",
        "    Encapsulates the results and artifacts produced by the orchestrator.\n",
        "\n",
        "    This dataclass standardizes the output format of the pipeline, ensuring that\n",
        "    all necessary data for downstream analysis (accuracy computation, token usage\n",
        "    tracking, debugging) is captured and easily accessible.\n",
        "\n",
        "    Attributes:\n",
        "        final_prompt (str): The exact text string sent to the target LLM for inference.\n",
        "        prediction (str): The raw text generated by the target LLM.\n",
        "        compression_metadata (Dict[str, Any]): Detailed metadata about the compression process.\n",
        "            Includes:\n",
        "            - 'retained_phrases': List of phrases kept after pruning.\n",
        "            - 'active_ngrams': Set of n-grams replaced by abbreviations.\n",
        "            - 'quantization_ranges': Min/max values used for quantization.\n",
        "        metrics (Dict[str, Any]): Quantitative metrics for performance analysis.\n",
        "            Includes:\n",
        "            - 'original_token_count': Token count before compression.\n",
        "            - 'compressed_token_count': Token count after compression.\n",
        "            - 'compression_ratio': Ratio of original to compressed tokens.\n",
        "    \"\"\"\n",
        "    final_prompt: str\n",
        "    prediction: str\n",
        "    compression_metadata: Dict[str, Any]\n",
        "    metrics: Dict[str, Any]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 37, Step 3: Define Logging Requirements\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "@dataclass\n",
        "class OrchestratorLog:\n",
        "    \"\"\"\n",
        "    Structured log record for a single orchestrator execution.\n",
        "\n",
        "    This dataclass defines the schema for logging pipeline execution details.\n",
        "    It is designed to be serialized (e.g., to JSON) for auditing, debugging,\n",
        "    and performance monitoring.\n",
        "\n",
        "    Attributes:\n",
        "        timestamp (str): ISO 8601 timestamp of when the execution occurred.\n",
        "        input_params (Dict[str, Any]): A copy of the input parameters provided to the orchestrator.\n",
        "        metrics (Dict[str, Any]): A copy of the output metrics generated by the pipeline.\n",
        "        latency_ms (float): Total execution time in milliseconds.\n",
        "        status (str): Execution status, either \"success\" or \"error\".\n",
        "        error_message (Optional[str]): Detailed error message if status is \"error\", else None.\n",
        "    \"\"\"\n",
        "    timestamp: str\n",
        "    input_params: Dict[str, Any]\n",
        "    metrics: Dict[str, Any]\n",
        "    latency_ms: float\n",
        "    status: str\n",
        "    error_message: Optional[str] = None\n"
      ],
      "metadata": {
        "id": "QmcsWNuKumKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# END-TO-END ORCHESTRATOR FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def run_compactprompt_pipeline(\n",
        "    tatqa_raw_df: pd.DataFrame,\n",
        "    finqa_raw_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    condition: str = \"compressed_plus_data\",\n",
        "    target_llm: str = \"claude-3.5-sonnet\",\n",
        "    scorer_llm: str = \"gpt-4o\",\n",
        "    ngram_params: Optional[Dict[str, int]] = None,\n",
        "    quantization_params: Optional[Dict[str, Any]] = None,\n",
        "    output_dir: str = \"compactprompt_outputs\",\n",
        "    skip_corpus_build: bool = False,\n",
        "    skip_human_eval: bool = False,\n",
        "    random_seed: int = 42\n",
        ") -> Tuple[OrchestratorOutput, OrchestratorLog]:\n",
        "    \"\"\"\n",
        "    Execute the complete CompactPrompt research pipeline end-to-end.\n",
        "\n",
        "    This function orchestrates all 35 task-specific functions to implement\n",
        "    the full compression and evaluation workflow from the CompactPrompt paper:\n",
        "    \"A Unified Pipeline for Prompt and Data Compression in LLM Workflows\".\n",
        "\n",
        "    The pipeline consists of 12 phases:\n",
        "        Phase 1 (Tasks 1-3): Input validation and configuration resolution\n",
        "        Phase 2 (Tasks 4-6): Data cleansing and normalization\n",
        "        Phase 3 (Tasks 7-10): Offline corpus construction and static self-information\n",
        "        Phase 4 (Tasks 11-13): Prompt serialization and template construction\n",
        "        Phase 5 (Task 14): LLM resource configuration\n",
        "        Phase 6 (Tasks 15-18): Dynamic scoring and combined score computation\n",
        "        Phase 7 (Tasks 19-21): Phrase-level grouping and budget-constrained pruning\n",
        "        Phase 8 (Tasks 22-24): N-gram abbreviation for textual data compression\n",
        "        Phase 9 (Tasks 25-27): Numeric quantization for table data compression\n",
        "        Phase 10 (Tasks 28-30): Embedding and representative exemplar selection\n",
        "        Phase 11 (Tasks 31-32): Semantic similarity evaluation\n",
        "        Phase 12 (Tasks 33-35): Human evaluation protocol\n",
        "\n",
        "    Args:\n",
        "        tatqa_raw_df: Raw TAT-QA DataFrame with columns: example_id, split,\n",
        "            question_text, tables, passages, answer_type, answer_value, answer_unit.\n",
        "        finqa_raw_df: Raw Fin-QA DataFrame with identical column schema.\n",
        "        study_config: Nested configuration dictionary containing all parameters\n",
        "            for corpus construction, compression, quantization, and evaluation.\n",
        "        condition: Experimental condition from experiment_design_config.prompting_conditions.\n",
        "            One of: \"baseline\", \"baseline_plus_3_random\", \"baseline_plus_3_representative\",\n",
        "            \"compressed_prompt\", \"compressed_plus_3_random\", \"compressed_plus_3_representative\",\n",
        "            \"compressed_plus_data\", \"compressed_plus_data_plus_added_context\".\n",
        "        target_llm: Model identifier for downstream QA generation. One of:\n",
        "            \"gpt-4o\", \"gpt-4.1-mini\", \"claude-3.5-sonnet\", \"llama-3.3-70b-instruct\".\n",
        "        scorer_llm: Model identifier for dynamic self-information scoring.\n",
        "            Must support log-probability access.\n",
        "        ngram_params: Dictionary with keys \"top_n_T\" (int) and \"ngram_size_G\" (int).\n",
        "            Defaults to {\"top_n_T\": 3, \"ngram_size_G\": 2} per paper's best configuration.\n",
        "        quantization_params: Dictionary with key \"bit_width\" (int) for uniform\n",
        "            quantization or \"num_clusters\" (int) for k-means. Defaults to {\"bit_width\": 8}.\n",
        "        output_dir: Base directory for all output artifacts including corpus,\n",
        "            embeddings, logs, and evaluation results.\n",
        "        skip_corpus_build: If True, attempt to load existing corpus statistics\n",
        "            from output_dir/corpus_stats instead of rebuilding. Useful for iteration.\n",
        "        skip_human_eval: If True, skip Tasks 33-35 (human evaluation protocol).\n",
        "            Useful for rapid experimentation without full evaluation.\n",
        "        random_seed: Seed for reproducibility of random exemplar selection.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            - OrchestratorOutput: All processed DataFrames, compression artifacts,\n",
        "              evaluation results, and aggregate metrics.\n",
        "            - OrchestratorLog: Complete audit trail with timing, status, and hashes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If input DataFrames fail critical validation checks.\n",
        "        RuntimeError: If LLM API calls fail after retries.\n",
        "        FileNotFoundError: If skip_corpus_build=True but corpus files don't exist.\n",
        "\n",
        "    Example:\n",
        "        >>> output, log = run_compactprompt_pipeline(\n",
        "        ...     tatqa_raw_df=tatqa_df,\n",
        "        ...     finqa_raw_df=finqa_df,\n",
        "        ...     study_config=config,\n",
        "        ...     condition=\"compressed_plus_data\",\n",
        "        ...     target_llm=\"claude-3.5-sonnet\",\n",
        "        ...     scorer_llm=\"gpt-4o\"\n",
        "        ... )\n",
        "        >>> print(f\"Mean compression ratio: {output.metrics['mean_compression_ratio']:.2f}x\")\n",
        "        >>> print(f\"Mean cosine similarity: {output.metrics['mean_cosine_similarity']:.3f}\")\n",
        "\n",
        "    References:\n",
        "        Choi et al. (2025). \"CompactPrompt: A Unified Pipeline for Prompt and\n",
        "        Data Compression in LLM Workflows.\" ACM ICAIF Workshop on LLMs and\n",
        "        Generative AI for Finance.\n",
        "    \"\"\"\n",
        "    # ==========================================================================\n",
        "    # INITIALIZATION\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Set random seed for reproducibility of random exemplar selection\n",
        "    random.seed(random_seed)\n",
        "\n",
        "    # Apply default values for optional parameters if not provided\n",
        "    if ngram_params is None:\n",
        "        ngram_params = {\"top_n_T\": 3, \"ngram_size_G\": 2}\n",
        "\n",
        "    # Apply default quantization parameters if not provided\n",
        "    if quantization_params is None:\n",
        "        quantization_params = {\"bit_width\": 8}\n",
        "\n",
        "    # Configure logging for pipeline execution tracking\n",
        "    logger = logging.getLogger(\"CompactPromptPipeline\")\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    # Record pipeline start timestamp in UTC for audit trail\n",
        "    start_timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
        "\n",
        "    # Construct OrchestratorInput dataclass for hashing and reference\n",
        "    orchestrator_input = OrchestratorInput(\n",
        "        tatqa_raw_df=tatqa_raw_df,\n",
        "        finqa_raw_df=finqa_raw_df,\n",
        "        study_config=study_config,\n",
        "        condition=condition,\n",
        "        target_llm=target_llm,\n",
        "        scorer_llm=scorer_llm,\n",
        "        ngram_params=ngram_params,\n",
        "        quantization_params=quantization_params,\n",
        "        output_dir=output_dir,\n",
        "        skip_corpus_build=skip_corpus_build,\n",
        "        skip_human_eval=skip_human_eval\n",
        "    )\n",
        "\n",
        "    # Compute SHA-256 hash of input specification for deduplication\n",
        "    input_repr = f\"{condition}|{target_llm}|{scorer_llm}|{ngram_params}|{quantization_params}\"\n",
        "    input_hash = hashlib.sha256(input_repr.encode()).hexdigest()[:16]\n",
        "\n",
        "    # Initialize orchestrator log with timestamp and input hash\n",
        "    orchestrator_log = OrchestratorLog(\n",
        "        timestamp=start_timestamp,\n",
        "        input_hash=input_hash,\n",
        "        config_snapshot={}\n",
        "    )\n",
        "\n",
        "    # Parse experimental condition to set pipeline control flags\n",
        "    pipeline_config = PipelineConfig(\n",
        "        condition=condition,\n",
        "        ngram_params=ngram_params,\n",
        "        quantization_scheme=\"uniform\" if \"bit_width\" in quantization_params else \"kmeans\"\n",
        "    )\n",
        "\n",
        "    # Log pipeline configuration for debugging\n",
        "    logger.info(f\"Pipeline condition: {condition}\")\n",
        "    logger.info(f\"Hard pruning: {pipeline_config.use_hard_pruning}\")\n",
        "    logger.info(f\"N-gram abbreviation: {pipeline_config.use_ngram_abbreviation}\")\n",
        "    logger.info(f\"Quantization: {pipeline_config.use_quantization}\")\n",
        "    logger.info(f\"Few-shot mode: {pipeline_config.exemplar_mode}\")\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 1: INPUT VALIDATION AND CONFIGURATION RESOLUTION (TASKS 1-3)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time for timing statistics\n",
        "    phase_1_start = time.perf_counter()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 1 Orchestrator: Validate TAT-QA DataFrame schema and content quality\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 1 orchestrator to validate TAT-QA schema and content\n",
        "    tatqa_validation_report: ValidationReport = validate_tatqa_dataset(\n",
        "        tatqa_raw_df=tatqa_raw_df\n",
        "    )\n",
        "\n",
        "    # Check if validation passed; if critical errors exist, pipeline must halt\n",
        "    if tatqa_validation_report.has_critical_errors:\n",
        "        # Record error in log before raising\n",
        "        orchestrator_log.errors.append(\n",
        "            f\"TAT-QA validation failed: {tatqa_validation_report.errors}\"\n",
        "        )\n",
        "        orchestrator_log.status = \"FAILED\"\n",
        "        # Raise exception to halt pipeline execution\n",
        "        raise ValueError(\n",
        "            f\"TAT-QA validation failed with {len(tatqa_validation_report.errors)} errors\"\n",
        "        )\n",
        "\n",
        "    # Log successful TAT-QA validation\n",
        "    logger.info(f\"TAT-QA validation passed: {len(tatqa_raw_df)} rows\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 2 Orchestrator: Validate Fin-QA DataFrame schema and content quality\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 2 orchestrator to validate Fin-QA schema and content\n",
        "    finqa_validation_report: ValidationReport = validate_finqa_dataset(\n",
        "        finqa_raw_df=finqa_raw_df\n",
        "    )\n",
        "\n",
        "    # Check if validation passed; halt pipeline on critical errors\n",
        "    if finqa_validation_report.has_critical_errors:\n",
        "        # Record error in log before raising\n",
        "        orchestrator_log.errors.append(\n",
        "            f\"Fin-QA validation failed: {finqa_validation_report.errors}\"\n",
        "        )\n",
        "        orchestrator_log.status = \"FAILED\"\n",
        "        # Raise exception to halt pipeline execution\n",
        "        raise ValueError(\n",
        "            f\"Fin-QA validation failed with {len(finqa_validation_report.errors)} errors\"\n",
        "        )\n",
        "\n",
        "    # Log successful Fin-QA validation\n",
        "    logger.info(f\"Fin-QA validation passed: {len(finqa_raw_df)} rows\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 3 Orchestrator: Resolve and validate study configuration\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 3 orchestrator to inventory placeholders and assign defaults\n",
        "    resolved_config: Dict[str, Any] = validate_and_update_study_config(\n",
        "        study_config=study_config\n",
        "    )\n",
        "\n",
        "    # Store resolved configuration snapshot in log for reproducibility\n",
        "    orchestrator_log.config_snapshot = resolved_config\n",
        "\n",
        "    # Log configuration resolution completion\n",
        "    logger.info(\"Study configuration validated and placeholders resolved\")\n",
        "\n",
        "    # Record Phase 1 elapsed time\n",
        "    phase_1_elapsed = time.perf_counter() - phase_1_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.VALIDATION] = phase_1_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 2: DATA CLEANSING AND NORMALIZATION (TASKS 4-6)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_2_start = time.perf_counter()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 4 Orchestrator: Cleanse and handle missing entries in TAT-QA\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 4 orchestrator to drop/repair malformed rows in TAT-QA\n",
        "    tatqa_clean_df, tatqa_cleansing_log = cleanse_tatqa_dataset(\n",
        "        tatqa_raw_df=tatqa_raw_df\n",
        "    )\n",
        "\n",
        "    # Record number of rows dropped for audit trail\n",
        "    rows_dropped_tatqa: int = tatqa_cleansing_log.total_rows_dropped\n",
        "\n",
        "    # Log TAT-QA cleansing results\n",
        "    logger.info(f\"TAT-QA cleansed: {rows_dropped_tatqa} rows dropped, {len(tatqa_clean_df)} remaining\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 5 Orchestrator: Cleanse and handle missing entries in Fin-QA\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 5 orchestrator to cleanse Fin-QA dataset\n",
        "    finqa_clean_df, finqa_cleansing_log = cleanse_finqa_dataset(\n",
        "        finqa_raw_df=finqa_raw_df\n",
        "    )\n",
        "\n",
        "    # Record number of rows dropped for Fin-QA\n",
        "    rows_dropped_finqa: int = finqa_cleansing_log.total_rows_dropped\n",
        "\n",
        "    # Log Fin-QA cleansing results\n",
        "    logger.info(f\"Fin-QA cleansed: {rows_dropped_finqa} rows dropped, {len(finqa_clean_df)} remaining\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 6 Orchestrator: Normalize numeric columns and answer representations\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 6 orchestrator to identify numeric columns and parse answers\n",
        "    tatqa_normalized_df, finqa_normalized_df, numeric_metadata = normalize_data_task(\n",
        "        tatqa_df=tatqa_clean_df,\n",
        "        finqa_df=finqa_clean_df,\n",
        "        output_dir=f\"{output_dir}/processed_data\"\n",
        "    )\n",
        "\n",
        "    # Log normalization results\n",
        "    numeric_col_count = sum(1 for v in numeric_metadata.values() if v)\n",
        "    logger.info(f\"Normalization complete: {numeric_col_count} numeric columns identified\")\n",
        "\n",
        "    # Record Phase 2 elapsed time\n",
        "    phase_2_elapsed = time.perf_counter() - phase_2_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.CLEANSING] = phase_2_elapsed\n",
        "\n",
        "    # Record rows processed for audit trail\n",
        "    orchestrator_log.rows_processed = {\n",
        "        \"TAT-QA\": len(tatqa_normalized_df),\n",
        "        \"Fin-QA\": len(finqa_normalized_df)\n",
        "    }\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 3: OFFLINE CORPUS AND STATIC SELF-INFORMATION (TASKS 7-10)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_3_start = time.perf_counter()\n",
        "\n",
        "    # Define paths for corpus artifacts\n",
        "    corpus_path = f\"{output_dir}/offline_corpus.jsonl\"\n",
        "    corpus_stats_dir = f\"{output_dir}/corpus_stats\"\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 7 Orchestrator: Build offline corpus (conditional)\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    if not skip_corpus_build:\n",
        "        # Invoke Task 7 orchestrator to collect and normalize corpus documents\n",
        "        corpus_path = build_offline_corpus(\n",
        "            config=resolved_config,\n",
        "            output_path=corpus_path\n",
        "        )\n",
        "\n",
        "        # Log corpus construction completion\n",
        "        logger.info(f\"Offline corpus built: {corpus_path}\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 8 Orchestrator: Tokenize corpus and compute token counts\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 8 orchestrator to tokenize and count tokens\n",
        "        token_counts, total_tokens_N = compute_corpus_statistics(\n",
        "            config=resolved_config,\n",
        "            corpus_path=corpus_path,\n",
        "            output_dir=corpus_stats_dir\n",
        "        )\n",
        "\n",
        "        # Log tokenization results\n",
        "        logger.info(f\"Corpus tokenized: {total_tokens_N:,} total tokens, {len(token_counts):,} unique\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 9 Orchestrator: Compute token frequencies and probabilities\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 9 orchestrator to convert counts to probabilities\n",
        "        token_probabilities: Dict[int, float] = compute_token_probabilities(\n",
        "            token_counts=token_counts,\n",
        "            total_tokens_N=total_tokens_N\n",
        "        )\n",
        "\n",
        "        # Log probability computation\n",
        "        logger.info(\"Token probabilities computed and validated\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 10 Orchestrator: Compute static self-information scores\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 10 orchestrator to compute I(T) = -log2(p(T))\n",
        "        s_stat_lookup: Dict[int, float] = compute_static_self_information(\n",
        "            p=token_probabilities,\n",
        "            output_dir=corpus_stats_dir,\n",
        "            extra_metadata={\"corpus_path\": corpus_path, \"total_tokens\": total_tokens_N}\n",
        "        )\n",
        "\n",
        "        # Log static self-information computation\n",
        "        logger.info(f\"Static self-information computed: {len(s_stat_lookup):,} tokens\")\n",
        "\n",
        "    else:\n",
        "        # Load existing corpus statistics from disk\n",
        "        import json\n",
        "\n",
        "        # Load pre-computed token counts\n",
        "        with open(f\"{corpus_stats_dir}/token_statistics.json\", \"r\") as f:\n",
        "            stats = json.load(f)\n",
        "            token_counts = Counter({int(k): v for k, v in stats[\"counts\"].items()})\n",
        "            total_tokens_N = stats[\"total_tokens\"]\n",
        "\n",
        "        # Recompute probabilities from loaded counts\n",
        "        token_probabilities = compute_token_probabilities(\n",
        "            token_counts=token_counts,\n",
        "            total_tokens_N=total_tokens_N\n",
        "        )\n",
        "\n",
        "        # Load pre-computed static self-information\n",
        "        with open(f\"{corpus_stats_dir}/static_self_information.json\", \"r\") as f:\n",
        "            s_stat_data = json.load(f)\n",
        "            s_stat_lookup = {int(k): v for k, v in s_stat_data[\"scores\"].items()}\n",
        "\n",
        "        # Log that existing stats were loaded\n",
        "        logger.info(f\"Loaded existing corpus statistics from {corpus_stats_dir}\")\n",
        "\n",
        "    # Record Phase 3 elapsed time\n",
        "    phase_3_elapsed = time.perf_counter() - phase_3_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.CORPUS_STATS] = phase_3_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 4: PROMPT SERIALIZATION AND TEMPLATE CONSTRUCTION (TASKS 11-13)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_4_start = time.perf_counter()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 11 Orchestrator: Serialize tables to text format\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 11 orchestrator to convert table structures to Markdown\n",
        "    tatqa_serialized_df, finqa_serialized_df = serialize_tables_task(\n",
        "        tatqa_df=tatqa_normalized_df,\n",
        "        finqa_df=finqa_normalized_df\n",
        "    )\n",
        "\n",
        "    # Log serialization completion\n",
        "    logger.info(\"Tables serialized to Markdown format\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Tasks 12-13 are invoked per-example within dynamic scoring phase\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Record Phase 4 elapsed time\n",
        "    phase_4_elapsed = time.perf_counter() - phase_4_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.SERIALIZATION] = phase_4_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 5: LLM RESOURCE CONFIGURATION (TASK 14)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_5_start = time.perf_counter()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 14 Orchestrator: Configure scorer and target LLM interfaces\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 14 orchestrator to instantiate all LLM interfaces\n",
        "    llm_interfaces: Dict[str, LLMInterface] = configure_llm_resources(\n",
        "        study_config=resolved_config\n",
        "    )\n",
        "\n",
        "    # Extract scorer interface for dynamic self-information computation\n",
        "    scorer_interface: LLMInterface = llm_interfaces[scorer_llm]\n",
        "\n",
        "    # Extract target interfaces for downstream QA generation\n",
        "    target_interface: LLMInterface = llm_interfaces[target_llm]\n",
        "\n",
        "    # Log LLM configuration\n",
        "    logger.info(f\"LLM interfaces configured: scorer={scorer_llm}, target={target_llm}\")\n",
        "\n",
        "    # Record Phase 5 elapsed time\n",
        "    phase_5_elapsed = time.perf_counter() - phase_5_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.LLM_CONFIG] = phase_5_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 6: DYNAMIC SCORING AND COMBINED SCORES (TASKS 15-18)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_6_start = time.perf_counter()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 15 Orchestrator: Prepare dynamic scoring inputs\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 15 orchestrator to serialize prompts and tokenize with offsets\n",
        "    tatqa_dynamic_inputs: Dict[str, Dict[str, Any]] = prepare_dynamic_scoring_inputs(\n",
        "        df=tatqa_serialized_df,\n",
        "        dataset_name=\"TAT-QA\",\n",
        "        scorer_interface=scorer_interface,\n",
        "        exemplars_str=\"\"\n",
        "    )\n",
        "\n",
        "    # Prepare Fin-QA dynamic scoring inputs\n",
        "    finqa_dynamic_inputs: Dict[str, Dict[str, Any]] = prepare_dynamic_scoring_inputs(\n",
        "        df=finqa_serialized_df,\n",
        "        dataset_name=\"Fin-QA\",\n",
        "        scorer_interface=scorer_interface,\n",
        "        exemplars_str=\"\"\n",
        "    )\n",
        "\n",
        "    # Merge inputs from both datasets for unified processing\n",
        "    all_dynamic_inputs: Dict[str, Dict[str, Any]] = {\n",
        "        **tatqa_dynamic_inputs,\n",
        "        **finqa_dynamic_inputs\n",
        "    }\n",
        "\n",
        "    # Log dynamic input preparation\n",
        "    logger.info(f\"Dynamic scoring inputs prepared: {len(all_dynamic_inputs)} examples\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 16 Orchestrator: Query LLM for conditional log-probabilities\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 16 orchestrator to obtain P_model(t_i | c_i) for each token\n",
        "    logprobs_map: Dict[str, List[float]] = get_prompt_logprobs_task(\n",
        "        dynamic_inputs=all_dynamic_inputs,\n",
        "        scorer_interface=scorer_interface\n",
        "    )\n",
        "\n",
        "    # Log logprob retrieval completion\n",
        "    logger.info(f\"Log-probabilities retrieved for {len(logprobs_map)} prompts\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 17 Orchestrator: Compute dynamic self-information\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 17 orchestrator to convert logprobs to self-information bits\n",
        "    s_dyn_map: Dict[str, List[float]] = compute_dynamic_scores_task(\n",
        "        logprobs_map=logprobs_map\n",
        "    )\n",
        "\n",
        "    # Log dynamic score computation\n",
        "    logger.info(\"Dynamic self-information scores computed\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 18 Orchestrator: Compute combined importance scores\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 18 orchestrator to fuse static and dynamic scores\n",
        "    combined_scores_map: Dict[str, List[float]] = compute_combined_scores_task(\n",
        "        dynamic_inputs=all_dynamic_inputs,\n",
        "        s_dyn_map=s_dyn_map,\n",
        "        s_stat_lookup=s_stat_lookup,\n",
        "        config=resolved_config\n",
        "    )\n",
        "\n",
        "    # Log combined score computation\n",
        "    logger.info(\"Combined importance scores computed using relative-difference rule\")\n",
        "\n",
        "    # Record Phase 6 elapsed time\n",
        "    phase_6_elapsed = time.perf_counter() - phase_6_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.DYNAMIC_SCORING] = phase_6_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 7: PHRASE-LEVEL PRUNING (TASKS 19-21) - CONDITIONAL\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_7_start = time.perf_counter()\n",
        "\n",
        "    # Initialize pruned_results; will be populated based on condition\n",
        "    pruned_results: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    if pipeline_config.use_hard_pruning:\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 19 Orchestrator: Group tokens into phrases using dependency parsing\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 19 orchestrator to run spaCy and map tokens to phrases\n",
        "        phrases_map: Dict[str, List[List[int]]] = group_tokens_into_phrases_task(\n",
        "            dynamic_inputs=all_dynamic_inputs,\n",
        "            config=resolved_config\n",
        "        )\n",
        "\n",
        "        # Log phrase grouping completion\n",
        "        logger.info(f\"Tokens grouped into phrases for {len(phrases_map)} examples\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 20 Orchestrator: Compute phrase-level importance scores\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 20 orchestrator to aggregate token scores within phrases\n",
        "        phrase_scores_data: Dict[str, Dict[str, List[Any]]] = compute_phrase_scores_task(\n",
        "            combined_scores_map=combined_scores_map,\n",
        "            phrases_map=phrases_map,\n",
        "            config=resolved_config\n",
        "        )\n",
        "\n",
        "        # Log phrase score computation\n",
        "        logger.info(\"Phrase-level importance scores computed\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 21 Orchestrator: Prune phrases to enforce token budget\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 21 orchestrator to select high-importance phrases under budget\n",
        "        pruned_results = prune_prompt_task(\n",
        "            dynamic_inputs=all_dynamic_inputs,\n",
        "            phrase_scores_data=phrase_scores_data,\n",
        "            phrases_map=phrases_map,\n",
        "            config=resolved_config\n",
        "        )\n",
        "\n",
        "        # Log pruning completion\n",
        "        mean_cr = np.mean([r[\"compression_ratio\"] for r in pruned_results.values()])\n",
        "        logger.info(f\"Prompts pruned: mean compression ratio = {mean_cr:.2f}x\")\n",
        "\n",
        "    else:\n",
        "        # No hard pruning; use original prompts as compressed prompts\n",
        "        for example_id, inputs in all_dynamic_inputs.items():\n",
        "            pruned_results[example_id] = {\n",
        "                \"compressed_prompt\": inputs[\"prompt_text\"],\n",
        "                \"original_tokens\": inputs.get(\"token_count\", len(inputs[\"token_ids\"])),\n",
        "                \"compressed_tokens\": inputs.get(\"token_count\", len(inputs[\"token_ids\"])),\n",
        "                \"compression_ratio\": 1.0\n",
        "            }\n",
        "\n",
        "        # Log that no pruning was applied\n",
        "        logger.info(\"Hard pruning skipped (baseline condition)\")\n",
        "\n",
        "    # Record Phase 7 elapsed time\n",
        "    phase_7_elapsed = time.perf_counter() - phase_7_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.HARD_PRUNING] = phase_7_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 8: N-GRAM ABBREVIATION (TASKS 22-24) - CONDITIONAL\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_8_start = time.perf_counter()\n",
        "\n",
        "    # Initialize abbreviation artifacts\n",
        "    abbrev_dict: Optional[Dict[str, Any]] = None\n",
        "    tatqa_abbreviated_df = tatqa_serialized_df.copy()\n",
        "    finqa_abbreviated_df = finqa_serialized_df.copy()\n",
        "\n",
        "    if pipeline_config.use_ngram_abbreviation:\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 22 Orchestrator: Extract n-grams and compute frequencies\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 22 orchestrator to extract top-K n-grams from passages\n",
        "        top_k_ngrams: List[Tuple[Tuple[int, ...], int]] = extract_top_ngrams_task(\n",
        "            tatqa_df=tatqa_serialized_df,\n",
        "            finqa_df=finqa_serialized_df,\n",
        "            config=resolved_config\n",
        "        )\n",
        "\n",
        "        # Log n-gram extraction\n",
        "        logger.info(f\"Extracted {len(top_k_ngrams)} top n-grams for abbreviation\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 23 Orchestrator: Construct abbreviation dictionary\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 23 orchestrator to build bidirectional placeholder mapping\n",
        "        abbrev_dict = construct_abbreviation_dict_task(\n",
        "            top_k_ngrams=top_k_ngrams\n",
        "        )\n",
        "\n",
        "        # Log dictionary construction\n",
        "        logger.info(f\"Abbreviation dictionary constructed with {len(abbrev_dict['ngram_to_placeholder'])} entries\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 24 Orchestrator: Apply abbreviation to passages\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 24 orchestrator to replace top-T n-grams with placeholders\n",
        "        tatqa_abbreviated_df, finqa_abbreviated_df = apply_abbreviation_task(\n",
        "            tatqa_df=tatqa_serialized_df,\n",
        "            finqa_df=finqa_serialized_df,\n",
        "            abbrev_dict=abbrev_dict,\n",
        "            config=resolved_config\n",
        "        )\n",
        "\n",
        "        # Log abbreviation application\n",
        "        logger.info(\"N-gram abbreviation applied to all passages\")\n",
        "\n",
        "    else:\n",
        "        # Log that abbreviation was skipped\n",
        "        logger.info(\"N-gram abbreviation skipped (condition does not require it)\")\n",
        "\n",
        "    # Record Phase 8 elapsed time\n",
        "    phase_8_elapsed = time.perf_counter() - phase_8_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.NGRAM_ABBREVIATION] = phase_8_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 9: NUMERIC QUANTIZATION (TASKS 25-27) - CONDITIONAL\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_9_start = time.perf_counter()\n",
        "\n",
        "    # Initialize quantization results\n",
        "    quantization_results: Optional[Dict[Tuple[str, str, str, int], Dict[str, Any]]] = None\n",
        "\n",
        "    if pipeline_config.use_quantization:\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 25 Orchestrator: Extract numeric values for quantization\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 25 orchestrator to extract float arrays from numeric columns\n",
        "        extracted_values: Dict[Tuple[str, str, str, int], Dict[str, Any]] = extract_numeric_values_task(\n",
        "            tatqa_df=tatqa_abbreviated_df,\n",
        "            finqa_df=finqa_abbreviated_df,\n",
        "            numeric_metadata=numeric_metadata\n",
        "        )\n",
        "\n",
        "        # Log numeric extraction\n",
        "        logger.info(f\"Extracted numeric values from {len(extracted_values)} columns\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 26 or 27 Orchestrator: Apply quantization based on scheme\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        if pipeline_config.quantization_scheme == \"uniform\":\n",
        "            # Invoke Task 26 orchestrator for uniform integer quantization\n",
        "            quantization_results = apply_uniform_quantization_task(\n",
        "                extracted_values=extracted_values,\n",
        "                config=resolved_config\n",
        "            )\n",
        "\n",
        "            # Log uniform quantization\n",
        "            logger.info(\"Uniform integer quantization applied\")\n",
        "\n",
        "        else:\n",
        "            # Invoke Task 27 orchestrator for k-means quantization\n",
        "            quantization_results = apply_kmeans_quantization_task(\n",
        "                extracted_values=extracted_values,\n",
        "                config=resolved_config\n",
        "            )\n",
        "\n",
        "            # Log k-means quantization\n",
        "            logger.info(\"K-means quantization applied\")\n",
        "\n",
        "    else:\n",
        "        # Log that quantization was skipped\n",
        "        logger.info(\"Numeric quantization skipped (condition does not require it)\")\n",
        "\n",
        "    # Record Phase 9 elapsed time\n",
        "    phase_9_elapsed = time.perf_counter() - phase_9_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.QUANTIZATION] = phase_9_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 10: EMBEDDING AND REPRESENTATIVE EXEMPLAR SELECTION (TASKS 28-30)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_10_start = time.perf_counter()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 28 Orchestrator: Embed candidate examples for few-shot selection\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 28 orchestrator for TAT-QA embeddings\n",
        "    tatqa_embeddings, tatqa_example_ids = embed_examples_task(\n",
        "        df=tatqa_abbreviated_df,\n",
        "        dataset_name=\"TAT-QA\",\n",
        "        config=resolved_config,\n",
        "        output_dir=f\"{output_dir}/embeddings\"\n",
        "    )\n",
        "\n",
        "    # Invoke Task 28 orchestrator for Fin-QA embeddings\n",
        "    finqa_embeddings, finqa_example_ids = embed_examples_task(\n",
        "        df=finqa_abbreviated_df,\n",
        "        dataset_name=\"Fin-QA\",\n",
        "        config=resolved_config,\n",
        "        output_dir=f\"{output_dir}/embeddings\"\n",
        "    )\n",
        "\n",
        "    # Log embedding computation\n",
        "    logger.info(f\"Embeddings computed: TAT-QA={tatqa_embeddings.shape}, Fin-QA={finqa_embeddings.shape}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 29 Orchestrator: Select optimal cluster count via silhouette score\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 29 orchestrator for TAT-QA optimal k selection\n",
        "    tatqa_k_star: int = select_optimal_k_task(\n",
        "        embeddings=tatqa_embeddings,\n",
        "        config=resolved_config\n",
        "    )\n",
        "\n",
        "    # Invoke Task 29 orchestrator for Fin-QA optimal k selection\n",
        "    finqa_k_star: int = select_optimal_k_task(\n",
        "        embeddings=finqa_embeddings,\n",
        "        config=resolved_config\n",
        "    )\n",
        "\n",
        "    # Log optimal k selection\n",
        "    logger.info(f\"Optimal cluster counts: TAT-QA k*={tatqa_k_star}, Fin-QA k*={finqa_k_star}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 30 Orchestrator: Select representative exemplars from clusters\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 30 orchestrator for TAT-QA representative exemplars\n",
        "    tatqa_representative_ids: List[str] = select_representative_exemplars_task(\n",
        "        embeddings=tatqa_embeddings,\n",
        "        example_ids=tatqa_example_ids,\n",
        "        k_star=tatqa_k_star,\n",
        "        config=resolved_config\n",
        "    )\n",
        "\n",
        "    # Invoke Task 30 orchestrator for Fin-QA representative exemplars\n",
        "    finqa_representative_ids: List[str] = select_representative_exemplars_task(\n",
        "        embeddings=finqa_embeddings,\n",
        "        example_ids=finqa_example_ids,\n",
        "        k_star=finqa_k_star,\n",
        "        config=resolved_config\n",
        "    )\n",
        "\n",
        "    # Log representative exemplar selection\n",
        "    logger.info(f\"Representative exemplars selected: TAT-QA={tatqa_representative_ids}, Fin-QA={finqa_representative_ids}\")\n",
        "\n",
        "    # Record Phase 10 elapsed time\n",
        "    phase_10_elapsed = time.perf_counter() - phase_10_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.EMBEDDING] = phase_10_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 11: SEMANTIC SIMILARITY EVALUATION (TASKS 31-32)\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_11_start = time.perf_counter()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 31 Orchestrator: Compute embeddings for semantic similarity\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 31 orchestrator to embed original and compressed prompt pairs\n",
        "    U_orig, V_comp, similarity_example_ids = compute_similarity_embeddings_task(\n",
        "        dynamic_inputs=all_dynamic_inputs,\n",
        "        pruned_results=pruned_results,\n",
        "        config=resolved_config,\n",
        "        output_dir=f\"{output_dir}/similarity_embeddings\"\n",
        "    )\n",
        "\n",
        "    # Log similarity embedding computation\n",
        "    logger.info(f\"Similarity embeddings computed for {len(similarity_example_ids)} pairs\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Task 32 Orchestrator: Evaluate semantic similarity and compute statistics\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Invoke Task 32 orchestrator to compute cosine similarities and statistics\n",
        "    similarity_results: Dict[str, Any] = evaluate_semantic_similarity_task(\n",
        "        U=U_orig,\n",
        "        V=V_comp,\n",
        "        example_ids=similarity_example_ids,\n",
        "        config=resolved_config\n",
        "    )\n",
        "\n",
        "    # Log similarity evaluation results\n",
        "    logger.info(\n",
        "        f\"Semantic similarity: mean={similarity_results['mean']:.3f}, \"\n",
        "        f\"5th percentile={similarity_results['percentile_5']:.3f}, \"\n",
        "        f\"below threshold={similarity_results['below_threshold_count']}\"\n",
        "    )\n",
        "\n",
        "    # Record Phase 11 elapsed time\n",
        "    phase_11_elapsed = time.perf_counter() - phase_11_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.SIMILARITY_EVAL] = phase_11_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # PHASE 12: HUMAN EVALUATION PROTOCOL (TASKS 33-35) - CONDITIONAL\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Record phase start time\n",
        "    phase_12_start = time.perf_counter()\n",
        "\n",
        "    # Initialize human evaluation results\n",
        "    human_evaluation: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    if not skip_human_eval:\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Construct evaluation pairs from dynamic inputs and pruned results\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Build TAT-QA pairs for evaluation\n",
        "        tatqa_pairs: List[Dict[str, str]] = [\n",
        "            {\n",
        "                \"example_id\": eid,\n",
        "                \"original\": all_dynamic_inputs[eid][\"prompt_text\"],\n",
        "                \"compressed\": pruned_results[eid][\"compressed_prompt\"]\n",
        "            }\n",
        "            for eid in tatqa_example_ids if eid in pruned_results\n",
        "        ]\n",
        "\n",
        "        # Build Fin-QA pairs for evaluation\n",
        "        finqa_pairs: List[Dict[str, str]] = [\n",
        "            {\n",
        "                \"example_id\": eid,\n",
        "                \"original\": all_dynamic_inputs[eid][\"prompt_text\"],\n",
        "                \"compressed\": pruned_results[eid][\"compressed_prompt\"]\n",
        "            }\n",
        "            for eid in finqa_example_ids if eid in pruned_results\n",
        "        ]\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 33 Orchestrator: Run calibration with LLM-based proxy annotators\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 33 orchestrator to calibrate proxy annotators\n",
        "        calibration_results: Dict[str, Any] = run_llm_calibration_task(\n",
        "            tatqa_pairs=tatqa_pairs[:15],\n",
        "            finqa_pairs=finqa_pairs[:15]\n",
        "        )\n",
        "\n",
        "        # Log calibration completion\n",
        "        logger.info(\"Proxy annotator calibration complete\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 34 Orchestrator: Conduct main evaluation on 90 pairs\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Extract calibrated agents from calibration results\n",
        "        calibrated_agents: Dict[str, LLMInterface] = calibration_results[\"agents\"]\n",
        "\n",
        "        # Invoke Task 34 orchestrator to collect ratings\n",
        "        raw_ratings: List[Dict[str, Any]] = run_main_evaluation_task(\n",
        "            tatqa_pairs=tatqa_pairs,\n",
        "            finqa_pairs=finqa_pairs,\n",
        "            agents=calibrated_agents,\n",
        "            output_dir=f\"{output_dir}/human_eval_results\"\n",
        "        )\n",
        "\n",
        "        # Log main evaluation completion\n",
        "        logger.info(f\"Main evaluation complete: {len(raw_ratings)} pairs rated\")\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Task 35 Orchestrator: Analyze human ratings vs embedding similarity\n",
        "        # ---------------------------------------------------------------------\n",
        "\n",
        "        # Invoke Task 35 orchestrator to compare ratings to cosine similarities\n",
        "        human_embedding_analysis: Dict[str, Any] = analyze_human_vs_embedding_task(\n",
        "            raw_ratings=raw_ratings,\n",
        "            similarity_results=similarity_results\n",
        "        )\n",
        "\n",
        "        # Log analysis completion\n",
        "        logger.info(\n",
        "            f\"Human-embedding analysis: mean rating={human_embedding_analysis['mean_human_rating']:.2f}/5, \"\n",
        "            f\"mismatch fraction={human_embedding_analysis['mismatch_fraction']:.2%}\"\n",
        "        )\n",
        "\n",
        "        # Package human evaluation results\n",
        "        human_evaluation = {\n",
        "            \"calibration\": calibration_results,\n",
        "            \"ratings\": raw_ratings,\n",
        "            \"analysis\": human_embedding_analysis\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        # Log that human evaluation was skipped\n",
        "        logger.info(\"Human evaluation skipped (skip_human_eval=True)\")\n",
        "\n",
        "    # Record Phase 12 elapsed time\n",
        "    phase_12_elapsed = time.perf_counter() - phase_12_start\n",
        "    orchestrator_log.stage_timings[PipelineStage.HUMAN_EVAL] = phase_12_elapsed\n",
        "\n",
        "    # ==========================================================================\n",
        "    # FINAL ASSEMBLY: CONSTRUCT OUTPUT AND FINALIZE LOG\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Compute aggregate metrics for output\n",
        "    mean_compression_ratio = np.mean([\n",
        "        r[\"compression_ratio\"] for r in pruned_results.values()\n",
        "    ])\n",
        "\n",
        "    # Compute mean cosine similarity from similarity results\n",
        "    mean_cosine_similarity = similarity_results[\"mean\"]\n",
        "\n",
        "    # Compute mean human rating if available\n",
        "    mean_human_rating = (\n",
        "        human_evaluation[\"analysis\"][\"mean_human_rating\"]\n",
        "        if human_evaluation is not None\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    # Construct aggregate metrics dictionary\n",
        "    metrics: Dict[str, float] = {\n",
        "        \"mean_compression_ratio\": float(mean_compression_ratio),\n",
        "        \"mean_cosine_similarity\": float(mean_cosine_similarity),\n",
        "        \"percentile_5_similarity\": float(similarity_results[\"percentile_5\"]),\n",
        "        \"below_threshold_fraction\": float(\n",
        "            similarity_results[\"below_threshold_count\"] / len(similarity_example_ids)\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Add human evaluation metric if available\n",
        "    if mean_human_rating is not None:\n",
        "        metrics[\"mean_human_rating\"] = float(mean_human_rating)\n",
        "\n",
        "    # Construct final OrchestratorOutput dataclass\n",
        "    orchestrator_output = OrchestratorOutput(\n",
        "        tatqa_processed_df=tatqa_abbreviated_df,\n",
        "        finqa_processed_df=finqa_abbreviated_df,\n",
        "        pruned_prompts=pruned_results,\n",
        "        abbreviation_dict=abbrev_dict,\n",
        "        quantization_results=quantization_results,\n",
        "        representative_exemplars={\n",
        "            \"TAT-QA\": tatqa_representative_ids,\n",
        "            \"Fin-QA\": finqa_representative_ids\n",
        "        },\n",
        "        similarity_results=similarity_results,\n",
        "        human_evaluation=human_evaluation,\n",
        "        metrics=metrics,\n",
        "        s_stat_lookup=s_stat_lookup,\n",
        "        validation_reports={\n",
        "            \"TAT-QA\": tatqa_validation_report,\n",
        "            \"Fin-QA\": finqa_validation_report\n",
        "        },\n",
        "        cleansing_logs={\n",
        "            \"TAT-QA\": tatqa_cleansing_log,\n",
        "            \"Fin-QA\": finqa_cleansing_log\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Compute output hash for verification\n",
        "    output_repr = f\"{mean_compression_ratio}|{mean_cosine_similarity}|{len(pruned_results)}\"\n",
        "    output_hash = hashlib.sha256(output_repr.encode()).hexdigest()[:16]\n",
        "\n",
        "    # Finalize orchestrator log\n",
        "    orchestrator_log.output_hash = output_hash\n",
        "    orchestrator_log.status = \"SUCCESS\"\n",
        "    orchestrator_log.compression_summary = {\n",
        "        \"mean_compression_ratio\": float(mean_compression_ratio),\n",
        "        \"total_examples_processed\": len(pruned_results),\n",
        "        \"tatqa_examples\": len(tatqa_example_ids),\n",
        "        \"finqa_examples\": len(finqa_example_ids)\n",
        "    }\n",
        "\n",
        "    # Compute total pipeline elapsed time\n",
        "    total_elapsed = sum(orchestrator_log.stage_timings.values())\n",
        "\n",
        "    # Log pipeline completion\n",
        "    logger.info(\n",
        "        f\"Pipeline complete: {orchestrator_log.status}, \"\n",
        "        f\"total time={total_elapsed:.1f}s, \"\n",
        "        f\"compression ratio={mean_compression_ratio:.2f}x\"\n",
        "    )\n",
        "\n",
        "    # Return output and log as tuple\n",
        "    return orchestrator_output, orchestrator_log\n"
      ],
      "metadata": {
        "id": "PqwNSJJy4fJj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}